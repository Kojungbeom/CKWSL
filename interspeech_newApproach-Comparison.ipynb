{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "import wave\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import torch.\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "import array\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import wave\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pickle\n",
    "from IPython.display import Audio\n",
    "import webrtcvad\n",
    "import wandb\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrhwndqja\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rhwndqja/KP/runs/6v64sgzy\" target=\"_blank\">spring-water-47</a></strong> to <a href=\"https://wandb.ai/rhwndqja/KP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rhwndqja/KP/runs/6v64sgzy?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe10ad6af10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"KP\", entity=\"rhwndqja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSL_Extractor(nn.Module):\n",
    "    \"\"\"Angle discriminator\n",
    "    \"\"\"\n",
    "    def __init__(self, de_ch):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.de_ch = de_ch\n",
    "        self.k = [4,4,4,4]\n",
    "        self.s = [2,2,2,2]\n",
    "        self.p = [1,1,1,1]\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(de_ch[3][1], de_ch[3][0], kernel_size=self.k[0], stride=self.s[0], padding=self.p[0]),\n",
    "            nn.BatchNorm1d(de_ch[3][0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(de_ch[2][1], de_ch[2][0], kernel_size=self.k[1], stride=self.s[1], padding=self.p[1]),\n",
    "            nn.BatchNorm1d(de_ch[2][0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(de_ch[1][1], de_ch[1][0], kernel_size=self.k[2], stride=self.s[2], padding=self.p[2]),\n",
    "            nn.BatchNorm1d(de_ch[1][0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(de_ch[0][1], de_ch[0][0], kernel_size=self.k[3], stride=self.s[3], padding=self.p[3]),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.decoder1(x)\n",
    "        out = self.decoder2(out) \n",
    "        out = self.decoder3(out) \n",
    "        out = self.decoder4(out)\n",
    "        return out\n",
    "    \n",
    "class KSL_Localizer(nn.Module):\n",
    "    \"\"\"Angle discriminator\n",
    "    \"\"\"\n",
    "    def __init__(self, last_en_ch):\n",
    "        super().__init__()\n",
    "        self.encoder_last = nn.Sequential(\n",
    "            nn.Conv1d(last_en_ch, last_en_ch, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(last_en_ch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.globalPooling = nn.AdaptiveMaxPool1d(1)   \n",
    "        self.fcl = nn.Sequential(\n",
    "            nn.Linear(last_en_ch, int(last_en_ch//2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(int(last_en_ch//2), 12)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #out = self.encoder_last(x)\n",
    "        out = self.globalPooling(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fcl(out).view(-1, 2, 6)\n",
    "        return out[:, 0, :], out[:, 1, :]\n",
    "    \n",
    "    \n",
    "class KSL_Detector(nn.Module):\n",
    "    \"\"\"Angle discriminator\n",
    "    \"\"\"\n",
    "    def __init__(self, last_en_ch):\n",
    "        super().__init__()\n",
    "        self.encoder_last = nn.Sequential(\n",
    "            nn.Conv1d(last_en_ch, last_en_ch, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(last_en_ch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.globalPooling = nn.AdaptiveAvgPool1d(1)   \n",
    "        self.fcl = nn.Sequential(\n",
    "            nn.Linear(last_en_ch, int(last_en_ch//2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(int(last_en_ch//2), 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #out = self.encoder_last(x)\n",
    "        out = self.globalPooling(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fcl(out)\n",
    "        return out\n",
    "\n",
    "class KeywordSpeakerLocalizer(nn.Module):\n",
    "    def __init__(self, en_ch):\n",
    "        super().__init__()\n",
    "        self.en_ch = en_ch # [[6, 32], [32, 64] ...]\n",
    "        last_ch = self.en_ch[-1][1]\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv1d(en_ch[0][0], en_ch[0][1], kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(en_ch[0][1]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(en_ch[1][0], en_ch[1][1], kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(en_ch[1][1]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv1d(en_ch[2][0], en_ch[2][1], kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(en_ch[2][1]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv1d(en_ch[3][0], en_ch[3][1], kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(en_ch[3][1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.ksl_Extractor = KSL_Extractor(self.en_ch)\n",
    "        self.ksl_Localizer = KSL_Localizer(last_ch)\n",
    "        self.ksl_Detector = KSL_Detector(last_ch)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.encoder1(x)\n",
    "        out = self.encoder2(out)\n",
    "        out = self.encoder3(out)\n",
    "        out = self.encoder4(out)\n",
    "        out_localizer1, out_localizer2 = self.ksl_Localizer(out)\n",
    "        out_extractor = self.ksl_Extractor(out)\n",
    "        out_detector = self.ksl_Detector(out)\n",
    "        return out_localizer1, out_localizer2, out_extractor, out_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeywordSpeakerLocalizer(\n",
      "  (relu): ReLU()\n",
      "  (encoder1): Sequential(\n",
      "    (0): Conv1d(6, 32, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (encoder2): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (encoder3): Sequential(\n",
      "    (0): Conv1d(64, 128, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (encoder4): Sequential(\n",
      "    (0): Conv1d(128, 128, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (ksl_Extractor): KSL_Extractor(\n",
      "    (decoder1): Sequential(\n",
      "      (0): ConvTranspose1d(128, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (decoder2): Sequential(\n",
      "      (0): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (decoder3): Sequential(\n",
      "      (0): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (decoder4): Sequential(\n",
      "      (0): ConvTranspose1d(32, 6, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    )\n",
      "  )\n",
      "  (ksl_Localizer): KSL_Localizer(\n",
      "    (encoder_last): Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (globalPooling): AdaptiveMaxPool1d(output_size=1)\n",
      "    (fcl): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=64, out_features=12, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ksl_Detector): KSL_Detector(\n",
      "    (encoder_last): Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (globalPooling): AdaptiveAvgPool1d(output_size=1)\n",
      "    (fcl): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Output of KSL_Localizer: torch.Size([1, 6])\n",
      "Output of KSL_Localizer: torch.Size([1, 6])\n",
      "Output of KSL_Localizer: torch.Size([1, 6, 16000])\n",
      "Output of KSL_Localizer: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    encoder_ch = [[6,32], [32, 64], [64, 128], [128, 128]]\n",
    "    net = KeywordSpeakerLocalizer(encoder_ch)\n",
    "    print(net)\n",
    "    sample = torch.randn(1, 6, 16000)\n",
    "    out_localizer1, out_localizer2, out_extractor, out_detector = net(sample)\n",
    "    print(f'Output of KSL_Localizer: {out_localizer1.shape}')\n",
    "    print(f'Output of KSL_Localizer: {out_localizer2.shape}')\n",
    "    print(f'Output of KSL_Localizer: {out_extractor.shape}')\n",
    "    print(f'Output of KSL_Localizer: {out_detector.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 32, 8000]             992\n",
      "       BatchNorm1d-2             [-1, 32, 8000]              64\n",
      "              ReLU-3             [-1, 32, 8000]               0\n",
      "            Conv1d-4             [-1, 64, 4000]          10,304\n",
      "       BatchNorm1d-5             [-1, 64, 4000]             128\n",
      "              ReLU-6             [-1, 64, 4000]               0\n",
      "            Conv1d-7            [-1, 128, 2000]          41,088\n",
      "       BatchNorm1d-8            [-1, 128, 2000]             256\n",
      "              ReLU-9            [-1, 128, 2000]               0\n",
      "           Conv1d-10            [-1, 128, 1000]          82,048\n",
      "      BatchNorm1d-11            [-1, 128, 1000]             256\n",
      "             ReLU-12            [-1, 128, 1000]               0\n",
      "AdaptiveMaxPool1d-13               [-1, 128, 1]               0\n",
      "           Linear-14                   [-1, 64]           8,256\n",
      "             ReLU-15                   [-1, 64]               0\n",
      "          Dropout-16                   [-1, 64]               0\n",
      "           Linear-17                   [-1, 12]             780\n",
      "    KSL_Localizer-18         [[-1, 6], [-1, 6]]               0\n",
      "  ConvTranspose1d-19            [-1, 128, 2000]          65,664\n",
      "      BatchNorm1d-20            [-1, 128, 2000]             256\n",
      "             ReLU-21            [-1, 128, 2000]               0\n",
      "  ConvTranspose1d-22             [-1, 64, 4000]          32,832\n",
      "      BatchNorm1d-23             [-1, 64, 4000]             128\n",
      "             ReLU-24             [-1, 64, 4000]               0\n",
      "  ConvTranspose1d-25             [-1, 32, 8000]           8,224\n",
      "      BatchNorm1d-26             [-1, 32, 8000]              64\n",
      "             ReLU-27             [-1, 32, 8000]               0\n",
      "  ConvTranspose1d-28             [-1, 6, 16000]             774\n",
      "    KSL_Extractor-29             [-1, 6, 16000]               0\n",
      "AdaptiveAvgPool1d-30               [-1, 128, 1]               0\n",
      "           Linear-31                   [-1, 64]           8,256\n",
      "             ReLU-32                   [-1, 64]               0\n",
      "          Dropout-33                   [-1, 64]               0\n",
      "           Linear-34                    [-1, 1]              65\n",
      "     KSL_Detector-35                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 260,435\n",
      "Trainable params: 260,435\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.37\n",
      "Forward/backward pass size (MB): 39.56\n",
      "Params size (MB): 0.99\n",
      "Estimated Total Size (MB): 40.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = KeywordSpeakerLocalizer(encoder_ch)\n",
    "summary(net.cuda(), (6, 16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_at():\n",
    "    angle_table_base = np.array([\n",
    "        120.0,\n",
    "        60.0,\n",
    "        0.0,\n",
    "        300.0,\n",
    "        240.0,\n",
    "        180.0]\n",
    "    )\n",
    "\n",
    "    at = angle_table_base\n",
    "    at = np.sort(at) \n",
    "    return at\n",
    "\n",
    "\n",
    "def get_label_index(angle, at):\n",
    "    label_index = 0\n",
    "    angle = int(angle)\n",
    "    if (angle < at[1] and angle >= at[0]):\n",
    "        label_index = 0\n",
    "        angle = angle / 60\n",
    "    elif (angle < at[2] and angle >= at[1]):\n",
    "        label_index = 1\n",
    "        angle = (angle - at[1]) / 60\n",
    "    elif (angle < at[3] and angle >= at[2]):\n",
    "        label_index = 2\n",
    "        angle = (angle - at[2]) / 60\n",
    "    elif (angle < at[4] and angle >= at[3]):\n",
    "        label_index = 3\n",
    "        angle = (angle - at[3]) / 60\n",
    "    elif (angle < at[5] and angle >= at[4]):\n",
    "        label_index = 4\n",
    "        angle = (angle - at[4]) / 60\n",
    "    elif (angle < 360 and angle >= at[5]):\n",
    "        label_index = 5\n",
    "        angle = (angle - at[5]) / 60\n",
    "        \n",
    "    if at[label_index] == angle:\n",
    "        angle = 0\n",
    "    if angle == 360:\n",
    "        angle = 0\n",
    "    return label_index, angle\n",
    "\n",
    "    \n",
    "class AutoDataset(Dataset):\n",
    "    def __init__(self, annotations_file, file_dir, target_word='sheila', transform=None, target_transform=None, DType=\"train\"):\n",
    "        \n",
    "        self.file_labels = pd.read_csv(annotations_file)\n",
    "        if DType == \"train\":\n",
    "            self.file_labels = self.file_labels[10000:-10000]\n",
    "        elif DType == \"valid\":\n",
    "            self.file_labels = self.file_labels[:10000]\n",
    "        elif DType == \"test\":\n",
    "            self.file_labels = self.file_labels[-10000:]\n",
    "            \n",
    "        self.target_word = target_word\n",
    "        self.no_target_labels = self.file_labels[~self.file_labels['fileName'].str.contains(self.target_word)]\n",
    "        self.only_target_labels = self.file_labels[self.file_labels['fileName'].str.contains(self.target_word)]\n",
    "        self.tar_idxs = len(self.only_target_labels)\n",
    "        self.vic_idxs = len(self.no_target_labels)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_classes = 6\n",
    "        self.at = get_at()\n",
    "        self.respeaker_mic = np.array([\n",
    "            [-0.02315, 0.04009,1], # 120 degree\n",
    "            [0.02315, 0.04009,1],  # 60 degree\n",
    "            [0.04629, 0.0,1],      # 0 degree\n",
    "            [0.02315,-0.04009,1],  # 300 degree\n",
    "            [-0.02315,-0.04009,1], # 240 degree\n",
    "            [-0.04629, 0.0,1]      # 180 degree\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.no_target_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #vic = random.sample(range(self.vic_idxs), 1)[0]\n",
    "        \n",
    "        is_tar_in_sg = random.sample(range(2), 1)[0] # Is target in signal? 1-> yes, 0 -> no\n",
    "        \n",
    "        if is_tar_in_sg:\n",
    "            vic1_fp = self.no_target_labels.iloc[idx, 2]\n",
    "            #tar = random.sample(range(self.tar_idxs), 1)[0]\n",
    "            tar = idx % self.tar_idxs\n",
    "            tar_fp = self.only_target_labels.iloc[tar, 2] # target file path (fp)\n",
    "            \n",
    "            vic1_sg, sr = torchaudio.load(vic1_fp)\n",
    "            tar_sg, sr = torchaudio.load(tar_fp)\n",
    "            \n",
    "            y_sg = tar_sg\n",
    "            \n",
    "            class_lb = 1.\n",
    "\n",
    "            mixed_signal = torch.cat([tar_sg.unsqueeze(0), vic1_sg.unsqueeze(0)])\n",
    "            #mixed_signal = vic1_sg + tar_sg\n",
    "            \n",
    "            x, y, z = self.only_target_labels.iloc[tar, 5:8]\n",
    "            angle = int(math.atan2(float(y), float(x)) * 180 / np.pi)\n",
    "            \n",
    "        else:\n",
    "            vic1_fp = self.no_target_labels.iloc[idx, 2]\n",
    "            vic2 = random.sample(range(self.vic_idxs), 1)[0]\n",
    "            vic2_fp = self.no_target_labels.iloc[vic2, 2] # victim file path (fp)\n",
    "            \n",
    "            vic1_sg, sr = torchaudio.load(vic1_fp)\n",
    "            vic2_sg, sr = torchaudio.load(vic2_fp)\n",
    "            \n",
    "            y_sg = vic1_sg\n",
    "            \n",
    "            class_lb = 0.\n",
    "            mixed_signal = torch.cat([vic1_sg.unsqueeze(0), vic2_sg.unsqueeze(0)])\n",
    "            #mixed_signal = vic1_sg + vic2_sg\n",
    "            \n",
    "            x, y, z = self.no_target_labels.iloc[idx, 5:8]\n",
    "            angle = int(math.atan2(float(y), float(x)) * 180 / np.pi)\n",
    "            \n",
    "        #mixed_signal[mixed_signal > 1] = 1.\n",
    "        #mixed_signal[mixed_signal < -1] = -1.\n",
    "        \n",
    "        if self.transform:\n",
    "            x_sg = self.transform(mixed_signal)\n",
    "        #x_sg = mixed_signal\n",
    "        \n",
    "        if angle < 0:\n",
    "            angle = angle + 360\n",
    "        y_region, y_angle = get_label_index(angle, self.at)\n",
    "        \n",
    "        reg = torch.tensor([y_region])\n",
    "        bce = torch.zeros(len(reg), self.n_classes).scatter_(1, reg.unsqueeze(1), 1.)\n",
    "        \n",
    "        y_angle  = (bce * y_angle).squeeze(0)\n",
    "        \n",
    "        y_class = torch.tensor(class_lb)\n",
    "\n",
    "        return x_sg.float(), y_sg.float(), y_region, y_angle, y_class, bce.squeeze(0), angle\n",
    "    \n",
    "class JB_Sample:\n",
    "    def __init__(self, sample, vad_mode):\n",
    "        self.sample = sample\n",
    "        self.sample_int16 = (sample * (2**16 / 2)).numpy().astype(np.int16)\n",
    "        self.vad = webrtcvad.Vad()\n",
    "        self.vad.set_mode(vad_mode)\n",
    "        self.chunk_num = 50\n",
    "    \n",
    "    def sample_vad(self):\n",
    "        vad_result = []\n",
    "        sample_chunk = np.reshape(self.sample_int16[:1, :].T,\n",
    "                                  (self.chunk_num, 320, 1))\n",
    "        for chunk in sample_chunk: \n",
    "            # Scaling Chunk\n",
    "            vad_result.append(self.vad.is_speech(chunk, 16000))\n",
    "        return vad_result\n",
    "    \n",
    "    \n",
    "    def find_point(self):\n",
    "        vad_result = self.sample_vad()\n",
    "        start = False\n",
    "        start_idx = 0\n",
    "        utt_list = []\n",
    "        for idx, i in enumerate(vad_result):\n",
    "            if i and not start:\n",
    "                start = True\n",
    "                start_idx = idx\n",
    "\n",
    "            elif i and start:\n",
    "                if idx == 49:\n",
    "                    utt_list.append([start_idx, idx])\n",
    "                continue\n",
    "\n",
    "            elif not i and start:\n",
    "                start = False\n",
    "                utt_list.append([start_idx, idx])\n",
    "\n",
    "        biggest = 0\n",
    "        if utt_list:\n",
    "            for block in utt_list:\n",
    "                size = block[1] - block[0]\n",
    "                if size > biggest:\n",
    "                    biggest = size\n",
    "                    start, end = block[0], block[1]\n",
    "        else:\n",
    "            start, end = 0, 49\n",
    "            \n",
    "        return start, end\n",
    "    \n",
    "    def get_result(self):\n",
    "        return self.sample_vad()\n",
    "    \n",
    "class Overlap_interference(object):\n",
    "    def __init__(self, overlap, sir, bg_noise=None):\n",
    "        self.overlap = overlap\n",
    "        self.len_overlap = len(overlap)\n",
    "        \n",
    "        self.sir = sir #[0,10,20,30]\n",
    "        self.len_sir = len(sir)\n",
    "        \n",
    "        self.vad_mode = 2\n",
    "        if bg_noise:\n",
    "            self.bg_noise = bg_noise\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # selected overlap\n",
    "        overlap_idx = random.sample(range(self.len_overlap), 1)[0]\n",
    "        # selected sir\n",
    "        sir_idx = random.sample(range(self.len_sir), 1)[0]\n",
    "        \n",
    "        x1_sg, x2_sg = JB_Sample(sample[0], self.vad_mode), JB_Sample(sample[1], self.vad_mode)\n",
    "        \n",
    "        x1_start, x1_end = x1_sg.find_point()\n",
    "        x2_start, x2_end = x2_sg.find_point()\n",
    "        \n",
    "        target_rms = torch.mean(torch.sqrt(torch.mean(torch.square(sample[0]), axis=0)))\n",
    "        interf_rms = torch.mean(torch.sqrt(torch.mean(torch.square(sample[1]), axis=0)))\n",
    "        \n",
    "        sir = float(self.sir[sir_idx]) / 20\n",
    "        overlap = self.overlap[overlap_idx]\n",
    "        #print(f'SNR: {self.sir[sir_idx]}dB, Overlap ratio: {overlap}')\n",
    "        \n",
    "        ad_interf_rms = target_rms / (10 ** sir)\n",
    "        ad_interf_amp = sample[1] * (ad_interf_rms / interf_rms)\n",
    "        \n",
    "        \n",
    "        mixture = self.overlapping([sample[0], x1_start, x1_end], [sample[1], x2_start, x2_end], overlap)\n",
    "        return mixture\n",
    "    \n",
    "    def overlapping(self, target, interf, overlap_ratio):\n",
    "        # direction = front or back\n",
    "        target_sg, t_start, t_end = target\n",
    "        interf_sg, i_start, i_end = interf\n",
    "        mixture = target_sg.clone()\n",
    "\n",
    "        t_mid = t_start + int((t_end - t_start) / 2)\n",
    "        #print(f'target start {t_start}, target mid {t_mid}, target end {t_end}')\n",
    "        i_mid = i_start + int((i_end - i_start) / 2)\n",
    "        #print(f'interf start {i_start}, interf mid {i_mid}, interf end {i_end}')\n",
    "\n",
    "        ignore = False\n",
    "        margin_start, margin_end = 0, 0\n",
    "        pm = 0\n",
    "        diff = t_mid - i_mid\n",
    "        if diff >= 0:\n",
    "            pm = 1\n",
    "        elif diff < 0:\n",
    "            pm = -1\n",
    "\n",
    "        if overlap_ratio == 1.0:\n",
    "            diff = t_mid - i_mid\n",
    "        elif overlap_ratio == 0.5:\n",
    "            if pm > 0:\n",
    "                diff = t_mid - i_end\n",
    "            elif pm < 0:\n",
    "                diff = t_mid - i_start\n",
    "        elif overlap_ratio == 0.0:\n",
    "            if pm > 0:\n",
    "                diff = t_start - i_end\n",
    "            elif pm < 0:\n",
    "                diff = t_end - i_start\n",
    "        #print(f'diff: {diff}')\n",
    "        n_start = i_start + diff\n",
    "        n_end = i_end + diff\n",
    "        n_mid = int((n_end - n_start) / 2) + n_start\n",
    "\n",
    "        if n_start < 0:\n",
    "            margin_start = abs(n_start)\n",
    "            n_start = 0\n",
    "\n",
    "        elif n_start > 49:\n",
    "            n_start = 49\n",
    "            ignore = True\n",
    "\n",
    "        if n_end < 0:\n",
    "            n_end = 0\n",
    "            ignore = True\n",
    "\n",
    "        elif n_end > 49:\n",
    "            margin_end = int(n_end - 49)\n",
    "            n_end = 49\n",
    "        #print(f'n_start:{n_start}, n_mid: {n_mid}, n_end: {n_end}')\n",
    "        #print(f'm_start:{margin_start}, m_end: {margin_end}')\n",
    "        #print(f'i_start:{i_start+margin_start}, i_mid: {i_mid}, i_end: {i_end-margin_end}')\n",
    "        if not ignore:\n",
    "            mixture[:, n_start*320:n_end*320] = target_sg[:, n_start*320:n_end*320] \\\n",
    "                                                 + interf_sg[:, int(i_start+margin_start)*320:int(i_end-margin_end)*320]\n",
    "        mixture[mixture > 1] = 1\n",
    "        mixture[mixture < -1] = -1\n",
    "\n",
    "        return mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = 'data_all.csv'\n",
    "file_dir = 'data_all/'\n",
    "root_dir = '../dataset'\n",
    "\n",
    "overlap = [1.0, 0.5, 0.]\n",
    "sir = [-10, 0, 10]\n",
    "OL_SIR = Overlap_interference(overlap=overlap, sir=sir)\n",
    "composed = transforms.Compose([OL_SIR])\n",
    "target_w = \"go\"\n",
    "dat = AutoDataset(\n",
    "    annotations_file=annotation,\n",
    "    file_dir=file_dir,\n",
    "    target_word=target_w,\n",
    "    transform=composed\n",
    ")\n",
    "\n",
    "valid = .2\n",
    "test= .5\n",
    "shuffle_dataset = True\n",
    "random_seed= 200\n",
    "\n",
    "dataset_size = len(dat)\n",
    "indices = list(range(dataset_size))\n",
    "val_split = int(np.floor(valid * dataset_size))\n",
    "\n",
    "\"\"\"\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[val_split:], indices[:val_split]\n",
    "\n",
    "test_split = int(np.floor(test * len(val_indices)))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(val_indices)\n",
    "val_indices, test_indices = val_indices[test_split:], val_indices[:test_split]\n",
    "\n",
    "\n",
    "with open('indice/train_indices_auto_go.txt', 'wb') as f:\n",
    "    pickle.dump(train_indices, f)\n",
    "with open('indice/val_indices_auto_go.txt', 'wb') as f:\n",
    "    pickle.dump(val_indices, f)\n",
    "with open('indice/test_indices_auto_go.txt', 'wb') as f:\n",
    "    pickle.dump(test_indices, f)\n",
    "\n",
    "\n",
    "with open('indice/train_indices_auto_go.txt', 'rb') as f:\n",
    "    train_indices = pickle.load(f)\n",
    "    \n",
    "with open('indice/val_indices_auto_go.txt', 'rb') as f:\n",
    "    val_indices = pickle.load(f)\n",
    "\n",
    "with open('indice/test_indices_auto_go.txt', 'rb') as f:\n",
    "    test_indices = pickle.load(f)\n",
    "\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\"\"\"\n",
    "dat_train = AutoDataset(\n",
    "    annotations_file=annotation,\n",
    "    file_dir=file_dir,\n",
    "    target_word=target_w,\n",
    "    transform=composed,\n",
    "    DType=\"train\"\n",
    ")\n",
    "dat_valid = AutoDataset(\n",
    "    annotations_file=annotation,\n",
    "    file_dir=file_dir,\n",
    "    target_word=target_w,\n",
    "    transform=composed,\n",
    "    DType=\"valid\"\n",
    ")\n",
    "dat_test = AutoDataset(\n",
    "    annotations_file=annotation,\n",
    "    file_dir=file_dir,\n",
    "    target_word=target_w,\n",
    "    transform=composed,\n",
    "    DType=\"test\"\n",
    ")\n",
    "\n",
    "batch_size= 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dat_train,\n",
    "                                           batch_size=batch_size)\n",
    "validation_loader = torch.utils.data.DataLoader(dat_valid,\n",
    "                                                batch_size=10)\n",
    "test_loader = torch.utils.data.DataLoader(dat_test,\n",
    "                                          batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if DType == \"train\":\n",
    "            self.file_labels = self.file_labels[10000:-10000]\n",
    "        elif DType == \"valid\":\n",
    "            self.file_labels = self.file_labels[:10000]\n",
    "        elif DType == \"test\":\n",
    "            self.file_labels = self.file_labels[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72641 9627 9648\n"
     ]
    }
   ],
   "source": [
    "file_labels = pd.read_csv(annotation)\n",
    "file_labels = file_labels[10000:-10000]\n",
    "no_target_labels = file_labels[~file_labels['fileName'].str.contains(target_w)]\n",
    "only_target_labels = file_labels[file_labels['fileName'].str.contains(target_w)]\n",
    "train_indices = random.sample(range(len(no_target_labels)), len(no_target_labels))\n",
    "\n",
    "file_labels = pd.read_csv(annotation)\n",
    "file_labels = file_labels[:10000]\n",
    "no_target_labels = file_labels[~file_labels['fileName'].str.contains(target_w)]\n",
    "only_target_labels = file_labels[file_labels['fileName'].str.contains(target_w)]\n",
    "val_indices = random.sample(range(len(no_target_labels)), len(no_target_labels))\n",
    "\n",
    "file_labels = pd.read_csv(annotation)\n",
    "file_labels = file_labels[-10000:]\n",
    "no_target_labels = file_labels[~file_labels['fileName'].str.contains(target_w)]\n",
    "only_target_labels = file_labels[file_labels['fileName'].str.contains(target_w)]\n",
    "test_indices = random.sample(range(len(no_target_labels)), len(no_target_labels))\n",
    "\n",
    "print(len(train_indices), len(val_indices), len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DOA(region_y, angle_y, region_p, angle_p, threshold, class_y, class_p):\n",
    "    at = get_at()\n",
    "    doa_err = 0.0\n",
    "    doa_acc = 0\n",
    "    residual = 0\n",
    "    bo = 0\n",
    "    count = 0\n",
    "    sig = nn.Sigmoid()\n",
    "    for ry, ay, rp, ap, cy, cp in zip(region_y, angle_y, region_p, angle_p, class_y, class_p):\n",
    "        if cy == 1: \n",
    "            doa_y = int(at[int(ry)] + ay[int(ry)].item() * 60)\n",
    "\n",
    "            rp_idx = int(torch.argmax(rp))\n",
    "            doa_p = int(at[rp_idx] + sig(ap[rp_idx]) * 60)\n",
    "            #print(doa_p)\n",
    "            \n",
    "            if doa_p > 360:\n",
    "                doa_p = doa_p - 360\n",
    "            elif doa_p < 0:\n",
    "                doa_p = 360 + doa_p\n",
    "\n",
    "            residual = abs(doa_y - doa_p)\n",
    "            if residual > 180:\n",
    "                residual = 360 - residual\n",
    "            doa_err += residual\n",
    "\n",
    "            if residual <= threshold:\n",
    "                bo = 1\n",
    "            else:\n",
    "                bo = 0\n",
    "            doa_acc += bo\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "    return doa_err, doa_acc, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "t = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")[:-3] + \"Drop\"\n",
    "weight_dir = 'weight/' + t\n",
    "sample_dir = 'samples/' + t\n",
    "\n",
    "os.mkdir(weight_dir)\n",
    "os.mkdir(sample_dir)\n",
    "os.mkdir(sample_dir + '_val')\n",
    "coarse_loss = nn.CrossEntropyLoss()\n",
    "fine_loss = nn.MSELoss(reduction=\"sum\")\n",
    "cls_loss = nn.BCELoss(reduction=\"sum\")\n",
    "ext_loss = nn.L1Loss(reduction=\"sum\")\n",
    "\n",
    "encoder_ch = [[6,32],[32,96],[96,192],[192,192]]\n",
    "net = KeywordSpeakerLocalizer(encoder_ch).cuda()\n",
    "learning_rate = 0.00001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,25], gamma=0.1)\n",
    "\n",
    "accuracy = 0.0\n",
    "loss = 0\n",
    "CoarW = 0.\n",
    "FineW = 0.\n",
    "ExtW = 0.\n",
    "ClsW = 1.\n",
    "\n",
    "coeff_total = CoarW + FineW + ExtW + ClsW\n",
    "CoarW = CoarW / coeff_total\n",
    "FineW = FineW / coeff_total\n",
    "ExtW = ExtW / coeff_total\n",
    "ClsW = ClsW / coeff_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrhwndqja\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rhwndqja/KP/runs/369i9a58\" target=\"_blank\">20230111-132Drop</a></strong> to <a href=\"https://wandb.ai/rhwndqja/KP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rhwndqja/KP/runs/369i9a58?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f296c2b18b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"KP\", \n",
    "    name=f\"{t}\", \n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"Multi-task\",\n",
    "        \"Overlap_ratio\": overlap,\n",
    "        \"SIR ratio\": sir,\n",
    "        \"coar fine cls ext\": [CoarW, FineW, ClsW, ExtW], \n",
    "        \"Date\": t + \"_new_go\",\n",
    "        \"epochs\": 30,\n",
    "        \"encoder_ch\": encoder_ch\n",
    "})\n",
    "#wandb.watch(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [320/72641]\tLoss: 0.7683\tLR: 0.000010\n",
      "Training Epoch: 1 [640/72641]\tLoss: 0.7698\tLR: 0.000010\n",
      "Training Epoch: 1 [960/72641]\tLoss: 0.7617\tLR: 0.000010\n",
      "Training Epoch: 1 [1280/72641]\tLoss: 0.7803\tLR: 0.000010\n",
      "Training Epoch: 1 [1600/72641]\tLoss: 0.7752\tLR: 0.000010\n",
      "Training Epoch: 1 [1920/72641]\tLoss: 0.7723\tLR: 0.000010\n",
      "Training Epoch: 1 [2240/72641]\tLoss: 0.7665\tLR: 0.000010\n",
      "Training Epoch: 1 [2560/72641]\tLoss: 0.7685\tLR: 0.000010\n",
      "Training Epoch: 1 [2880/72641]\tLoss: 0.7693\tLR: 0.000010\n",
      "Training Epoch: 1 [3200/72641]\tLoss: 0.7662\tLR: 0.000010\n",
      "Training Epoch: 1 [3520/72641]\tLoss: 0.7665\tLR: 0.000010\n",
      "Training Epoch: 1 [3840/72641]\tLoss: 0.7612\tLR: 0.000010\n",
      "Training Epoch: 1 [4160/72641]\tLoss: 0.7642\tLR: 0.000010\n",
      "Training Epoch: 1 [4480/72641]\tLoss: 0.7612\tLR: 0.000010\n",
      "Training Epoch: 1 [4800/72641]\tLoss: 0.7706\tLR: 0.000010\n",
      "Training Epoch: 1 [5120/72641]\tLoss: 0.7658\tLR: 0.000010\n",
      "Training Epoch: 1 [5440/72641]\tLoss: 0.7691\tLR: 0.000010\n",
      "Training Epoch: 1 [5760/72641]\tLoss: 0.7582\tLR: 0.000010\n",
      "Training Epoch: 1 [6080/72641]\tLoss: 0.7542\tLR: 0.000010\n",
      "Training Epoch: 1 [6400/72641]\tLoss: 0.7605\tLR: 0.000010\n",
      "Training Epoch: 1 [6720/72641]\tLoss: 0.7697\tLR: 0.000010\n",
      "Training Epoch: 1 [7040/72641]\tLoss: 0.7607\tLR: 0.000010\n",
      "Training Epoch: 1 [7360/72641]\tLoss: 0.7642\tLR: 0.000010\n",
      "Training Epoch: 1 [7680/72641]\tLoss: 0.7584\tLR: 0.000010\n",
      "Training Epoch: 1 [8000/72641]\tLoss: 0.7655\tLR: 0.000010\n",
      "Training Epoch: 1 [8320/72641]\tLoss: 0.7629\tLR: 0.000010\n",
      "Training Epoch: 1 [8640/72641]\tLoss: 0.7561\tLR: 0.000010\n",
      "Training Epoch: 1 [8960/72641]\tLoss: 0.7520\tLR: 0.000010\n",
      "Training Epoch: 1 [9280/72641]\tLoss: 0.7509\tLR: 0.000010\n",
      "Training Epoch: 1 [9600/72641]\tLoss: 0.7528\tLR: 0.000010\n",
      "Training Epoch: 1 [9920/72641]\tLoss: 0.7610\tLR: 0.000010\n",
      "Training Epoch: 1 [10240/72641]\tLoss: 0.7494\tLR: 0.000010\n",
      "Training Epoch: 1 [10560/72641]\tLoss: 0.7588\tLR: 0.000010\n",
      "Training Epoch: 1 [10880/72641]\tLoss: 0.7557\tLR: 0.000010\n",
      "Training Epoch: 1 [11200/72641]\tLoss: 0.7587\tLR: 0.000010\n",
      "Training Epoch: 1 [11520/72641]\tLoss: 0.7584\tLR: 0.000010\n",
      "Training Epoch: 1 [11840/72641]\tLoss: 0.7506\tLR: 0.000010\n",
      "Training Epoch: 1 [12160/72641]\tLoss: 0.7600\tLR: 0.000010\n",
      "Training Epoch: 1 [12480/72641]\tLoss: 0.7622\tLR: 0.000010\n",
      "Training Epoch: 1 [12800/72641]\tLoss: 0.7496\tLR: 0.000010\n",
      "Training Epoch: 1 [13120/72641]\tLoss: 0.7507\tLR: 0.000010\n",
      "Training Epoch: 1 [13440/72641]\tLoss: 0.7517\tLR: 0.000010\n",
      "Training Epoch: 1 [13760/72641]\tLoss: 0.7492\tLR: 0.000010\n",
      "Training Epoch: 1 [14080/72641]\tLoss: 0.7544\tLR: 0.000010\n",
      "Training Epoch: 1 [14400/72641]\tLoss: 0.7521\tLR: 0.000010\n",
      "Training Epoch: 1 [14720/72641]\tLoss: 0.7443\tLR: 0.000010\n",
      "Training Epoch: 1 [15040/72641]\tLoss: 0.7555\tLR: 0.000010\n",
      "Training Epoch: 1 [15360/72641]\tLoss: 0.7609\tLR: 0.000010\n",
      "Training Epoch: 1 [15680/72641]\tLoss: 0.7504\tLR: 0.000010\n",
      "Training Epoch: 1 [16000/72641]\tLoss: 0.7571\tLR: 0.000010\n",
      "Training Epoch: 1 [16320/72641]\tLoss: 0.7511\tLR: 0.000010\n",
      "Training Epoch: 1 [16640/72641]\tLoss: 0.7609\tLR: 0.000010\n",
      "Training Epoch: 1 [16960/72641]\tLoss: 0.7468\tLR: 0.000010\n",
      "Training Epoch: 1 [17280/72641]\tLoss: 0.7460\tLR: 0.000010\n",
      "Training Epoch: 1 [17600/72641]\tLoss: 0.7479\tLR: 0.000010\n",
      "Training Epoch: 1 [17920/72641]\tLoss: 0.7576\tLR: 0.000010\n",
      "Training Epoch: 1 [18240/72641]\tLoss: 0.7561\tLR: 0.000010\n",
      "Training Epoch: 1 [18560/72641]\tLoss: 0.7519\tLR: 0.000010\n",
      "Training Epoch: 1 [18880/72641]\tLoss: 0.7464\tLR: 0.000010\n",
      "Training Epoch: 1 [19200/72641]\tLoss: 0.7570\tLR: 0.000010\n",
      "Training Epoch: 1 [19520/72641]\tLoss: 0.7474\tLR: 0.000010\n",
      "Training Epoch: 1 [19840/72641]\tLoss: 0.7552\tLR: 0.000010\n",
      "Training Epoch: 1 [20160/72641]\tLoss: 0.7533\tLR: 0.000010\n",
      "Training Epoch: 1 [20480/72641]\tLoss: 0.7368\tLR: 0.000010\n",
      "Training Epoch: 1 [20800/72641]\tLoss: 0.7458\tLR: 0.000010\n",
      "Training Epoch: 1 [21120/72641]\tLoss: 0.7380\tLR: 0.000010\n",
      "Training Epoch: 1 [21440/72641]\tLoss: 0.7394\tLR: 0.000010\n",
      "Training Epoch: 1 [21760/72641]\tLoss: 0.7294\tLR: 0.000010\n",
      "Training Epoch: 1 [22080/72641]\tLoss: 0.7509\tLR: 0.000010\n",
      "Training Epoch: 1 [22400/72641]\tLoss: 0.7577\tLR: 0.000010\n",
      "Training Epoch: 1 [22720/72641]\tLoss: 0.7432\tLR: 0.000010\n",
      "Training Epoch: 1 [23040/72641]\tLoss: 0.7365\tLR: 0.000010\n",
      "Training Epoch: 1 [23360/72641]\tLoss: 0.7445\tLR: 0.000010\n",
      "Training Epoch: 1 [23680/72641]\tLoss: 0.7290\tLR: 0.000010\n",
      "Training Epoch: 1 [24000/72641]\tLoss: 0.7346\tLR: 0.000010\n",
      "Training Epoch: 1 [24320/72641]\tLoss: 0.7357\tLR: 0.000010\n",
      "Training Epoch: 1 [24640/72641]\tLoss: 0.7384\tLR: 0.000010\n",
      "Training Epoch: 1 [24960/72641]\tLoss: 0.7468\tLR: 0.000010\n",
      "Training Epoch: 1 [25280/72641]\tLoss: 0.7457\tLR: 0.000010\n",
      "Training Epoch: 1 [25600/72641]\tLoss: 0.7416\tLR: 0.000010\n",
      "Training Epoch: 1 [25920/72641]\tLoss: 0.7354\tLR: 0.000010\n",
      "Training Epoch: 1 [26240/72641]\tLoss: 0.7633\tLR: 0.000010\n",
      "Training Epoch: 1 [26560/72641]\tLoss: 0.7429\tLR: 0.000010\n",
      "Training Epoch: 1 [26880/72641]\tLoss: 0.7496\tLR: 0.000010\n",
      "Training Epoch: 1 [27200/72641]\tLoss: 0.7373\tLR: 0.000010\n",
      "Training Epoch: 1 [27520/72641]\tLoss: 0.7423\tLR: 0.000010\n",
      "Training Epoch: 1 [27840/72641]\tLoss: 0.7428\tLR: 0.000010\n",
      "Training Epoch: 1 [28160/72641]\tLoss: 0.7534\tLR: 0.000010\n",
      "Training Epoch: 1 [28480/72641]\tLoss: 0.7402\tLR: 0.000010\n",
      "Training Epoch: 1 [28800/72641]\tLoss: 0.7462\tLR: 0.000010\n",
      "Training Epoch: 1 [29120/72641]\tLoss: 0.7719\tLR: 0.000010\n",
      "Training Epoch: 1 [29440/72641]\tLoss: 0.7304\tLR: 0.000010\n",
      "Training Epoch: 1 [29760/72641]\tLoss: 0.7363\tLR: 0.000010\n",
      "Training Epoch: 1 [30080/72641]\tLoss: 0.7277\tLR: 0.000010\n",
      "Training Epoch: 1 [30400/72641]\tLoss: 0.7487\tLR: 0.000010\n",
      "Training Epoch: 1 [30720/72641]\tLoss: 0.7505\tLR: 0.000010\n",
      "Training Epoch: 1 [31040/72641]\tLoss: 0.7426\tLR: 0.000010\n",
      "Training Epoch: 1 [31360/72641]\tLoss: 0.7264\tLR: 0.000010\n",
      "Training Epoch: 1 [31680/72641]\tLoss: 0.7504\tLR: 0.000010\n",
      "Training Epoch: 1 [32000/72641]\tLoss: 0.7510\tLR: 0.000010\n",
      "Training Epoch: 1 [32320/72641]\tLoss: 0.7375\tLR: 0.000010\n",
      "Training Epoch: 1 [32640/72641]\tLoss: 0.7285\tLR: 0.000010\n",
      "Training Epoch: 1 [32960/72641]\tLoss: 0.7310\tLR: 0.000010\n",
      "Training Epoch: 1 [33280/72641]\tLoss: 0.7377\tLR: 0.000010\n",
      "Training Epoch: 1 [33600/72641]\tLoss: 0.7363\tLR: 0.000010\n",
      "Training Epoch: 1 [33920/72641]\tLoss: 0.7386\tLR: 0.000010\n",
      "Training Epoch: 1 [34240/72641]\tLoss: 0.7534\tLR: 0.000010\n",
      "Training Epoch: 1 [34560/72641]\tLoss: 0.7586\tLR: 0.000010\n",
      "Training Epoch: 1 [34880/72641]\tLoss: 0.7463\tLR: 0.000010\n",
      "Training Epoch: 1 [35200/72641]\tLoss: 0.7348\tLR: 0.000010\n",
      "Training Epoch: 1 [35520/72641]\tLoss: 0.7172\tLR: 0.000010\n",
      "Training Epoch: 1 [35840/72641]\tLoss: 0.7463\tLR: 0.000010\n",
      "Training Epoch: 1 [36160/72641]\tLoss: 0.7504\tLR: 0.000010\n",
      "Training Epoch: 1 [36480/72641]\tLoss: 0.7220\tLR: 0.000010\n",
      "Training Epoch: 1 [36800/72641]\tLoss: 0.7316\tLR: 0.000010\n",
      "Training Epoch: 1 [37120/72641]\tLoss: 0.7275\tLR: 0.000010\n",
      "Training Epoch: 1 [37440/72641]\tLoss: 0.7308\tLR: 0.000010\n",
      "Training Epoch: 1 [37760/72641]\tLoss: 0.7380\tLR: 0.000010\n",
      "Training Epoch: 1 [38080/72641]\tLoss: 0.7372\tLR: 0.000010\n",
      "Training Epoch: 1 [38400/72641]\tLoss: 0.7487\tLR: 0.000010\n",
      "Training Epoch: 1 [38720/72641]\tLoss: 0.7080\tLR: 0.000010\n",
      "Training Epoch: 1 [39040/72641]\tLoss: 0.7173\tLR: 0.000010\n",
      "Training Epoch: 1 [39360/72641]\tLoss: 0.7247\tLR: 0.000010\n",
      "Training Epoch: 1 [39680/72641]\tLoss: 0.6998\tLR: 0.000010\n",
      "Training Epoch: 1 [40000/72641]\tLoss: 0.7310\tLR: 0.000010\n",
      "Training Epoch: 1 [40320/72641]\tLoss: 0.7454\tLR: 0.000010\n",
      "Training Epoch: 1 [40640/72641]\tLoss: 0.7287\tLR: 0.000010\n",
      "Training Epoch: 1 [40960/72641]\tLoss: 0.7307\tLR: 0.000010\n",
      "Training Epoch: 1 [41280/72641]\tLoss: 0.7466\tLR: 0.000010\n",
      "Training Epoch: 1 [41600/72641]\tLoss: 0.7283\tLR: 0.000010\n",
      "Training Epoch: 1 [41920/72641]\tLoss: 0.7287\tLR: 0.000010\n",
      "Training Epoch: 1 [42240/72641]\tLoss: 0.7342\tLR: 0.000010\n",
      "Training Epoch: 1 [42560/72641]\tLoss: 0.7450\tLR: 0.000010\n",
      "Training Epoch: 1 [42880/72641]\tLoss: 0.7314\tLR: 0.000010\n",
      "Training Epoch: 1 [43200/72641]\tLoss: 0.7228\tLR: 0.000010\n",
      "Training Epoch: 1 [43520/72641]\tLoss: 0.7230\tLR: 0.000010\n",
      "Training Epoch: 1 [43840/72641]\tLoss: 0.7422\tLR: 0.000010\n",
      "Training Epoch: 1 [44160/72641]\tLoss: 0.7334\tLR: 0.000010\n",
      "Training Epoch: 1 [44480/72641]\tLoss: 0.7328\tLR: 0.000010\n",
      "Training Epoch: 1 [44800/72641]\tLoss: 0.7386\tLR: 0.000010\n",
      "Training Epoch: 1 [45120/72641]\tLoss: 0.7183\tLR: 0.000010\n",
      "Training Epoch: 1 [45440/72641]\tLoss: 0.7307\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [45760/72641]\tLoss: 0.7220\tLR: 0.000010\n",
      "Training Epoch: 1 [46080/72641]\tLoss: 0.7262\tLR: 0.000010\n",
      "Training Epoch: 1 [46400/72641]\tLoss: 0.7408\tLR: 0.000010\n",
      "Training Epoch: 1 [46720/72641]\tLoss: 0.7223\tLR: 0.000010\n",
      "Training Epoch: 1 [47040/72641]\tLoss: 0.7236\tLR: 0.000010\n",
      "Training Epoch: 1 [47360/72641]\tLoss: 0.7250\tLR: 0.000010\n",
      "Training Epoch: 1 [47680/72641]\tLoss: 0.7323\tLR: 0.000010\n",
      "Training Epoch: 1 [48000/72641]\tLoss: 0.7175\tLR: 0.000010\n",
      "Training Epoch: 1 [48320/72641]\tLoss: 0.7389\tLR: 0.000010\n",
      "Training Epoch: 1 [48640/72641]\tLoss: 0.7262\tLR: 0.000010\n",
      "Training Epoch: 1 [48960/72641]\tLoss: 0.7542\tLR: 0.000010\n",
      "Training Epoch: 1 [49280/72641]\tLoss: 0.7487\tLR: 0.000010\n",
      "Training Epoch: 1 [49600/72641]\tLoss: 0.7490\tLR: 0.000010\n",
      "Training Epoch: 1 [49920/72641]\tLoss: 0.7129\tLR: 0.000010\n",
      "Training Epoch: 1 [50240/72641]\tLoss: 0.7275\tLR: 0.000010\n",
      "Training Epoch: 1 [50560/72641]\tLoss: 0.7153\tLR: 0.000010\n",
      "Training Epoch: 1 [50880/72641]\tLoss: 0.7639\tLR: 0.000010\n",
      "Training Epoch: 1 [51200/72641]\tLoss: 0.7260\tLR: 0.000010\n",
      "Training Epoch: 1 [51520/72641]\tLoss: 0.7449\tLR: 0.000010\n",
      "Training Epoch: 1 [51840/72641]\tLoss: 0.7312\tLR: 0.000010\n",
      "Training Epoch: 1 [52160/72641]\tLoss: 0.7289\tLR: 0.000010\n",
      "Training Epoch: 1 [52480/72641]\tLoss: 0.7493\tLR: 0.000010\n",
      "Training Epoch: 1 [52800/72641]\tLoss: 0.7407\tLR: 0.000010\n",
      "Training Epoch: 1 [53120/72641]\tLoss: 0.7234\tLR: 0.000010\n",
      "Training Epoch: 1 [53440/72641]\tLoss: 0.7009\tLR: 0.000010\n",
      "Training Epoch: 1 [53760/72641]\tLoss: 0.7525\tLR: 0.000010\n",
      "Training Epoch: 1 [54080/72641]\tLoss: 0.7313\tLR: 0.000010\n",
      "Training Epoch: 1 [54400/72641]\tLoss: 0.7406\tLR: 0.000010\n",
      "Training Epoch: 1 [54720/72641]\tLoss: 0.7353\tLR: 0.000010\n",
      "Training Epoch: 1 [55040/72641]\tLoss: 0.7233\tLR: 0.000010\n",
      "Training Epoch: 1 [55360/72641]\tLoss: 0.7196\tLR: 0.000010\n",
      "Training Epoch: 1 [55680/72641]\tLoss: 0.7171\tLR: 0.000010\n",
      "Training Epoch: 1 [56000/72641]\tLoss: 0.7294\tLR: 0.000010\n",
      "Training Epoch: 1 [56320/72641]\tLoss: 0.7423\tLR: 0.000010\n",
      "Training Epoch: 1 [56640/72641]\tLoss: 0.7377\tLR: 0.000010\n",
      "Training Epoch: 1 [56960/72641]\tLoss: 0.7386\tLR: 0.000010\n",
      "Training Epoch: 1 [57280/72641]\tLoss: 0.7170\tLR: 0.000010\n",
      "Training Epoch: 1 [57600/72641]\tLoss: 0.7305\tLR: 0.000010\n",
      "Training Epoch: 1 [57920/72641]\tLoss: 0.7293\tLR: 0.000010\n",
      "Training Epoch: 1 [58240/72641]\tLoss: 0.7248\tLR: 0.000010\n",
      "Training Epoch: 1 [58560/72641]\tLoss: 0.7142\tLR: 0.000010\n",
      "Training Epoch: 1 [58880/72641]\tLoss: 0.7313\tLR: 0.000010\n",
      "Training Epoch: 1 [59200/72641]\tLoss: 0.7438\tLR: 0.000010\n",
      "Training Epoch: 1 [59520/72641]\tLoss: 0.7371\tLR: 0.000010\n",
      "Training Epoch: 1 [59840/72641]\tLoss: 0.7267\tLR: 0.000010\n",
      "Training Epoch: 1 [60160/72641]\tLoss: 0.7429\tLR: 0.000010\n",
      "Training Epoch: 1 [60480/72641]\tLoss: 0.7273\tLR: 0.000010\n",
      "Training Epoch: 1 [60800/72641]\tLoss: 0.7133\tLR: 0.000010\n",
      "Training Epoch: 1 [61120/72641]\tLoss: 0.7243\tLR: 0.000010\n",
      "Training Epoch: 1 [61440/72641]\tLoss: 0.7218\tLR: 0.000010\n",
      "Training Epoch: 1 [61760/72641]\tLoss: 0.7077\tLR: 0.000010\n",
      "Training Epoch: 1 [62080/72641]\tLoss: 0.7392\tLR: 0.000010\n",
      "Training Epoch: 1 [62400/72641]\tLoss: 0.7232\tLR: 0.000010\n",
      "Training Epoch: 1 [62720/72641]\tLoss: 0.7095\tLR: 0.000010\n",
      "Training Epoch: 1 [63040/72641]\tLoss: 0.7128\tLR: 0.000010\n",
      "Training Epoch: 1 [63360/72641]\tLoss: 0.7293\tLR: 0.000010\n",
      "Training Epoch: 1 [63680/72641]\tLoss: 0.7128\tLR: 0.000010\n",
      "Training Epoch: 1 [64000/72641]\tLoss: 0.7243\tLR: 0.000010\n",
      "Training Epoch: 1 [64320/72641]\tLoss: 0.7350\tLR: 0.000010\n",
      "Training Epoch: 1 [64640/72641]\tLoss: 0.7263\tLR: 0.000010\n",
      "Training Epoch: 1 [64960/72641]\tLoss: 0.7233\tLR: 0.000010\n",
      "Training Epoch: 1 [65280/72641]\tLoss: 0.7408\tLR: 0.000010\n",
      "Training Epoch: 1 [65600/72641]\tLoss: 0.7127\tLR: 0.000010\n",
      "Training Epoch: 1 [65920/72641]\tLoss: 0.7359\tLR: 0.000010\n",
      "Training Epoch: 1 [66240/72641]\tLoss: 0.7236\tLR: 0.000010\n",
      "Training Epoch: 1 [66560/72641]\tLoss: 0.7034\tLR: 0.000010\n",
      "Training Epoch: 1 [66880/72641]\tLoss: 0.7162\tLR: 0.000010\n",
      "Training Epoch: 1 [67200/72641]\tLoss: 0.7045\tLR: 0.000010\n",
      "Training Epoch: 1 [67520/72641]\tLoss: 0.7359\tLR: 0.000010\n",
      "Training Epoch: 1 [67840/72641]\tLoss: 0.6974\tLR: 0.000010\n",
      "Training Epoch: 1 [68160/72641]\tLoss: 0.7378\tLR: 0.000010\n",
      "Training Epoch: 1 [68480/72641]\tLoss: 0.7077\tLR: 0.000010\n",
      "Training Epoch: 1 [68800/72641]\tLoss: 0.6991\tLR: 0.000010\n",
      "Training Epoch: 1 [69120/72641]\tLoss: 0.7325\tLR: 0.000010\n",
      "Training Epoch: 1 [69440/72641]\tLoss: 0.6886\tLR: 0.000010\n",
      "Training Epoch: 1 [69760/72641]\tLoss: 0.7168\tLR: 0.000010\n",
      "Training Epoch: 1 [70080/72641]\tLoss: 0.7053\tLR: 0.000010\n",
      "Training Epoch: 1 [70400/72641]\tLoss: 0.7288\tLR: 0.000010\n",
      "Training Epoch: 1 [70720/72641]\tLoss: 0.7084\tLR: 0.000010\n",
      "Training Epoch: 1 [71040/72641]\tLoss: 0.7265\tLR: 0.000010\n",
      "Training Epoch: 1 [71360/72641]\tLoss: 0.7173\tLR: 0.000010\n",
      "Training Epoch: 1 [71680/72641]\tLoss: 0.7245\tLR: 0.000010\n",
      "Training Epoch: 1 [72000/72641]\tLoss: 0.7164\tLR: 0.000010\n",
      "Training Epoch: 1 [72320/72641]\tLoss: 0.7286\tLR: 0.000010\n",
      "Training Epoch: 1 [72640/72641]\tLoss: 0.6928\tLR: 0.000010\n",
      "Val Result: Acc: 0.1467, C_ACC: 0.6492, DOA: 88.9189, ACC_k: 0.0917\n",
      "ext:0.0, cls:0.642055, coar:0.0, fine:0.0,\n",
      "Training Epoch: 2 [320/72641]\tLoss: 0.7375\tLR: 0.000010\n",
      "Training Epoch: 2 [640/72641]\tLoss: 0.7245\tLR: 0.000010\n",
      "Training Epoch: 2 [960/72641]\tLoss: 0.7390\tLR: 0.000010\n",
      "Training Epoch: 2 [1280/72641]\tLoss: 0.6968\tLR: 0.000010\n",
      "Training Epoch: 2 [1600/72641]\tLoss: 0.7273\tLR: 0.000010\n",
      "Training Epoch: 2 [1920/72641]\tLoss: 0.7463\tLR: 0.000010\n",
      "Training Epoch: 2 [2240/72641]\tLoss: 0.7138\tLR: 0.000010\n",
      "Training Epoch: 2 [2560/72641]\tLoss: 0.7354\tLR: 0.000010\n",
      "Training Epoch: 2 [2880/72641]\tLoss: 0.7229\tLR: 0.000010\n",
      "Training Epoch: 2 [3200/72641]\tLoss: 0.7315\tLR: 0.000010\n",
      "Training Epoch: 2 [3520/72641]\tLoss: 0.7282\tLR: 0.000010\n",
      "Training Epoch: 2 [3840/72641]\tLoss: 0.7144\tLR: 0.000010\n",
      "Training Epoch: 2 [4160/72641]\tLoss: 0.7348\tLR: 0.000010\n",
      "Training Epoch: 2 [4480/72641]\tLoss: 0.7167\tLR: 0.000010\n",
      "Training Epoch: 2 [4800/72641]\tLoss: 0.7324\tLR: 0.000010\n",
      "Training Epoch: 2 [5120/72641]\tLoss: 0.7099\tLR: 0.000010\n",
      "Training Epoch: 2 [5440/72641]\tLoss: 0.7342\tLR: 0.000010\n",
      "Training Epoch: 2 [5760/72641]\tLoss: 0.7085\tLR: 0.000010\n",
      "Training Epoch: 2 [6080/72641]\tLoss: 0.6963\tLR: 0.000010\n",
      "Training Epoch: 2 [6400/72641]\tLoss: 0.7196\tLR: 0.000010\n",
      "Training Epoch: 2 [6720/72641]\tLoss: 0.7202\tLR: 0.000010\n",
      "Training Epoch: 2 [7040/72641]\tLoss: 0.7196\tLR: 0.000010\n",
      "Training Epoch: 2 [7360/72641]\tLoss: 0.7358\tLR: 0.000010\n",
      "Training Epoch: 2 [7680/72641]\tLoss: 0.7249\tLR: 0.000010\n",
      "Training Epoch: 2 [8000/72641]\tLoss: 0.6830\tLR: 0.000010\n",
      "Training Epoch: 2 [8320/72641]\tLoss: 0.7304\tLR: 0.000010\n",
      "Training Epoch: 2 [8640/72641]\tLoss: 0.7198\tLR: 0.000010\n",
      "Training Epoch: 2 [8960/72641]\tLoss: 0.6733\tLR: 0.000010\n",
      "Training Epoch: 2 [9280/72641]\tLoss: 0.7149\tLR: 0.000010\n",
      "Training Epoch: 2 [9600/72641]\tLoss: 0.7154\tLR: 0.000010\n",
      "Training Epoch: 2 [9920/72641]\tLoss: 0.7267\tLR: 0.000010\n",
      "Training Epoch: 2 [10240/72641]\tLoss: 0.7231\tLR: 0.000010\n",
      "Training Epoch: 2 [10560/72641]\tLoss: 0.6820\tLR: 0.000010\n",
      "Training Epoch: 2 [10880/72641]\tLoss: 0.7360\tLR: 0.000010\n",
      "Training Epoch: 2 [11200/72641]\tLoss: 0.7475\tLR: 0.000010\n",
      "Training Epoch: 2 [11520/72641]\tLoss: 0.7253\tLR: 0.000010\n",
      "Training Epoch: 2 [11840/72641]\tLoss: 0.7181\tLR: 0.000010\n",
      "Training Epoch: 2 [12160/72641]\tLoss: 0.7082\tLR: 0.000010\n",
      "Training Epoch: 2 [12480/72641]\tLoss: 0.7442\tLR: 0.000010\n",
      "Training Epoch: 2 [12800/72641]\tLoss: 0.7113\tLR: 0.000010\n",
      "Training Epoch: 2 [13120/72641]\tLoss: 0.7152\tLR: 0.000010\n",
      "Training Epoch: 2 [13440/72641]\tLoss: 0.7125\tLR: 0.000010\n",
      "Training Epoch: 2 [13760/72641]\tLoss: 0.7305\tLR: 0.000010\n",
      "Training Epoch: 2 [14080/72641]\tLoss: 0.7026\tLR: 0.000010\n",
      "Training Epoch: 2 [14400/72641]\tLoss: 0.7220\tLR: 0.000010\n",
      "Training Epoch: 2 [14720/72641]\tLoss: 0.7239\tLR: 0.000010\n",
      "Training Epoch: 2 [15040/72641]\tLoss: 0.7190\tLR: 0.000010\n",
      "Training Epoch: 2 [15360/72641]\tLoss: 0.7100\tLR: 0.000010\n",
      "Training Epoch: 2 [15680/72641]\tLoss: 0.7501\tLR: 0.000010\n",
      "Training Epoch: 2 [16000/72641]\tLoss: 0.7325\tLR: 0.000010\n",
      "Training Epoch: 2 [16320/72641]\tLoss: 0.7033\tLR: 0.000010\n",
      "Training Epoch: 2 [16640/72641]\tLoss: 0.7148\tLR: 0.000010\n",
      "Training Epoch: 2 [16960/72641]\tLoss: 0.7094\tLR: 0.000010\n",
      "Training Epoch: 2 [17280/72641]\tLoss: 0.7071\tLR: 0.000010\n",
      "Training Epoch: 2 [17600/72641]\tLoss: 0.7137\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [17920/72641]\tLoss: 0.7164\tLR: 0.000010\n",
      "Training Epoch: 2 [18240/72641]\tLoss: 0.7258\tLR: 0.000010\n",
      "Training Epoch: 2 [18560/72641]\tLoss: 0.7345\tLR: 0.000010\n",
      "Training Epoch: 2 [18880/72641]\tLoss: 0.6957\tLR: 0.000010\n",
      "Training Epoch: 2 [19200/72641]\tLoss: 0.6954\tLR: 0.000010\n",
      "Training Epoch: 2 [19520/72641]\tLoss: 0.7296\tLR: 0.000010\n",
      "Training Epoch: 2 [19840/72641]\tLoss: 0.7324\tLR: 0.000010\n",
      "Training Epoch: 2 [20160/72641]\tLoss: 0.6985\tLR: 0.000010\n",
      "Training Epoch: 2 [20480/72641]\tLoss: 0.7151\tLR: 0.000010\n",
      "Training Epoch: 2 [20800/72641]\tLoss: 0.7176\tLR: 0.000010\n",
      "Training Epoch: 2 [21120/72641]\tLoss: 0.7397\tLR: 0.000010\n",
      "Training Epoch: 2 [21440/72641]\tLoss: 0.7357\tLR: 0.000010\n",
      "Training Epoch: 2 [21760/72641]\tLoss: 0.7147\tLR: 0.000010\n",
      "Training Epoch: 2 [22080/72641]\tLoss: 0.7200\tLR: 0.000010\n",
      "Training Epoch: 2 [22400/72641]\tLoss: 0.7006\tLR: 0.000010\n",
      "Training Epoch: 2 [22720/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 2 [23040/72641]\tLoss: 0.6926\tLR: 0.000010\n",
      "Training Epoch: 2 [23360/72641]\tLoss: 0.7070\tLR: 0.000010\n",
      "Training Epoch: 2 [23680/72641]\tLoss: 0.7003\tLR: 0.000010\n",
      "Training Epoch: 2 [24000/72641]\tLoss: 0.7136\tLR: 0.000010\n",
      "Training Epoch: 2 [24320/72641]\tLoss: 0.6815\tLR: 0.000010\n",
      "Training Epoch: 2 [24640/72641]\tLoss: 0.7094\tLR: 0.000010\n",
      "Training Epoch: 2 [24960/72641]\tLoss: 0.7249\tLR: 0.000010\n",
      "Training Epoch: 2 [25280/72641]\tLoss: 0.7036\tLR: 0.000010\n",
      "Training Epoch: 2 [25600/72641]\tLoss: 0.7225\tLR: 0.000010\n",
      "Training Epoch: 2 [25920/72641]\tLoss: 0.6851\tLR: 0.000010\n",
      "Training Epoch: 2 [26240/72641]\tLoss: 0.7385\tLR: 0.000010\n",
      "Training Epoch: 2 [26560/72641]\tLoss: 0.7113\tLR: 0.000010\n",
      "Training Epoch: 2 [26880/72641]\tLoss: 0.7017\tLR: 0.000010\n",
      "Training Epoch: 2 [27200/72641]\tLoss: 0.6821\tLR: 0.000010\n",
      "Training Epoch: 2 [27520/72641]\tLoss: 0.7268\tLR: 0.000010\n",
      "Training Epoch: 2 [27840/72641]\tLoss: 0.6666\tLR: 0.000010\n",
      "Training Epoch: 2 [28160/72641]\tLoss: 0.7260\tLR: 0.000010\n",
      "Training Epoch: 2 [28480/72641]\tLoss: 0.7143\tLR: 0.000010\n",
      "Training Epoch: 2 [28800/72641]\tLoss: 0.6964\tLR: 0.000010\n",
      "Training Epoch: 2 [29120/72641]\tLoss: 0.7540\tLR: 0.000010\n",
      "Training Epoch: 2 [29440/72641]\tLoss: 0.6993\tLR: 0.000010\n",
      "Training Epoch: 2 [29760/72641]\tLoss: 0.7221\tLR: 0.000010\n",
      "Training Epoch: 2 [30080/72641]\tLoss: 0.7245\tLR: 0.000010\n",
      "Training Epoch: 2 [30400/72641]\tLoss: 0.7139\tLR: 0.000010\n",
      "Training Epoch: 2 [30720/72641]\tLoss: 0.7171\tLR: 0.000010\n",
      "Training Epoch: 2 [31040/72641]\tLoss: 0.7075\tLR: 0.000010\n",
      "Training Epoch: 2 [31360/72641]\tLoss: 0.7035\tLR: 0.000010\n",
      "Training Epoch: 2 [31680/72641]\tLoss: 0.7112\tLR: 0.000010\n",
      "Training Epoch: 2 [32000/72641]\tLoss: 0.7008\tLR: 0.000010\n",
      "Training Epoch: 2 [32320/72641]\tLoss: 0.6954\tLR: 0.000010\n",
      "Training Epoch: 2 [32640/72641]\tLoss: 0.6993\tLR: 0.000010\n",
      "Training Epoch: 2 [32960/72641]\tLoss: 0.6759\tLR: 0.000010\n",
      "Training Epoch: 2 [33280/72641]\tLoss: 0.7147\tLR: 0.000010\n",
      "Training Epoch: 2 [33600/72641]\tLoss: 0.6789\tLR: 0.000010\n",
      "Training Epoch: 2 [33920/72641]\tLoss: 0.7361\tLR: 0.000010\n",
      "Training Epoch: 2 [34240/72641]\tLoss: 0.7012\tLR: 0.000010\n",
      "Training Epoch: 2 [34560/72641]\tLoss: 0.7276\tLR: 0.000010\n",
      "Training Epoch: 2 [34880/72641]\tLoss: 0.7161\tLR: 0.000010\n",
      "Training Epoch: 2 [35200/72641]\tLoss: 0.7274\tLR: 0.000010\n",
      "Training Epoch: 2 [35520/72641]\tLoss: 0.7115\tLR: 0.000010\n",
      "Training Epoch: 2 [35840/72641]\tLoss: 0.7007\tLR: 0.000010\n",
      "Training Epoch: 2 [36160/72641]\tLoss: 0.6989\tLR: 0.000010\n",
      "Training Epoch: 2 [36480/72641]\tLoss: 0.7266\tLR: 0.000010\n",
      "Training Epoch: 2 [36800/72641]\tLoss: 0.7032\tLR: 0.000010\n",
      "Training Epoch: 2 [37120/72641]\tLoss: 0.7322\tLR: 0.000010\n",
      "Training Epoch: 2 [37440/72641]\tLoss: 0.7385\tLR: 0.000010\n",
      "Training Epoch: 2 [37760/72641]\tLoss: 0.6963\tLR: 0.000010\n",
      "Training Epoch: 2 [38080/72641]\tLoss: 0.6817\tLR: 0.000010\n",
      "Training Epoch: 2 [38400/72641]\tLoss: 0.6988\tLR: 0.000010\n",
      "Training Epoch: 2 [38720/72641]\tLoss: 0.7210\tLR: 0.000010\n",
      "Training Epoch: 2 [39040/72641]\tLoss: 0.6942\tLR: 0.000010\n",
      "Training Epoch: 2 [39360/72641]\tLoss: 0.6911\tLR: 0.000010\n",
      "Training Epoch: 2 [39680/72641]\tLoss: 0.6689\tLR: 0.000010\n",
      "Training Epoch: 2 [40000/72641]\tLoss: 0.7385\tLR: 0.000010\n",
      "Training Epoch: 2 [40320/72641]\tLoss: 0.7071\tLR: 0.000010\n",
      "Training Epoch: 2 [40640/72641]\tLoss: 0.7231\tLR: 0.000010\n",
      "Training Epoch: 2 [40960/72641]\tLoss: 0.6941\tLR: 0.000010\n",
      "Training Epoch: 2 [41280/72641]\tLoss: 0.7179\tLR: 0.000010\n",
      "Training Epoch: 2 [41600/72641]\tLoss: 0.6928\tLR: 0.000010\n",
      "Training Epoch: 2 [41920/72641]\tLoss: 0.6980\tLR: 0.000010\n",
      "Training Epoch: 2 [42240/72641]\tLoss: 0.7236\tLR: 0.000010\n",
      "Training Epoch: 2 [42560/72641]\tLoss: 0.6966\tLR: 0.000010\n",
      "Training Epoch: 2 [42880/72641]\tLoss: 0.7237\tLR: 0.000010\n",
      "Training Epoch: 2 [43200/72641]\tLoss: 0.6744\tLR: 0.000010\n",
      "Training Epoch: 2 [43520/72641]\tLoss: 0.7006\tLR: 0.000010\n",
      "Training Epoch: 2 [43840/72641]\tLoss: 0.6830\tLR: 0.000010\n",
      "Training Epoch: 2 [44160/72641]\tLoss: 0.7110\tLR: 0.000010\n",
      "Training Epoch: 2 [44480/72641]\tLoss: 0.7050\tLR: 0.000010\n",
      "Training Epoch: 2 [44800/72641]\tLoss: 0.7289\tLR: 0.000010\n",
      "Training Epoch: 2 [45120/72641]\tLoss: 0.7179\tLR: 0.000010\n",
      "Training Epoch: 2 [45440/72641]\tLoss: 0.6905\tLR: 0.000010\n",
      "Training Epoch: 2 [45760/72641]\tLoss: 0.6871\tLR: 0.000010\n",
      "Training Epoch: 2 [46080/72641]\tLoss: 0.7404\tLR: 0.000010\n",
      "Training Epoch: 2 [46400/72641]\tLoss: 0.7034\tLR: 0.000010\n",
      "Training Epoch: 2 [46720/72641]\tLoss: 0.7011\tLR: 0.000010\n",
      "Training Epoch: 2 [47040/72641]\tLoss: 0.7178\tLR: 0.000010\n",
      "Training Epoch: 2 [47360/72641]\tLoss: 0.6956\tLR: 0.000010\n",
      "Training Epoch: 2 [47680/72641]\tLoss: 0.6995\tLR: 0.000010\n",
      "Training Epoch: 2 [48000/72641]\tLoss: 0.7239\tLR: 0.000010\n",
      "Training Epoch: 2 [48320/72641]\tLoss: 0.7213\tLR: 0.000010\n",
      "Training Epoch: 2 [48640/72641]\tLoss: 0.7407\tLR: 0.000010\n",
      "Training Epoch: 2 [48960/72641]\tLoss: 0.7180\tLR: 0.000010\n",
      "Training Epoch: 2 [49280/72641]\tLoss: 0.7154\tLR: 0.000010\n",
      "Training Epoch: 2 [49600/72641]\tLoss: 0.6983\tLR: 0.000010\n",
      "Training Epoch: 2 [49920/72641]\tLoss: 0.6918\tLR: 0.000010\n",
      "Training Epoch: 2 [50240/72641]\tLoss: 0.6850\tLR: 0.000010\n",
      "Training Epoch: 2 [50560/72641]\tLoss: 0.7055\tLR: 0.000010\n",
      "Training Epoch: 2 [50880/72641]\tLoss: 0.7075\tLR: 0.000010\n",
      "Training Epoch: 2 [51200/72641]\tLoss: 0.7343\tLR: 0.000010\n",
      "Training Epoch: 2 [51520/72641]\tLoss: 0.7343\tLR: 0.000010\n",
      "Training Epoch: 2 [51840/72641]\tLoss: 0.6710\tLR: 0.000010\n",
      "Training Epoch: 2 [52160/72641]\tLoss: 0.6954\tLR: 0.000010\n",
      "Training Epoch: 2 [52480/72641]\tLoss: 0.7025\tLR: 0.000010\n",
      "Training Epoch: 2 [52800/72641]\tLoss: 0.7182\tLR: 0.000010\n",
      "Training Epoch: 2 [53120/72641]\tLoss: 0.7201\tLR: 0.000010\n",
      "Training Epoch: 2 [53440/72641]\tLoss: 0.7166\tLR: 0.000010\n",
      "Training Epoch: 2 [53760/72641]\tLoss: 0.7160\tLR: 0.000010\n",
      "Training Epoch: 2 [54080/72641]\tLoss: 0.7152\tLR: 0.000010\n",
      "Training Epoch: 2 [54400/72641]\tLoss: 0.6877\tLR: 0.000010\n",
      "Training Epoch: 2 [54720/72641]\tLoss: 0.6862\tLR: 0.000010\n",
      "Training Epoch: 2 [55040/72641]\tLoss: 0.7185\tLR: 0.000010\n",
      "Training Epoch: 2 [55360/72641]\tLoss: 0.7058\tLR: 0.000010\n",
      "Training Epoch: 2 [55680/72641]\tLoss: 0.6822\tLR: 0.000010\n",
      "Training Epoch: 2 [56000/72641]\tLoss: 0.7060\tLR: 0.000010\n",
      "Training Epoch: 2 [56320/72641]\tLoss: 0.6925\tLR: 0.000010\n",
      "Training Epoch: 2 [56640/72641]\tLoss: 0.7134\tLR: 0.000010\n",
      "Training Epoch: 2 [56960/72641]\tLoss: 0.7228\tLR: 0.000010\n",
      "Training Epoch: 2 [57280/72641]\tLoss: 0.7035\tLR: 0.000010\n",
      "Training Epoch: 2 [57600/72641]\tLoss: 0.7067\tLR: 0.000010\n",
      "Training Epoch: 2 [57920/72641]\tLoss: 0.7111\tLR: 0.000010\n",
      "Training Epoch: 2 [58240/72641]\tLoss: 0.6941\tLR: 0.000010\n",
      "Training Epoch: 2 [58560/72641]\tLoss: 0.7158\tLR: 0.000010\n",
      "Training Epoch: 2 [58880/72641]\tLoss: 0.7107\tLR: 0.000010\n",
      "Training Epoch: 2 [59200/72641]\tLoss: 0.7196\tLR: 0.000010\n",
      "Training Epoch: 2 [59520/72641]\tLoss: 0.7057\tLR: 0.000010\n",
      "Training Epoch: 2 [59840/72641]\tLoss: 0.7061\tLR: 0.000010\n",
      "Training Epoch: 2 [60160/72641]\tLoss: 0.6814\tLR: 0.000010\n",
      "Training Epoch: 2 [60480/72641]\tLoss: 0.7002\tLR: 0.000010\n",
      "Training Epoch: 2 [60800/72641]\tLoss: 0.6811\tLR: 0.000010\n",
      "Training Epoch: 2 [61120/72641]\tLoss: 0.6956\tLR: 0.000010\n",
      "Training Epoch: 2 [61440/72641]\tLoss: 0.6843\tLR: 0.000010\n",
      "Training Epoch: 2 [61760/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 2 [62080/72641]\tLoss: 0.7145\tLR: 0.000010\n",
      "Training Epoch: 2 [62400/72641]\tLoss: 0.7268\tLR: 0.000010\n",
      "Training Epoch: 2 [62720/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 2 [63040/72641]\tLoss: 0.6704\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [63360/72641]\tLoss: 0.7179\tLR: 0.000010\n",
      "Training Epoch: 2 [63680/72641]\tLoss: 0.6922\tLR: 0.000010\n",
      "Training Epoch: 2 [64000/72641]\tLoss: 0.7115\tLR: 0.000010\n",
      "Training Epoch: 2 [64320/72641]\tLoss: 0.7117\tLR: 0.000010\n",
      "Training Epoch: 2 [64640/72641]\tLoss: 0.7024\tLR: 0.000010\n",
      "Training Epoch: 2 [64960/72641]\tLoss: 0.7020\tLR: 0.000010\n",
      "Training Epoch: 2 [65280/72641]\tLoss: 0.7427\tLR: 0.000010\n",
      "Training Epoch: 2 [65600/72641]\tLoss: 0.6883\tLR: 0.000010\n",
      "Training Epoch: 2 [65920/72641]\tLoss: 0.7405\tLR: 0.000010\n",
      "Training Epoch: 2 [66240/72641]\tLoss: 0.7047\tLR: 0.000010\n",
      "Training Epoch: 2 [66560/72641]\tLoss: 0.7061\tLR: 0.000010\n",
      "Training Epoch: 2 [66880/72641]\tLoss: 0.7083\tLR: 0.000010\n",
      "Training Epoch: 2 [67200/72641]\tLoss: 0.6695\tLR: 0.000010\n",
      "Training Epoch: 2 [67520/72641]\tLoss: 0.7186\tLR: 0.000010\n",
      "Training Epoch: 2 [67840/72641]\tLoss: 0.7080\tLR: 0.000010\n",
      "Training Epoch: 2 [68160/72641]\tLoss: 0.6634\tLR: 0.000010\n",
      "Training Epoch: 2 [68480/72641]\tLoss: 0.6904\tLR: 0.000010\n",
      "Training Epoch: 2 [68800/72641]\tLoss: 0.6971\tLR: 0.000010\n",
      "Training Epoch: 2 [69120/72641]\tLoss: 0.7121\tLR: 0.000010\n",
      "Training Epoch: 2 [69440/72641]\tLoss: 0.6883\tLR: 0.000010\n",
      "Training Epoch: 2 [69760/72641]\tLoss: 0.7058\tLR: 0.000010\n",
      "Training Epoch: 2 [70080/72641]\tLoss: 0.6999\tLR: 0.000010\n",
      "Training Epoch: 2 [70400/72641]\tLoss: 0.7169\tLR: 0.000010\n",
      "Training Epoch: 2 [70720/72641]\tLoss: 0.7043\tLR: 0.000010\n",
      "Training Epoch: 2 [71040/72641]\tLoss: 0.6808\tLR: 0.000010\n",
      "Training Epoch: 2 [71360/72641]\tLoss: 0.6883\tLR: 0.000010\n",
      "Training Epoch: 2 [71680/72641]\tLoss: 0.7086\tLR: 0.000010\n",
      "Training Epoch: 2 [72000/72641]\tLoss: 0.7275\tLR: 0.000010\n",
      "Training Epoch: 2 [72320/72641]\tLoss: 0.6790\tLR: 0.000010\n",
      "Training Epoch: 2 [72640/72641]\tLoss: 0.7208\tLR: 0.000010\n",
      "Val Result: Acc: 0.1443, C_ACC: 0.6614, DOA: 89.2962, ACC_k: 0.0994\n",
      "ext:0.0, cls:0.62243, coar:0.0, fine:0.0,\n",
      "Training Epoch: 3 [320/72641]\tLoss: 0.6998\tLR: 0.000010\n",
      "Training Epoch: 3 [640/72641]\tLoss: 0.6989\tLR: 0.000010\n",
      "Training Epoch: 3 [960/72641]\tLoss: 0.7065\tLR: 0.000010\n",
      "Training Epoch: 3 [1280/72641]\tLoss: 0.7166\tLR: 0.000010\n",
      "Training Epoch: 3 [1600/72641]\tLoss: 0.7465\tLR: 0.000010\n",
      "Training Epoch: 3 [1920/72641]\tLoss: 0.7037\tLR: 0.000010\n",
      "Training Epoch: 3 [2240/72641]\tLoss: 0.7038\tLR: 0.000010\n",
      "Training Epoch: 3 [2560/72641]\tLoss: 0.6918\tLR: 0.000010\n",
      "Training Epoch: 3 [2880/72641]\tLoss: 0.7242\tLR: 0.000010\n",
      "Training Epoch: 3 [3200/72641]\tLoss: 0.7191\tLR: 0.000010\n",
      "Training Epoch: 3 [3520/72641]\tLoss: 0.7131\tLR: 0.000010\n",
      "Training Epoch: 3 [3840/72641]\tLoss: 0.6840\tLR: 0.000010\n",
      "Training Epoch: 3 [4160/72641]\tLoss: 0.7237\tLR: 0.000010\n",
      "Training Epoch: 3 [4480/72641]\tLoss: 0.6913\tLR: 0.000010\n",
      "Training Epoch: 3 [4800/72641]\tLoss: 0.6605\tLR: 0.000010\n",
      "Training Epoch: 3 [5120/72641]\tLoss: 0.6858\tLR: 0.000010\n",
      "Training Epoch: 3 [5440/72641]\tLoss: 0.6976\tLR: 0.000010\n",
      "Training Epoch: 3 [5760/72641]\tLoss: 0.6931\tLR: 0.000010\n",
      "Training Epoch: 3 [6080/72641]\tLoss: 0.6965\tLR: 0.000010\n",
      "Training Epoch: 3 [6400/72641]\tLoss: 0.7047\tLR: 0.000010\n",
      "Training Epoch: 3 [6720/72641]\tLoss: 0.6942\tLR: 0.000010\n",
      "Training Epoch: 3 [7040/72641]\tLoss: 0.7118\tLR: 0.000010\n",
      "Training Epoch: 3 [7360/72641]\tLoss: 0.7420\tLR: 0.000010\n",
      "Training Epoch: 3 [7680/72641]\tLoss: 0.7439\tLR: 0.000010\n",
      "Training Epoch: 3 [8000/72641]\tLoss: 0.7111\tLR: 0.000010\n",
      "Training Epoch: 3 [8320/72641]\tLoss: 0.6864\tLR: 0.000010\n",
      "Training Epoch: 3 [8640/72641]\tLoss: 0.7059\tLR: 0.000010\n",
      "Training Epoch: 3 [8960/72641]\tLoss: 0.6866\tLR: 0.000010\n",
      "Training Epoch: 3 [9280/72641]\tLoss: 0.6828\tLR: 0.000010\n",
      "Training Epoch: 3 [9600/72641]\tLoss: 0.7423\tLR: 0.000010\n",
      "Training Epoch: 3 [9920/72641]\tLoss: 0.7229\tLR: 0.000010\n",
      "Training Epoch: 3 [10240/72641]\tLoss: 0.7149\tLR: 0.000010\n",
      "Training Epoch: 3 [10560/72641]\tLoss: 0.6770\tLR: 0.000010\n",
      "Training Epoch: 3 [10880/72641]\tLoss: 0.7041\tLR: 0.000010\n",
      "Training Epoch: 3 [11200/72641]\tLoss: 0.6895\tLR: 0.000010\n",
      "Training Epoch: 3 [11520/72641]\tLoss: 0.6972\tLR: 0.000010\n",
      "Training Epoch: 3 [11840/72641]\tLoss: 0.7206\tLR: 0.000010\n",
      "Training Epoch: 3 [12160/72641]\tLoss: 0.7085\tLR: 0.000010\n",
      "Training Epoch: 3 [12480/72641]\tLoss: 0.6478\tLR: 0.000010\n",
      "Training Epoch: 3 [12800/72641]\tLoss: 0.7182\tLR: 0.000010\n",
      "Training Epoch: 3 [13120/72641]\tLoss: 0.7299\tLR: 0.000010\n",
      "Training Epoch: 3 [13440/72641]\tLoss: 0.6868\tLR: 0.000010\n",
      "Training Epoch: 3 [13760/72641]\tLoss: 0.7215\tLR: 0.000010\n",
      "Training Epoch: 3 [14080/72641]\tLoss: 0.6908\tLR: 0.000010\n",
      "Training Epoch: 3 [14400/72641]\tLoss: 0.7151\tLR: 0.000010\n",
      "Training Epoch: 3 [14720/72641]\tLoss: 0.6885\tLR: 0.000010\n",
      "Training Epoch: 3 [15040/72641]\tLoss: 0.7063\tLR: 0.000010\n",
      "Training Epoch: 3 [15360/72641]\tLoss: 0.7331\tLR: 0.000010\n",
      "Training Epoch: 3 [15680/72641]\tLoss: 0.6987\tLR: 0.000010\n",
      "Training Epoch: 3 [16000/72641]\tLoss: 0.7286\tLR: 0.000010\n",
      "Training Epoch: 3 [16320/72641]\tLoss: 0.7293\tLR: 0.000010\n",
      "Training Epoch: 3 [16640/72641]\tLoss: 0.7013\tLR: 0.000010\n",
      "Training Epoch: 3 [16960/72641]\tLoss: 0.7053\tLR: 0.000010\n",
      "Training Epoch: 3 [17280/72641]\tLoss: 0.6859\tLR: 0.000010\n",
      "Training Epoch: 3 [17600/72641]\tLoss: 0.6838\tLR: 0.000010\n",
      "Training Epoch: 3 [17920/72641]\tLoss: 0.6977\tLR: 0.000010\n",
      "Training Epoch: 3 [18240/72641]\tLoss: 0.6771\tLR: 0.000010\n",
      "Training Epoch: 3 [18560/72641]\tLoss: 0.7076\tLR: 0.000010\n",
      "Training Epoch: 3 [18880/72641]\tLoss: 0.7068\tLR: 0.000010\n",
      "Training Epoch: 3 [19200/72641]\tLoss: 0.6966\tLR: 0.000010\n",
      "Training Epoch: 3 [19520/72641]\tLoss: 0.7165\tLR: 0.000010\n",
      "Training Epoch: 3 [19840/72641]\tLoss: 0.7209\tLR: 0.000010\n",
      "Training Epoch: 3 [20160/72641]\tLoss: 0.6575\tLR: 0.000010\n",
      "Training Epoch: 3 [20480/72641]\tLoss: 0.6865\tLR: 0.000010\n",
      "Training Epoch: 3 [20800/72641]\tLoss: 0.7047\tLR: 0.000010\n",
      "Training Epoch: 3 [21120/72641]\tLoss: 0.7067\tLR: 0.000010\n",
      "Training Epoch: 3 [21440/72641]\tLoss: 0.6994\tLR: 0.000010\n",
      "Training Epoch: 3 [21760/72641]\tLoss: 0.6545\tLR: 0.000010\n",
      "Training Epoch: 3 [22080/72641]\tLoss: 0.7051\tLR: 0.000010\n",
      "Training Epoch: 3 [22400/72641]\tLoss: 0.6770\tLR: 0.000010\n",
      "Training Epoch: 3 [22720/72641]\tLoss: 0.6978\tLR: 0.000010\n",
      "Training Epoch: 3 [23040/72641]\tLoss: 0.7164\tLR: 0.000010\n",
      "Training Epoch: 3 [23360/72641]\tLoss: 0.6925\tLR: 0.000010\n",
      "Training Epoch: 3 [23680/72641]\tLoss: 0.7357\tLR: 0.000010\n",
      "Training Epoch: 3 [24000/72641]\tLoss: 0.7284\tLR: 0.000010\n",
      "Training Epoch: 3 [24320/72641]\tLoss: 0.7172\tLR: 0.000010\n",
      "Training Epoch: 3 [24640/72641]\tLoss: 0.6856\tLR: 0.000010\n",
      "Training Epoch: 3 [24960/72641]\tLoss: 0.6942\tLR: 0.000010\n",
      "Training Epoch: 3 [25280/72641]\tLoss: 0.6736\tLR: 0.000010\n",
      "Training Epoch: 3 [25600/72641]\tLoss: 0.6880\tLR: 0.000010\n",
      "Training Epoch: 3 [25920/72641]\tLoss: 0.6496\tLR: 0.000010\n",
      "Training Epoch: 3 [26240/72641]\tLoss: 0.7633\tLR: 0.000010\n",
      "Training Epoch: 3 [26560/72641]\tLoss: 0.7124\tLR: 0.000010\n",
      "Training Epoch: 3 [26880/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 3 [27200/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 3 [27520/72641]\tLoss: 0.6893\tLR: 0.000010\n",
      "Training Epoch: 3 [27840/72641]\tLoss: 0.6848\tLR: 0.000010\n",
      "Training Epoch: 3 [28160/72641]\tLoss: 0.6880\tLR: 0.000010\n",
      "Training Epoch: 3 [28480/72641]\tLoss: 0.7120\tLR: 0.000010\n",
      "Training Epoch: 3 [28800/72641]\tLoss: 0.6860\tLR: 0.000010\n",
      "Training Epoch: 3 [29120/72641]\tLoss: 0.7004\tLR: 0.000010\n",
      "Training Epoch: 3 [29440/72641]\tLoss: 0.6631\tLR: 0.000010\n",
      "Training Epoch: 3 [29760/72641]\tLoss: 0.7088\tLR: 0.000010\n",
      "Training Epoch: 3 [30080/72641]\tLoss: 0.6939\tLR: 0.000010\n",
      "Training Epoch: 3 [30400/72641]\tLoss: 0.6600\tLR: 0.000010\n",
      "Training Epoch: 3 [30720/72641]\tLoss: 0.7170\tLR: 0.000010\n",
      "Training Epoch: 3 [31040/72641]\tLoss: 0.6848\tLR: 0.000010\n",
      "Training Epoch: 3 [31360/72641]\tLoss: 0.7006\tLR: 0.000010\n",
      "Training Epoch: 3 [31680/72641]\tLoss: 0.7031\tLR: 0.000010\n",
      "Training Epoch: 3 [32000/72641]\tLoss: 0.7003\tLR: 0.000010\n",
      "Training Epoch: 3 [32320/72641]\tLoss: 0.6977\tLR: 0.000010\n",
      "Training Epoch: 3 [32640/72641]\tLoss: 0.7040\tLR: 0.000010\n",
      "Training Epoch: 3 [32960/72641]\tLoss: 0.7321\tLR: 0.000010\n",
      "Training Epoch: 3 [33280/72641]\tLoss: 0.7022\tLR: 0.000010\n",
      "Training Epoch: 3 [33600/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 3 [33920/72641]\tLoss: 0.6909\tLR: 0.000010\n",
      "Training Epoch: 3 [34240/72641]\tLoss: 0.6976\tLR: 0.000010\n",
      "Training Epoch: 3 [34560/72641]\tLoss: 0.7379\tLR: 0.000010\n",
      "Training Epoch: 3 [34880/72641]\tLoss: 0.7260\tLR: 0.000010\n",
      "Training Epoch: 3 [35200/72641]\tLoss: 0.7145\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [35520/72641]\tLoss: 0.7132\tLR: 0.000010\n",
      "Training Epoch: 3 [35840/72641]\tLoss: 0.6930\tLR: 0.000010\n",
      "Training Epoch: 3 [36160/72641]\tLoss: 0.7095\tLR: 0.000010\n",
      "Training Epoch: 3 [36480/72641]\tLoss: 0.6922\tLR: 0.000010\n",
      "Training Epoch: 3 [36800/72641]\tLoss: 0.6992\tLR: 0.000010\n",
      "Training Epoch: 3 [37120/72641]\tLoss: 0.7093\tLR: 0.000010\n",
      "Training Epoch: 3 [37440/72641]\tLoss: 0.7112\tLR: 0.000010\n",
      "Training Epoch: 3 [37760/72641]\tLoss: 0.6801\tLR: 0.000010\n",
      "Training Epoch: 3 [38080/72641]\tLoss: 0.6790\tLR: 0.000010\n",
      "Training Epoch: 3 [38400/72641]\tLoss: 0.7187\tLR: 0.000010\n",
      "Training Epoch: 3 [38720/72641]\tLoss: 0.7146\tLR: 0.000010\n",
      "Training Epoch: 3 [39040/72641]\tLoss: 0.6860\tLR: 0.000010\n",
      "Training Epoch: 3 [39360/72641]\tLoss: 0.6953\tLR: 0.000010\n",
      "Training Epoch: 3 [39680/72641]\tLoss: 0.6947\tLR: 0.000010\n",
      "Training Epoch: 3 [40000/72641]\tLoss: 0.7214\tLR: 0.000010\n",
      "Training Epoch: 3 [40320/72641]\tLoss: 0.7114\tLR: 0.000010\n",
      "Training Epoch: 3 [40640/72641]\tLoss: 0.7352\tLR: 0.000010\n",
      "Training Epoch: 3 [40960/72641]\tLoss: 0.6857\tLR: 0.000010\n",
      "Training Epoch: 3 [41280/72641]\tLoss: 0.7121\tLR: 0.000010\n",
      "Training Epoch: 3 [41600/72641]\tLoss: 0.7230\tLR: 0.000010\n",
      "Training Epoch: 3 [41920/72641]\tLoss: 0.6799\tLR: 0.000010\n",
      "Training Epoch: 3 [42240/72641]\tLoss: 0.7003\tLR: 0.000010\n",
      "Training Epoch: 3 [42560/72641]\tLoss: 0.7011\tLR: 0.000010\n",
      "Training Epoch: 3 [42880/72641]\tLoss: 0.6753\tLR: 0.000010\n",
      "Training Epoch: 3 [43200/72641]\tLoss: 0.6922\tLR: 0.000010\n",
      "Training Epoch: 3 [43520/72641]\tLoss: 0.6785\tLR: 0.000010\n",
      "Training Epoch: 3 [43840/72641]\tLoss: 0.6967\tLR: 0.000010\n",
      "Training Epoch: 3 [44160/72641]\tLoss: 0.7203\tLR: 0.000010\n",
      "Training Epoch: 3 [44480/72641]\tLoss: 0.6852\tLR: 0.000010\n",
      "Training Epoch: 3 [44800/72641]\tLoss: 0.6917\tLR: 0.000010\n",
      "Training Epoch: 3 [45120/72641]\tLoss: 0.7116\tLR: 0.000010\n",
      "Training Epoch: 3 [45440/72641]\tLoss: 0.6997\tLR: 0.000010\n",
      "Training Epoch: 3 [45760/72641]\tLoss: 0.6987\tLR: 0.000010\n",
      "Training Epoch: 3 [46080/72641]\tLoss: 0.7164\tLR: 0.000010\n",
      "Training Epoch: 3 [46400/72641]\tLoss: 0.7058\tLR: 0.000010\n",
      "Training Epoch: 3 [46720/72641]\tLoss: 0.7076\tLR: 0.000010\n",
      "Training Epoch: 3 [47040/72641]\tLoss: 0.7049\tLR: 0.000010\n",
      "Training Epoch: 3 [47360/72641]\tLoss: 0.7245\tLR: 0.000010\n",
      "Training Epoch: 3 [47680/72641]\tLoss: 0.7192\tLR: 0.000010\n",
      "Training Epoch: 3 [48000/72641]\tLoss: 0.6990\tLR: 0.000010\n",
      "Training Epoch: 3 [48320/72641]\tLoss: 0.7309\tLR: 0.000010\n",
      "Training Epoch: 3 [48640/72641]\tLoss: 0.6959\tLR: 0.000010\n",
      "Training Epoch: 3 [48960/72641]\tLoss: 0.6964\tLR: 0.000010\n",
      "Training Epoch: 3 [49280/72641]\tLoss: 0.7133\tLR: 0.000010\n",
      "Training Epoch: 3 [49600/72641]\tLoss: 0.7074\tLR: 0.000010\n",
      "Training Epoch: 3 [49920/72641]\tLoss: 0.6591\tLR: 0.000010\n",
      "Training Epoch: 3 [50240/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 3 [50560/72641]\tLoss: 0.6762\tLR: 0.000010\n",
      "Training Epoch: 3 [50880/72641]\tLoss: 0.6923\tLR: 0.000010\n",
      "Training Epoch: 3 [51200/72641]\tLoss: 0.7104\tLR: 0.000010\n",
      "Training Epoch: 3 [51520/72641]\tLoss: 0.7051\tLR: 0.000010\n",
      "Training Epoch: 3 [51840/72641]\tLoss: 0.6944\tLR: 0.000010\n",
      "Training Epoch: 3 [52160/72641]\tLoss: 0.7187\tLR: 0.000010\n",
      "Training Epoch: 3 [52480/72641]\tLoss: 0.6825\tLR: 0.000010\n",
      "Training Epoch: 3 [52800/72641]\tLoss: 0.7152\tLR: 0.000010\n",
      "Training Epoch: 3 [53120/72641]\tLoss: 0.7008\tLR: 0.000010\n",
      "Training Epoch: 3 [53440/72641]\tLoss: 0.6760\tLR: 0.000010\n",
      "Training Epoch: 3 [53760/72641]\tLoss: 0.7320\tLR: 0.000010\n",
      "Training Epoch: 3 [54080/72641]\tLoss: 0.6992\tLR: 0.000010\n",
      "Training Epoch: 3 [54400/72641]\tLoss: 0.7119\tLR: 0.000010\n",
      "Training Epoch: 3 [54720/72641]\tLoss: 0.7034\tLR: 0.000010\n",
      "Training Epoch: 3 [55040/72641]\tLoss: 0.7076\tLR: 0.000010\n",
      "Training Epoch: 3 [55360/72641]\tLoss: 0.7051\tLR: 0.000010\n",
      "Training Epoch: 3 [55680/72641]\tLoss: 0.6533\tLR: 0.000010\n",
      "Training Epoch: 3 [56000/72641]\tLoss: 0.6662\tLR: 0.000010\n",
      "Training Epoch: 3 [56320/72641]\tLoss: 0.6892\tLR: 0.000010\n",
      "Training Epoch: 3 [56640/72641]\tLoss: 0.7092\tLR: 0.000010\n",
      "Training Epoch: 3 [56960/72641]\tLoss: 0.7026\tLR: 0.000010\n",
      "Training Epoch: 3 [57280/72641]\tLoss: 0.6870\tLR: 0.000010\n",
      "Training Epoch: 3 [57600/72641]\tLoss: 0.7195\tLR: 0.000010\n",
      "Training Epoch: 3 [57920/72641]\tLoss: 0.6952\tLR: 0.000010\n",
      "Training Epoch: 3 [58240/72641]\tLoss: 0.6819\tLR: 0.000010\n",
      "Training Epoch: 3 [58560/72641]\tLoss: 0.7011\tLR: 0.000010\n",
      "Training Epoch: 3 [58880/72641]\tLoss: 0.7017\tLR: 0.000010\n",
      "Training Epoch: 3 [59200/72641]\tLoss: 0.7003\tLR: 0.000010\n",
      "Training Epoch: 3 [59520/72641]\tLoss: 0.6824\tLR: 0.000010\n",
      "Training Epoch: 3 [59840/72641]\tLoss: 0.7006\tLR: 0.000010\n",
      "Training Epoch: 3 [60160/72641]\tLoss: 0.7058\tLR: 0.000010\n",
      "Training Epoch: 3 [60480/72641]\tLoss: 0.7171\tLR: 0.000010\n",
      "Training Epoch: 3 [60800/72641]\tLoss: 0.6962\tLR: 0.000010\n",
      "Training Epoch: 3 [61120/72641]\tLoss: 0.6673\tLR: 0.000010\n",
      "Training Epoch: 3 [61440/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 3 [61760/72641]\tLoss: 0.6914\tLR: 0.000010\n",
      "Training Epoch: 3 [62080/72641]\tLoss: 0.7188\tLR: 0.000010\n",
      "Training Epoch: 3 [62400/72641]\tLoss: 0.6670\tLR: 0.000010\n",
      "Training Epoch: 3 [62720/72641]\tLoss: 0.6767\tLR: 0.000010\n",
      "Training Epoch: 3 [63040/72641]\tLoss: 0.6989\tLR: 0.000010\n",
      "Training Epoch: 3 [63360/72641]\tLoss: 0.7019\tLR: 0.000010\n",
      "Training Epoch: 3 [63680/72641]\tLoss: 0.7118\tLR: 0.000010\n",
      "Training Epoch: 3 [64000/72641]\tLoss: 0.7002\tLR: 0.000010\n",
      "Training Epoch: 3 [64320/72641]\tLoss: 0.6783\tLR: 0.000010\n",
      "Training Epoch: 3 [64640/72641]\tLoss: 0.7100\tLR: 0.000010\n",
      "Training Epoch: 3 [64960/72641]\tLoss: 0.7075\tLR: 0.000010\n",
      "Training Epoch: 3 [65280/72641]\tLoss: 0.7027\tLR: 0.000010\n",
      "Training Epoch: 3 [65600/72641]\tLoss: 0.6966\tLR: 0.000010\n",
      "Training Epoch: 3 [65920/72641]\tLoss: 0.6971\tLR: 0.000010\n",
      "Training Epoch: 3 [66240/72641]\tLoss: 0.7114\tLR: 0.000010\n",
      "Training Epoch: 3 [66560/72641]\tLoss: 0.7103\tLR: 0.000010\n",
      "Training Epoch: 3 [66880/72641]\tLoss: 0.6987\tLR: 0.000010\n",
      "Training Epoch: 3 [67200/72641]\tLoss: 0.6789\tLR: 0.000010\n",
      "Training Epoch: 3 [67520/72641]\tLoss: 0.7303\tLR: 0.000010\n",
      "Training Epoch: 3 [67840/72641]\tLoss: 0.6976\tLR: 0.000010\n",
      "Training Epoch: 3 [68160/72641]\tLoss: 0.6895\tLR: 0.000010\n",
      "Training Epoch: 3 [68480/72641]\tLoss: 0.6930\tLR: 0.000010\n",
      "Training Epoch: 3 [68800/72641]\tLoss: 0.7104\tLR: 0.000010\n",
      "Training Epoch: 3 [69120/72641]\tLoss: 0.6975\tLR: 0.000010\n",
      "Training Epoch: 3 [69440/72641]\tLoss: 0.6909\tLR: 0.000010\n",
      "Training Epoch: 3 [69760/72641]\tLoss: 0.6766\tLR: 0.000010\n",
      "Training Epoch: 3 [70080/72641]\tLoss: 0.6786\tLR: 0.000010\n",
      "Training Epoch: 3 [70400/72641]\tLoss: 0.7192\tLR: 0.000010\n",
      "Training Epoch: 3 [70720/72641]\tLoss: 0.7097\tLR: 0.000010\n",
      "Training Epoch: 3 [71040/72641]\tLoss: 0.7204\tLR: 0.000010\n",
      "Training Epoch: 3 [71360/72641]\tLoss: 0.7153\tLR: 0.000010\n",
      "Training Epoch: 3 [71680/72641]\tLoss: 0.7060\tLR: 0.000010\n",
      "Training Epoch: 3 [72000/72641]\tLoss: 0.6682\tLR: 0.000010\n",
      "Training Epoch: 3 [72320/72641]\tLoss: 0.7180\tLR: 0.000010\n",
      "Training Epoch: 3 [72640/72641]\tLoss: 0.6784\tLR: 0.000010\n",
      "Val Result: Acc: 0.1446, C_ACC: 0.6700, DOA: 88.8060, ACC_k: 0.1024\n",
      "ext:0.0, cls:0.614949, coar:0.0, fine:0.0,\n",
      "Training Epoch: 4 [320/72641]\tLoss: 0.7135\tLR: 0.000010\n",
      "Training Epoch: 4 [640/72641]\tLoss: 0.6629\tLR: 0.000010\n",
      "Training Epoch: 4 [960/72641]\tLoss: 0.7146\tLR: 0.000010\n",
      "Training Epoch: 4 [1280/72641]\tLoss: 0.7188\tLR: 0.000010\n",
      "Training Epoch: 4 [1600/72641]\tLoss: 0.7238\tLR: 0.000010\n",
      "Training Epoch: 4 [1920/72641]\tLoss: 0.6888\tLR: 0.000010\n",
      "Training Epoch: 4 [2240/72641]\tLoss: 0.6986\tLR: 0.000010\n",
      "Training Epoch: 4 [2560/72641]\tLoss: 0.6806\tLR: 0.000010\n",
      "Training Epoch: 4 [2880/72641]\tLoss: 0.6878\tLR: 0.000010\n",
      "Training Epoch: 4 [3200/72641]\tLoss: 0.7141\tLR: 0.000010\n",
      "Training Epoch: 4 [3520/72641]\tLoss: 0.7149\tLR: 0.000010\n",
      "Training Epoch: 4 [3840/72641]\tLoss: 0.6803\tLR: 0.000010\n",
      "Training Epoch: 4 [4160/72641]\tLoss: 0.7015\tLR: 0.000010\n",
      "Training Epoch: 4 [4480/72641]\tLoss: 0.6713\tLR: 0.000010\n",
      "Training Epoch: 4 [4800/72641]\tLoss: 0.6979\tLR: 0.000010\n",
      "Training Epoch: 4 [5120/72641]\tLoss: 0.6853\tLR: 0.000010\n",
      "Training Epoch: 4 [5440/72641]\tLoss: 0.6943\tLR: 0.000010\n",
      "Training Epoch: 4 [5760/72641]\tLoss: 0.6662\tLR: 0.000010\n",
      "Training Epoch: 4 [6080/72641]\tLoss: 0.6659\tLR: 0.000010\n",
      "Training Epoch: 4 [6400/72641]\tLoss: 0.6879\tLR: 0.000010\n",
      "Training Epoch: 4 [6720/72641]\tLoss: 0.6706\tLR: 0.000010\n",
      "Training Epoch: 4 [7040/72641]\tLoss: 0.6918\tLR: 0.000010\n",
      "Training Epoch: 4 [7360/72641]\tLoss: 0.6496\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [7680/72641]\tLoss: 0.6810\tLR: 0.000010\n",
      "Training Epoch: 4 [8000/72641]\tLoss: 0.7140\tLR: 0.000010\n",
      "Training Epoch: 4 [8320/72641]\tLoss: 0.7010\tLR: 0.000010\n",
      "Training Epoch: 4 [8640/72641]\tLoss: 0.7100\tLR: 0.000010\n",
      "Training Epoch: 4 [8960/72641]\tLoss: 0.6839\tLR: 0.000010\n",
      "Training Epoch: 4 [9280/72641]\tLoss: 0.7058\tLR: 0.000010\n",
      "Training Epoch: 4 [9600/72641]\tLoss: 0.7133\tLR: 0.000010\n",
      "Training Epoch: 4 [9920/72641]\tLoss: 0.7022\tLR: 0.000010\n",
      "Training Epoch: 4 [10240/72641]\tLoss: 0.6982\tLR: 0.000010\n",
      "Training Epoch: 4 [10560/72641]\tLoss: 0.6567\tLR: 0.000010\n",
      "Training Epoch: 4 [10880/72641]\tLoss: 0.7131\tLR: 0.000010\n",
      "Training Epoch: 4 [11200/72641]\tLoss: 0.7029\tLR: 0.000010\n",
      "Training Epoch: 4 [11520/72641]\tLoss: 0.6839\tLR: 0.000010\n",
      "Training Epoch: 4 [11840/72641]\tLoss: 0.6899\tLR: 0.000010\n",
      "Training Epoch: 4 [12160/72641]\tLoss: 0.6622\tLR: 0.000010\n",
      "Training Epoch: 4 [12480/72641]\tLoss: 0.6687\tLR: 0.000010\n",
      "Training Epoch: 4 [12800/72641]\tLoss: 0.6941\tLR: 0.000010\n",
      "Training Epoch: 4 [13120/72641]\tLoss: 0.7026\tLR: 0.000010\n",
      "Training Epoch: 4 [13440/72641]\tLoss: 0.6995\tLR: 0.000010\n",
      "Training Epoch: 4 [13760/72641]\tLoss: 0.7071\tLR: 0.000010\n",
      "Training Epoch: 4 [14080/72641]\tLoss: 0.6933\tLR: 0.000010\n",
      "Training Epoch: 4 [14400/72641]\tLoss: 0.6996\tLR: 0.000010\n",
      "Training Epoch: 4 [14720/72641]\tLoss: 0.6823\tLR: 0.000010\n",
      "Training Epoch: 4 [15040/72641]\tLoss: 0.6638\tLR: 0.000010\n",
      "Training Epoch: 4 [15360/72641]\tLoss: 0.7364\tLR: 0.000010\n",
      "Training Epoch: 4 [15680/72641]\tLoss: 0.7011\tLR: 0.000010\n",
      "Training Epoch: 4 [16000/72641]\tLoss: 0.7081\tLR: 0.000010\n",
      "Training Epoch: 4 [16320/72641]\tLoss: 0.6970\tLR: 0.000010\n",
      "Training Epoch: 4 [16640/72641]\tLoss: 0.7277\tLR: 0.000010\n",
      "Training Epoch: 4 [16960/72641]\tLoss: 0.6994\tLR: 0.000010\n",
      "Training Epoch: 4 [17280/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 4 [17600/72641]\tLoss: 0.6606\tLR: 0.000010\n",
      "Training Epoch: 4 [17920/72641]\tLoss: 0.6887\tLR: 0.000010\n",
      "Training Epoch: 4 [18240/72641]\tLoss: 0.6509\tLR: 0.000010\n",
      "Training Epoch: 4 [18560/72641]\tLoss: 0.6642\tLR: 0.000010\n",
      "Training Epoch: 4 [18880/72641]\tLoss: 0.6851\tLR: 0.000010\n",
      "Training Epoch: 4 [19200/72641]\tLoss: 0.7147\tLR: 0.000010\n",
      "Training Epoch: 4 [19520/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 4 [19840/72641]\tLoss: 0.7011\tLR: 0.000010\n",
      "Training Epoch: 4 [20160/72641]\tLoss: 0.6724\tLR: 0.000010\n",
      "Training Epoch: 4 [20480/72641]\tLoss: 0.6911\tLR: 0.000010\n",
      "Training Epoch: 4 [20800/72641]\tLoss: 0.7142\tLR: 0.000010\n",
      "Training Epoch: 4 [21120/72641]\tLoss: 0.6658\tLR: 0.000010\n",
      "Training Epoch: 4 [21440/72641]\tLoss: 0.6889\tLR: 0.000010\n",
      "Training Epoch: 4 [21760/72641]\tLoss: 0.7280\tLR: 0.000010\n",
      "Training Epoch: 4 [22080/72641]\tLoss: 0.7237\tLR: 0.000010\n",
      "Training Epoch: 4 [22400/72641]\tLoss: 0.6955\tLR: 0.000010\n",
      "Training Epoch: 4 [22720/72641]\tLoss: 0.6463\tLR: 0.000010\n",
      "Training Epoch: 4 [23040/72641]\tLoss: 0.6978\tLR: 0.000010\n",
      "Training Epoch: 4 [23360/72641]\tLoss: 0.6651\tLR: 0.000010\n",
      "Training Epoch: 4 [23680/72641]\tLoss: 0.6719\tLR: 0.000010\n",
      "Training Epoch: 4 [24000/72641]\tLoss: 0.7153\tLR: 0.000010\n",
      "Training Epoch: 4 [24320/72641]\tLoss: 0.6900\tLR: 0.000010\n",
      "Training Epoch: 4 [24640/72641]\tLoss: 0.6993\tLR: 0.000010\n",
      "Training Epoch: 4 [24960/72641]\tLoss: 0.6963\tLR: 0.000010\n",
      "Training Epoch: 4 [25280/72641]\tLoss: 0.7040\tLR: 0.000010\n",
      "Training Epoch: 4 [25600/72641]\tLoss: 0.7157\tLR: 0.000010\n",
      "Training Epoch: 4 [25920/72641]\tLoss: 0.6351\tLR: 0.000010\n",
      "Training Epoch: 4 [26240/72641]\tLoss: 0.6823\tLR: 0.000010\n",
      "Training Epoch: 4 [26560/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 4 [26880/72641]\tLoss: 0.6810\tLR: 0.000010\n",
      "Training Epoch: 4 [27200/72641]\tLoss: 0.7063\tLR: 0.000010\n",
      "Training Epoch: 4 [27520/72641]\tLoss: 0.6827\tLR: 0.000010\n",
      "Training Epoch: 4 [27840/72641]\tLoss: 0.7026\tLR: 0.000010\n",
      "Training Epoch: 4 [28160/72641]\tLoss: 0.6801\tLR: 0.000010\n",
      "Training Epoch: 4 [28480/72641]\tLoss: 0.6990\tLR: 0.000010\n",
      "Training Epoch: 4 [28800/72641]\tLoss: 0.6940\tLR: 0.000010\n",
      "Training Epoch: 4 [29120/72641]\tLoss: 0.6826\tLR: 0.000010\n",
      "Training Epoch: 4 [29440/72641]\tLoss: 0.6894\tLR: 0.000010\n",
      "Training Epoch: 4 [29760/72641]\tLoss: 0.6803\tLR: 0.000010\n",
      "Training Epoch: 4 [30080/72641]\tLoss: 0.6642\tLR: 0.000010\n",
      "Training Epoch: 4 [30400/72641]\tLoss: 0.6878\tLR: 0.000010\n",
      "Training Epoch: 4 [30720/72641]\tLoss: 0.7060\tLR: 0.000010\n",
      "Training Epoch: 4 [31040/72641]\tLoss: 0.7096\tLR: 0.000010\n",
      "Training Epoch: 4 [31360/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 4 [31680/72641]\tLoss: 0.6892\tLR: 0.000010\n",
      "Training Epoch: 4 [32000/72641]\tLoss: 0.6844\tLR: 0.000010\n",
      "Training Epoch: 4 [32320/72641]\tLoss: 0.6951\tLR: 0.000010\n",
      "Training Epoch: 4 [32640/72641]\tLoss: 0.6874\tLR: 0.000010\n",
      "Training Epoch: 4 [32960/72641]\tLoss: 0.6758\tLR: 0.000010\n",
      "Training Epoch: 4 [33280/72641]\tLoss: 0.6831\tLR: 0.000010\n",
      "Training Epoch: 4 [33600/72641]\tLoss: 0.6560\tLR: 0.000010\n",
      "Training Epoch: 4 [33920/72641]\tLoss: 0.6838\tLR: 0.000010\n",
      "Training Epoch: 4 [34240/72641]\tLoss: 0.6790\tLR: 0.000010\n",
      "Training Epoch: 4 [34560/72641]\tLoss: 0.7326\tLR: 0.000010\n",
      "Training Epoch: 4 [34880/72641]\tLoss: 0.6775\tLR: 0.000010\n",
      "Training Epoch: 4 [35200/72641]\tLoss: 0.6953\tLR: 0.000010\n",
      "Training Epoch: 4 [35520/72641]\tLoss: 0.7092\tLR: 0.000010\n",
      "Training Epoch: 4 [35840/72641]\tLoss: 0.7192\tLR: 0.000010\n",
      "Training Epoch: 4 [36160/72641]\tLoss: 0.6812\tLR: 0.000010\n",
      "Training Epoch: 4 [36480/72641]\tLoss: 0.6778\tLR: 0.000010\n",
      "Training Epoch: 4 [36800/72641]\tLoss: 0.6911\tLR: 0.000010\n",
      "Training Epoch: 4 [37120/72641]\tLoss: 0.6843\tLR: 0.000010\n",
      "Training Epoch: 4 [37440/72641]\tLoss: 0.6916\tLR: 0.000010\n",
      "Training Epoch: 4 [37760/72641]\tLoss: 0.6857\tLR: 0.000010\n",
      "Training Epoch: 4 [38080/72641]\tLoss: 0.6899\tLR: 0.000010\n",
      "Training Epoch: 4 [38400/72641]\tLoss: 0.7083\tLR: 0.000010\n",
      "Training Epoch: 4 [38720/72641]\tLoss: 0.6899\tLR: 0.000010\n",
      "Training Epoch: 4 [39040/72641]\tLoss: 0.6696\tLR: 0.000010\n",
      "Training Epoch: 4 [39360/72641]\tLoss: 0.6826\tLR: 0.000010\n",
      "Training Epoch: 4 [39680/72641]\tLoss: 0.6924\tLR: 0.000010\n",
      "Training Epoch: 4 [40000/72641]\tLoss: 0.7167\tLR: 0.000010\n",
      "Training Epoch: 4 [40320/72641]\tLoss: 0.6957\tLR: 0.000010\n",
      "Training Epoch: 4 [40640/72641]\tLoss: 0.7058\tLR: 0.000010\n",
      "Training Epoch: 4 [40960/72641]\tLoss: 0.6543\tLR: 0.000010\n",
      "Training Epoch: 4 [41280/72641]\tLoss: 0.6686\tLR: 0.000010\n",
      "Training Epoch: 4 [41600/72641]\tLoss: 0.6941\tLR: 0.000010\n",
      "Training Epoch: 4 [41920/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 4 [42240/72641]\tLoss: 0.6461\tLR: 0.000010\n",
      "Training Epoch: 4 [42560/72641]\tLoss: 0.6817\tLR: 0.000010\n",
      "Training Epoch: 4 [42880/72641]\tLoss: 0.7204\tLR: 0.000010\n",
      "Training Epoch: 4 [43200/72641]\tLoss: 0.6879\tLR: 0.000010\n",
      "Training Epoch: 4 [43520/72641]\tLoss: 0.6814\tLR: 0.000010\n",
      "Training Epoch: 4 [43840/72641]\tLoss: 0.7078\tLR: 0.000010\n",
      "Training Epoch: 4 [44160/72641]\tLoss: 0.7101\tLR: 0.000010\n",
      "Training Epoch: 4 [44480/72641]\tLoss: 0.6885\tLR: 0.000010\n",
      "Training Epoch: 4 [44800/72641]\tLoss: 0.6700\tLR: 0.000010\n",
      "Training Epoch: 4 [45120/72641]\tLoss: 0.6636\tLR: 0.000010\n",
      "Training Epoch: 4 [45440/72641]\tLoss: 0.7104\tLR: 0.000010\n",
      "Training Epoch: 4 [45760/72641]\tLoss: 0.7117\tLR: 0.000010\n",
      "Training Epoch: 4 [46080/72641]\tLoss: 0.6733\tLR: 0.000010\n",
      "Training Epoch: 4 [46400/72641]\tLoss: 0.6724\tLR: 0.000010\n",
      "Training Epoch: 4 [46720/72641]\tLoss: 0.6891\tLR: 0.000010\n",
      "Training Epoch: 4 [47040/72641]\tLoss: 0.6977\tLR: 0.000010\n",
      "Training Epoch: 4 [47360/72641]\tLoss: 0.6797\tLR: 0.000010\n",
      "Training Epoch: 4 [47680/72641]\tLoss: 0.6527\tLR: 0.000010\n",
      "Training Epoch: 4 [48000/72641]\tLoss: 0.6726\tLR: 0.000010\n",
      "Training Epoch: 4 [48320/72641]\tLoss: 0.7187\tLR: 0.000010\n",
      "Training Epoch: 4 [48640/72641]\tLoss: 0.6824\tLR: 0.000010\n",
      "Training Epoch: 4 [48960/72641]\tLoss: 0.7096\tLR: 0.000010\n",
      "Training Epoch: 4 [49280/72641]\tLoss: 0.7159\tLR: 0.000010\n",
      "Training Epoch: 4 [49600/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 4 [49920/72641]\tLoss: 0.6853\tLR: 0.000010\n",
      "Training Epoch: 4 [50240/72641]\tLoss: 0.6886\tLR: 0.000010\n",
      "Training Epoch: 4 [50560/72641]\tLoss: 0.6812\tLR: 0.000010\n",
      "Training Epoch: 4 [50880/72641]\tLoss: 0.6518\tLR: 0.000010\n",
      "Training Epoch: 4 [51200/72641]\tLoss: 0.6758\tLR: 0.000010\n",
      "Training Epoch: 4 [51520/72641]\tLoss: 0.6919\tLR: 0.000010\n",
      "Training Epoch: 4 [51840/72641]\tLoss: 0.6830\tLR: 0.000010\n",
      "Training Epoch: 4 [52160/72641]\tLoss: 0.6596\tLR: 0.000010\n",
      "Training Epoch: 4 [52480/72641]\tLoss: 0.6956\tLR: 0.000010\n",
      "Training Epoch: 4 [52800/72641]\tLoss: 0.6986\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [53120/72641]\tLoss: 0.6697\tLR: 0.000010\n",
      "Training Epoch: 4 [53440/72641]\tLoss: 0.6950\tLR: 0.000010\n",
      "Training Epoch: 4 [53760/72641]\tLoss: 0.7043\tLR: 0.000010\n",
      "Training Epoch: 4 [54080/72641]\tLoss: 0.6812\tLR: 0.000010\n",
      "Training Epoch: 4 [54400/72641]\tLoss: 0.7224\tLR: 0.000010\n",
      "Training Epoch: 4 [54720/72641]\tLoss: 0.7127\tLR: 0.000010\n",
      "Training Epoch: 4 [55040/72641]\tLoss: 0.7079\tLR: 0.000010\n",
      "Training Epoch: 4 [55360/72641]\tLoss: 0.6804\tLR: 0.000010\n",
      "Training Epoch: 4 [55680/72641]\tLoss: 0.6642\tLR: 0.000010\n",
      "Training Epoch: 4 [56000/72641]\tLoss: 0.7069\tLR: 0.000010\n",
      "Training Epoch: 4 [56320/72641]\tLoss: 0.6853\tLR: 0.000010\n",
      "Training Epoch: 4 [56640/72641]\tLoss: 0.7412\tLR: 0.000010\n",
      "Training Epoch: 4 [56960/72641]\tLoss: 0.6840\tLR: 0.000010\n",
      "Training Epoch: 4 [57280/72641]\tLoss: 0.6981\tLR: 0.000010\n",
      "Training Epoch: 4 [57600/72641]\tLoss: 0.7090\tLR: 0.000010\n",
      "Training Epoch: 4 [57920/72641]\tLoss: 0.7017\tLR: 0.000010\n",
      "Training Epoch: 4 [58240/72641]\tLoss: 0.6732\tLR: 0.000010\n",
      "Training Epoch: 4 [58560/72641]\tLoss: 0.6747\tLR: 0.000010\n",
      "Training Epoch: 4 [58880/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 4 [59200/72641]\tLoss: 0.6874\tLR: 0.000010\n",
      "Training Epoch: 4 [59520/72641]\tLoss: 0.7220\tLR: 0.000010\n",
      "Training Epoch: 4 [59840/72641]\tLoss: 0.7257\tLR: 0.000010\n",
      "Training Epoch: 4 [60160/72641]\tLoss: 0.6678\tLR: 0.000010\n",
      "Training Epoch: 4 [60480/72641]\tLoss: 0.6727\tLR: 0.000010\n",
      "Training Epoch: 4 [60800/72641]\tLoss: 0.6806\tLR: 0.000010\n",
      "Training Epoch: 4 [61120/72641]\tLoss: 0.6760\tLR: 0.000010\n",
      "Training Epoch: 4 [61440/72641]\tLoss: 0.6494\tLR: 0.000010\n",
      "Training Epoch: 4 [61760/72641]\tLoss: 0.7205\tLR: 0.000010\n",
      "Training Epoch: 4 [62080/72641]\tLoss: 0.7082\tLR: 0.000010\n",
      "Training Epoch: 4 [62400/72641]\tLoss: 0.6853\tLR: 0.000010\n",
      "Training Epoch: 4 [62720/72641]\tLoss: 0.7137\tLR: 0.000010\n",
      "Training Epoch: 4 [63040/72641]\tLoss: 0.6554\tLR: 0.000010\n",
      "Training Epoch: 4 [63360/72641]\tLoss: 0.6905\tLR: 0.000010\n",
      "Training Epoch: 4 [63680/72641]\tLoss: 0.6945\tLR: 0.000010\n",
      "Training Epoch: 4 [64000/72641]\tLoss: 0.6661\tLR: 0.000010\n",
      "Training Epoch: 4 [64320/72641]\tLoss: 0.7136\tLR: 0.000010\n",
      "Training Epoch: 4 [64640/72641]\tLoss: 0.6943\tLR: 0.000010\n",
      "Training Epoch: 4 [64960/72641]\tLoss: 0.7004\tLR: 0.000010\n",
      "Training Epoch: 4 [65280/72641]\tLoss: 0.6850\tLR: 0.000010\n",
      "Training Epoch: 4 [65600/72641]\tLoss: 0.6469\tLR: 0.000010\n",
      "Training Epoch: 4 [65920/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 4 [66240/72641]\tLoss: 0.6963\tLR: 0.000010\n",
      "Training Epoch: 4 [66560/72641]\tLoss: 0.6830\tLR: 0.000010\n",
      "Training Epoch: 4 [66880/72641]\tLoss: 0.6763\tLR: 0.000010\n",
      "Training Epoch: 4 [67200/72641]\tLoss: 0.6823\tLR: 0.000010\n",
      "Training Epoch: 4 [67520/72641]\tLoss: 0.6932\tLR: 0.000010\n",
      "Training Epoch: 4 [67840/72641]\tLoss: 0.6897\tLR: 0.000010\n",
      "Training Epoch: 4 [68160/72641]\tLoss: 0.6398\tLR: 0.000010\n",
      "Training Epoch: 4 [68480/72641]\tLoss: 0.6630\tLR: 0.000010\n",
      "Training Epoch: 4 [68800/72641]\tLoss: 0.7296\tLR: 0.000010\n",
      "Training Epoch: 4 [69120/72641]\tLoss: 0.7126\tLR: 0.000010\n",
      "Training Epoch: 4 [69440/72641]\tLoss: 0.6686\tLR: 0.000010\n",
      "Training Epoch: 4 [69760/72641]\tLoss: 0.6558\tLR: 0.000010\n",
      "Training Epoch: 4 [70080/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 4 [70400/72641]\tLoss: 0.6998\tLR: 0.000010\n",
      "Training Epoch: 4 [70720/72641]\tLoss: 0.6881\tLR: 0.000010\n",
      "Training Epoch: 4 [71040/72641]\tLoss: 0.6781\tLR: 0.000010\n",
      "Training Epoch: 4 [71360/72641]\tLoss: 0.6911\tLR: 0.000010\n",
      "Training Epoch: 4 [71680/72641]\tLoss: 0.6775\tLR: 0.000010\n",
      "Training Epoch: 4 [72000/72641]\tLoss: 0.6732\tLR: 0.000010\n",
      "Training Epoch: 4 [72320/72641]\tLoss: 0.6902\tLR: 0.000010\n",
      "Training Epoch: 4 [72640/72641]\tLoss: 0.6707\tLR: 0.000010\n",
      "Val Result: Acc: 0.1476, C_ACC: 0.6741, DOA: 88.4987, ACC_k: 0.1050\n",
      "ext:0.0, cls:0.608106, coar:0.0, fine:0.0,\n",
      "Training Epoch: 5 [320/72641]\tLoss: 0.6891\tLR: 0.000010\n",
      "Training Epoch: 5 [640/72641]\tLoss: 0.6568\tLR: 0.000010\n",
      "Training Epoch: 5 [960/72641]\tLoss: 0.6638\tLR: 0.000010\n",
      "Training Epoch: 5 [1280/72641]\tLoss: 0.6878\tLR: 0.000010\n",
      "Training Epoch: 5 [1600/72641]\tLoss: 0.6999\tLR: 0.000010\n",
      "Training Epoch: 5 [1920/72641]\tLoss: 0.7348\tLR: 0.000010\n",
      "Training Epoch: 5 [2240/72641]\tLoss: 0.6959\tLR: 0.000010\n",
      "Training Epoch: 5 [2560/72641]\tLoss: 0.6958\tLR: 0.000010\n",
      "Training Epoch: 5 [2880/72641]\tLoss: 0.6945\tLR: 0.000010\n",
      "Training Epoch: 5 [3200/72641]\tLoss: 0.6893\tLR: 0.000010\n",
      "Training Epoch: 5 [3520/72641]\tLoss: 0.7098\tLR: 0.000010\n",
      "Training Epoch: 5 [3840/72641]\tLoss: 0.6551\tLR: 0.000010\n",
      "Training Epoch: 5 [4160/72641]\tLoss: 0.7283\tLR: 0.000010\n",
      "Training Epoch: 5 [4480/72641]\tLoss: 0.6635\tLR: 0.000010\n",
      "Training Epoch: 5 [4800/72641]\tLoss: 0.6837\tLR: 0.000010\n",
      "Training Epoch: 5 [5120/72641]\tLoss: 0.6682\tLR: 0.000010\n",
      "Training Epoch: 5 [5440/72641]\tLoss: 0.6775\tLR: 0.000010\n",
      "Training Epoch: 5 [5760/72641]\tLoss: 0.7022\tLR: 0.000010\n",
      "Training Epoch: 5 [6080/72641]\tLoss: 0.7031\tLR: 0.000010\n",
      "Training Epoch: 5 [6400/72641]\tLoss: 0.6646\tLR: 0.000010\n",
      "Training Epoch: 5 [6720/72641]\tLoss: 0.6695\tLR: 0.000010\n",
      "Training Epoch: 5 [7040/72641]\tLoss: 0.7313\tLR: 0.000010\n",
      "Training Epoch: 5 [7360/72641]\tLoss: 0.6791\tLR: 0.000010\n",
      "Training Epoch: 5 [7680/72641]\tLoss: 0.7059\tLR: 0.000010\n",
      "Training Epoch: 5 [8000/72641]\tLoss: 0.7235\tLR: 0.000010\n",
      "Training Epoch: 5 [8320/72641]\tLoss: 0.6900\tLR: 0.000010\n",
      "Training Epoch: 5 [8640/72641]\tLoss: 0.6879\tLR: 0.000010\n",
      "Training Epoch: 5 [8960/72641]\tLoss: 0.6977\tLR: 0.000010\n",
      "Training Epoch: 5 [9280/72641]\tLoss: 0.7154\tLR: 0.000010\n",
      "Training Epoch: 5 [9600/72641]\tLoss: 0.6998\tLR: 0.000010\n",
      "Training Epoch: 5 [9920/72641]\tLoss: 0.6986\tLR: 0.000010\n",
      "Training Epoch: 5 [10240/72641]\tLoss: 0.7044\tLR: 0.000010\n",
      "Training Epoch: 5 [10560/72641]\tLoss: 0.6587\tLR: 0.000010\n",
      "Training Epoch: 5 [10880/72641]\tLoss: 0.7172\tLR: 0.000010\n",
      "Training Epoch: 5 [11200/72641]\tLoss: 0.7025\tLR: 0.000010\n",
      "Training Epoch: 5 [11520/72641]\tLoss: 0.6777\tLR: 0.000010\n",
      "Training Epoch: 5 [11840/72641]\tLoss: 0.6898\tLR: 0.000010\n",
      "Training Epoch: 5 [12160/72641]\tLoss: 0.6644\tLR: 0.000010\n",
      "Training Epoch: 5 [12480/72641]\tLoss: 0.6932\tLR: 0.000010\n",
      "Training Epoch: 5 [12800/72641]\tLoss: 0.7317\tLR: 0.000010\n",
      "Training Epoch: 5 [13120/72641]\tLoss: 0.6826\tLR: 0.000010\n",
      "Training Epoch: 5 [13440/72641]\tLoss: 0.6964\tLR: 0.000010\n",
      "Training Epoch: 5 [13760/72641]\tLoss: 0.7124\tLR: 0.000010\n",
      "Training Epoch: 5 [14080/72641]\tLoss: 0.7073\tLR: 0.000010\n",
      "Training Epoch: 5 [14400/72641]\tLoss: 0.6670\tLR: 0.000010\n",
      "Training Epoch: 5 [14720/72641]\tLoss: 0.6623\tLR: 0.000010\n",
      "Training Epoch: 5 [15040/72641]\tLoss: 0.6688\tLR: 0.000010\n",
      "Training Epoch: 5 [15360/72641]\tLoss: 0.7278\tLR: 0.000010\n",
      "Training Epoch: 5 [15680/72641]\tLoss: 0.6940\tLR: 0.000010\n",
      "Training Epoch: 5 [16000/72641]\tLoss: 0.6843\tLR: 0.000010\n",
      "Training Epoch: 5 [16320/72641]\tLoss: 0.7036\tLR: 0.000010\n",
      "Training Epoch: 5 [16640/72641]\tLoss: 0.6979\tLR: 0.000010\n",
      "Training Epoch: 5 [16960/72641]\tLoss: 0.6616\tLR: 0.000010\n",
      "Training Epoch: 5 [17280/72641]\tLoss: 0.7190\tLR: 0.000010\n",
      "Training Epoch: 5 [17600/72641]\tLoss: 0.6371\tLR: 0.000010\n",
      "Training Epoch: 5 [17920/72641]\tLoss: 0.7058\tLR: 0.000010\n",
      "Training Epoch: 5 [18240/72641]\tLoss: 0.6826\tLR: 0.000010\n",
      "Training Epoch: 5 [18560/72641]\tLoss: 0.6460\tLR: 0.000010\n",
      "Training Epoch: 5 [18880/72641]\tLoss: 0.6778\tLR: 0.000010\n",
      "Training Epoch: 5 [19200/72641]\tLoss: 0.7304\tLR: 0.000010\n",
      "Training Epoch: 5 [19520/72641]\tLoss: 0.6872\tLR: 0.000010\n",
      "Training Epoch: 5 [19840/72641]\tLoss: 0.6700\tLR: 0.000010\n",
      "Training Epoch: 5 [20160/72641]\tLoss: 0.6477\tLR: 0.000010\n",
      "Training Epoch: 5 [20480/72641]\tLoss: 0.6915\tLR: 0.000010\n",
      "Training Epoch: 5 [20800/72641]\tLoss: 0.6922\tLR: 0.000010\n",
      "Training Epoch: 5 [21120/72641]\tLoss: 0.7063\tLR: 0.000010\n",
      "Training Epoch: 5 [21440/72641]\tLoss: 0.7054\tLR: 0.000010\n",
      "Training Epoch: 5 [21760/72641]\tLoss: 0.7211\tLR: 0.000010\n",
      "Training Epoch: 5 [22080/72641]\tLoss: 0.6670\tLR: 0.000010\n",
      "Training Epoch: 5 [22400/72641]\tLoss: 0.6855\tLR: 0.000010\n",
      "Training Epoch: 5 [22720/72641]\tLoss: 0.6712\tLR: 0.000010\n",
      "Training Epoch: 5 [23040/72641]\tLoss: 0.6789\tLR: 0.000010\n",
      "Training Epoch: 5 [23360/72641]\tLoss: 0.6716\tLR: 0.000010\n",
      "Training Epoch: 5 [23680/72641]\tLoss: 0.6915\tLR: 0.000010\n",
      "Training Epoch: 5 [24000/72641]\tLoss: 0.6938\tLR: 0.000010\n",
      "Training Epoch: 5 [24320/72641]\tLoss: 0.6748\tLR: 0.000010\n",
      "Training Epoch: 5 [24640/72641]\tLoss: 0.6926\tLR: 0.000010\n",
      "Training Epoch: 5 [24960/72641]\tLoss: 0.6818\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [25280/72641]\tLoss: 0.6901\tLR: 0.000010\n",
      "Training Epoch: 5 [25600/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 5 [25920/72641]\tLoss: 0.6861\tLR: 0.000010\n",
      "Training Epoch: 5 [26240/72641]\tLoss: 0.6804\tLR: 0.000010\n",
      "Training Epoch: 5 [26560/72641]\tLoss: 0.6938\tLR: 0.000010\n",
      "Training Epoch: 5 [26880/72641]\tLoss: 0.7049\tLR: 0.000010\n",
      "Training Epoch: 5 [27200/72641]\tLoss: 0.6994\tLR: 0.000010\n",
      "Training Epoch: 5 [27520/72641]\tLoss: 0.7036\tLR: 0.000010\n",
      "Training Epoch: 5 [27840/72641]\tLoss: 0.7214\tLR: 0.000010\n",
      "Training Epoch: 5 [28160/72641]\tLoss: 0.6555\tLR: 0.000010\n",
      "Training Epoch: 5 [28480/72641]\tLoss: 0.6654\tLR: 0.000010\n",
      "Training Epoch: 5 [28800/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 5 [29120/72641]\tLoss: 0.6690\tLR: 0.000010\n",
      "Training Epoch: 5 [29440/72641]\tLoss: 0.6667\tLR: 0.000010\n",
      "Training Epoch: 5 [29760/72641]\tLoss: 0.7387\tLR: 0.000010\n",
      "Training Epoch: 5 [30080/72641]\tLoss: 0.6924\tLR: 0.000010\n",
      "Training Epoch: 5 [30400/72641]\tLoss: 0.6798\tLR: 0.000010\n",
      "Training Epoch: 5 [30720/72641]\tLoss: 0.6692\tLR: 0.000010\n",
      "Training Epoch: 5 [31040/72641]\tLoss: 0.6786\tLR: 0.000010\n",
      "Training Epoch: 5 [31360/72641]\tLoss: 0.6626\tLR: 0.000010\n",
      "Training Epoch: 5 [31680/72641]\tLoss: 0.6973\tLR: 0.000010\n",
      "Training Epoch: 5 [32000/72641]\tLoss: 0.6734\tLR: 0.000010\n",
      "Training Epoch: 5 [32320/72641]\tLoss: 0.6624\tLR: 0.000010\n",
      "Training Epoch: 5 [32640/72641]\tLoss: 0.7100\tLR: 0.000010\n",
      "Training Epoch: 5 [32960/72641]\tLoss: 0.6973\tLR: 0.000010\n",
      "Training Epoch: 5 [33280/72641]\tLoss: 0.6619\tLR: 0.000010\n",
      "Training Epoch: 5 [33600/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 5 [33920/72641]\tLoss: 0.6429\tLR: 0.000010\n",
      "Training Epoch: 5 [34240/72641]\tLoss: 0.6667\tLR: 0.000010\n",
      "Training Epoch: 5 [34560/72641]\tLoss: 0.7006\tLR: 0.000010\n",
      "Training Epoch: 5 [34880/72641]\tLoss: 0.6743\tLR: 0.000010\n",
      "Training Epoch: 5 [35200/72641]\tLoss: 0.6634\tLR: 0.000010\n",
      "Training Epoch: 5 [35520/72641]\tLoss: 0.6626\tLR: 0.000010\n",
      "Training Epoch: 5 [35840/72641]\tLoss: 0.7099\tLR: 0.000010\n",
      "Training Epoch: 5 [36160/72641]\tLoss: 0.6822\tLR: 0.000010\n",
      "Training Epoch: 5 [36480/72641]\tLoss: 0.6850\tLR: 0.000010\n",
      "Training Epoch: 5 [36800/72641]\tLoss: 0.7035\tLR: 0.000010\n",
      "Training Epoch: 5 [37120/72641]\tLoss: 0.6706\tLR: 0.000010\n",
      "Training Epoch: 5 [37440/72641]\tLoss: 0.6950\tLR: 0.000010\n",
      "Training Epoch: 5 [37760/72641]\tLoss: 0.6668\tLR: 0.000010\n",
      "Training Epoch: 5 [38080/72641]\tLoss: 0.6672\tLR: 0.000010\n",
      "Training Epoch: 5 [38400/72641]\tLoss: 0.7089\tLR: 0.000010\n",
      "Training Epoch: 5 [38720/72641]\tLoss: 0.6820\tLR: 0.000010\n",
      "Training Epoch: 5 [39040/72641]\tLoss: 0.6464\tLR: 0.000010\n",
      "Training Epoch: 5 [39360/72641]\tLoss: 0.6914\tLR: 0.000010\n",
      "Training Epoch: 5 [39680/72641]\tLoss: 0.6411\tLR: 0.000010\n",
      "Training Epoch: 5 [40000/72641]\tLoss: 0.6997\tLR: 0.000010\n",
      "Training Epoch: 5 [40320/72641]\tLoss: 0.6897\tLR: 0.000010\n",
      "Training Epoch: 5 [40640/72641]\tLoss: 0.6842\tLR: 0.000010\n",
      "Training Epoch: 5 [40960/72641]\tLoss: 0.7011\tLR: 0.000010\n",
      "Training Epoch: 5 [41280/72641]\tLoss: 0.7020\tLR: 0.000010\n",
      "Training Epoch: 5 [41600/72641]\tLoss: 0.6629\tLR: 0.000010\n",
      "Training Epoch: 5 [41920/72641]\tLoss: 0.6925\tLR: 0.000010\n",
      "Training Epoch: 5 [42240/72641]\tLoss: 0.6575\tLR: 0.000010\n",
      "Training Epoch: 5 [42560/72641]\tLoss: 0.6565\tLR: 0.000010\n",
      "Training Epoch: 5 [42880/72641]\tLoss: 0.7198\tLR: 0.000010\n",
      "Training Epoch: 5 [43200/72641]\tLoss: 0.6808\tLR: 0.000010\n",
      "Training Epoch: 5 [43520/72641]\tLoss: 0.6683\tLR: 0.000010\n",
      "Training Epoch: 5 [43840/72641]\tLoss: 0.7109\tLR: 0.000010\n",
      "Training Epoch: 5 [44160/72641]\tLoss: 0.6955\tLR: 0.000010\n",
      "Training Epoch: 5 [44480/72641]\tLoss: 0.6923\tLR: 0.000010\n",
      "Training Epoch: 5 [44800/72641]\tLoss: 0.6805\tLR: 0.000010\n",
      "Training Epoch: 5 [45120/72641]\tLoss: 0.7282\tLR: 0.000010\n",
      "Training Epoch: 5 [45440/72641]\tLoss: 0.7200\tLR: 0.000010\n",
      "Training Epoch: 5 [45760/72641]\tLoss: 0.6675\tLR: 0.000010\n",
      "Training Epoch: 5 [46080/72641]\tLoss: 0.6886\tLR: 0.000010\n",
      "Training Epoch: 5 [46400/72641]\tLoss: 0.7038\tLR: 0.000010\n",
      "Training Epoch: 5 [46720/72641]\tLoss: 0.6835\tLR: 0.000010\n",
      "Training Epoch: 5 [47040/72641]\tLoss: 0.6380\tLR: 0.000010\n",
      "Training Epoch: 5 [47360/72641]\tLoss: 0.6951\tLR: 0.000010\n",
      "Training Epoch: 5 [47680/72641]\tLoss: 0.6821\tLR: 0.000010\n",
      "Training Epoch: 5 [48000/72641]\tLoss: 0.6910\tLR: 0.000010\n",
      "Training Epoch: 5 [48320/72641]\tLoss: 0.7102\tLR: 0.000010\n",
      "Training Epoch: 5 [48640/72641]\tLoss: 0.6889\tLR: 0.000010\n",
      "Training Epoch: 5 [48960/72641]\tLoss: 0.6614\tLR: 0.000010\n",
      "Training Epoch: 5 [49280/72641]\tLoss: 0.6617\tLR: 0.000010\n",
      "Training Epoch: 5 [49600/72641]\tLoss: 0.6959\tLR: 0.000010\n",
      "Training Epoch: 5 [49920/72641]\tLoss: 0.6687\tLR: 0.000010\n",
      "Training Epoch: 5 [50240/72641]\tLoss: 0.7078\tLR: 0.000010\n",
      "Training Epoch: 5 [50560/72641]\tLoss: 0.6795\tLR: 0.000010\n",
      "Training Epoch: 5 [50880/72641]\tLoss: 0.6853\tLR: 0.000010\n",
      "Training Epoch: 5 [51200/72641]\tLoss: 0.6642\tLR: 0.000010\n",
      "Training Epoch: 5 [51520/72641]\tLoss: 0.6842\tLR: 0.000010\n",
      "Training Epoch: 5 [51840/72641]\tLoss: 0.6862\tLR: 0.000010\n",
      "Training Epoch: 5 [52160/72641]\tLoss: 0.6637\tLR: 0.000010\n",
      "Training Epoch: 5 [52480/72641]\tLoss: 0.6704\tLR: 0.000010\n",
      "Training Epoch: 5 [52800/72641]\tLoss: 0.6966\tLR: 0.000010\n",
      "Training Epoch: 5 [53120/72641]\tLoss: 0.6763\tLR: 0.000010\n",
      "Training Epoch: 5 [53440/72641]\tLoss: 0.6491\tLR: 0.000010\n",
      "Training Epoch: 5 [53760/72641]\tLoss: 0.6826\tLR: 0.000010\n",
      "Training Epoch: 5 [54080/72641]\tLoss: 0.7071\tLR: 0.000010\n",
      "Training Epoch: 5 [54400/72641]\tLoss: 0.6622\tLR: 0.000010\n",
      "Training Epoch: 5 [54720/72641]\tLoss: 0.6313\tLR: 0.000010\n",
      "Training Epoch: 5 [55040/72641]\tLoss: 0.6847\tLR: 0.000010\n",
      "Training Epoch: 5 [55360/72641]\tLoss: 0.6514\tLR: 0.000010\n",
      "Training Epoch: 5 [55680/72641]\tLoss: 0.7000\tLR: 0.000010\n",
      "Training Epoch: 5 [56000/72641]\tLoss: 0.6713\tLR: 0.000010\n",
      "Training Epoch: 5 [56320/72641]\tLoss: 0.6727\tLR: 0.000010\n",
      "Training Epoch: 5 [56640/72641]\tLoss: 0.7072\tLR: 0.000010\n",
      "Training Epoch: 5 [56960/72641]\tLoss: 0.6659\tLR: 0.000010\n",
      "Training Epoch: 5 [57280/72641]\tLoss: 0.6851\tLR: 0.000010\n",
      "Training Epoch: 5 [57600/72641]\tLoss: 0.6658\tLR: 0.000010\n",
      "Training Epoch: 5 [57920/72641]\tLoss: 0.7095\tLR: 0.000010\n",
      "Training Epoch: 5 [58240/72641]\tLoss: 0.6555\tLR: 0.000010\n",
      "Training Epoch: 5 [58560/72641]\tLoss: 0.7214\tLR: 0.000010\n",
      "Training Epoch: 5 [58880/72641]\tLoss: 0.6399\tLR: 0.000010\n",
      "Training Epoch: 5 [59200/72641]\tLoss: 0.6844\tLR: 0.000010\n",
      "Training Epoch: 5 [59520/72641]\tLoss: 0.6987\tLR: 0.000010\n",
      "Training Epoch: 5 [59840/72641]\tLoss: 0.7182\tLR: 0.000010\n",
      "Training Epoch: 5 [60160/72641]\tLoss: 0.6866\tLR: 0.000010\n",
      "Training Epoch: 5 [60480/72641]\tLoss: 0.6565\tLR: 0.000010\n",
      "Training Epoch: 5 [60800/72641]\tLoss: 0.6866\tLR: 0.000010\n",
      "Training Epoch: 5 [61120/72641]\tLoss: 0.6885\tLR: 0.000010\n",
      "Training Epoch: 5 [61440/72641]\tLoss: 0.6416\tLR: 0.000010\n",
      "Training Epoch: 5 [61760/72641]\tLoss: 0.6464\tLR: 0.000010\n",
      "Training Epoch: 5 [62080/72641]\tLoss: 0.6656\tLR: 0.000010\n",
      "Training Epoch: 5 [62400/72641]\tLoss: 0.6517\tLR: 0.000010\n",
      "Training Epoch: 5 [62720/72641]\tLoss: 0.6514\tLR: 0.000010\n",
      "Training Epoch: 5 [63040/72641]\tLoss: 0.6654\tLR: 0.000010\n",
      "Training Epoch: 5 [63360/72641]\tLoss: 0.6866\tLR: 0.000010\n",
      "Training Epoch: 5 [63680/72641]\tLoss: 0.6630\tLR: 0.000010\n",
      "Training Epoch: 5 [64000/72641]\tLoss: 0.6694\tLR: 0.000010\n",
      "Training Epoch: 5 [64320/72641]\tLoss: 0.6622\tLR: 0.000010\n",
      "Training Epoch: 5 [64640/72641]\tLoss: 0.6718\tLR: 0.000010\n",
      "Training Epoch: 5 [64960/72641]\tLoss: 0.6690\tLR: 0.000010\n",
      "Training Epoch: 5 [65280/72641]\tLoss: 0.6685\tLR: 0.000010\n",
      "Training Epoch: 5 [65600/72641]\tLoss: 0.6785\tLR: 0.000010\n",
      "Training Epoch: 5 [65920/72641]\tLoss: 0.6794\tLR: 0.000010\n",
      "Training Epoch: 5 [66240/72641]\tLoss: 0.6950\tLR: 0.000010\n",
      "Training Epoch: 5 [66560/72641]\tLoss: 0.6593\tLR: 0.000010\n",
      "Training Epoch: 5 [66880/72641]\tLoss: 0.6487\tLR: 0.000010\n",
      "Training Epoch: 5 [67200/72641]\tLoss: 0.6919\tLR: 0.000010\n",
      "Training Epoch: 5 [67520/72641]\tLoss: 0.7446\tLR: 0.000010\n",
      "Training Epoch: 5 [67840/72641]\tLoss: 0.6907\tLR: 0.000010\n",
      "Training Epoch: 5 [68160/72641]\tLoss: 0.6473\tLR: 0.000010\n",
      "Training Epoch: 5 [68480/72641]\tLoss: 0.7274\tLR: 0.000010\n",
      "Training Epoch: 5 [68800/72641]\tLoss: 0.7094\tLR: 0.000010\n",
      "Training Epoch: 5 [69120/72641]\tLoss: 0.6660\tLR: 0.000010\n",
      "Training Epoch: 5 [69440/72641]\tLoss: 0.6870\tLR: 0.000010\n",
      "Training Epoch: 5 [69760/72641]\tLoss: 0.6635\tLR: 0.000010\n",
      "Training Epoch: 5 [70080/72641]\tLoss: 0.7165\tLR: 0.000010\n",
      "Training Epoch: 5 [70400/72641]\tLoss: 0.7251\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [70720/72641]\tLoss: 0.7022\tLR: 0.000010\n",
      "Training Epoch: 5 [71040/72641]\tLoss: 0.6879\tLR: 0.000010\n",
      "Training Epoch: 5 [71360/72641]\tLoss: 0.6710\tLR: 0.000010\n",
      "Training Epoch: 5 [71680/72641]\tLoss: 0.6526\tLR: 0.000010\n",
      "Training Epoch: 5 [72000/72641]\tLoss: 0.6905\tLR: 0.000010\n",
      "Training Epoch: 5 [72320/72641]\tLoss: 0.6707\tLR: 0.000010\n",
      "Training Epoch: 5 [72640/72641]\tLoss: 0.6825\tLR: 0.000010\n",
      "Val Result: Acc: 0.1464, C_ACC: 0.6814, DOA: 88.4178, ACC_k: 0.1049\n",
      "ext:0.0, cls:0.600381, coar:0.0, fine:0.0,\n",
      "Training Epoch: 6 [320/72641]\tLoss: 0.6959\tLR: 0.000010\n",
      "Training Epoch: 6 [640/72641]\tLoss: 0.6487\tLR: 0.000010\n",
      "Training Epoch: 6 [960/72641]\tLoss: 0.6658\tLR: 0.000010\n",
      "Training Epoch: 6 [1280/72641]\tLoss: 0.6648\tLR: 0.000010\n",
      "Training Epoch: 6 [1600/72641]\tLoss: 0.7304\tLR: 0.000010\n",
      "Training Epoch: 6 [1920/72641]\tLoss: 0.6878\tLR: 0.000010\n",
      "Training Epoch: 6 [2240/72641]\tLoss: 0.6972\tLR: 0.000010\n",
      "Training Epoch: 6 [2560/72641]\tLoss: 0.7063\tLR: 0.000010\n",
      "Training Epoch: 6 [2880/72641]\tLoss: 0.6767\tLR: 0.000010\n",
      "Training Epoch: 6 [3200/72641]\tLoss: 0.7061\tLR: 0.000010\n",
      "Training Epoch: 6 [3520/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 6 [3840/72641]\tLoss: 0.6869\tLR: 0.000010\n",
      "Training Epoch: 6 [4160/72641]\tLoss: 0.6744\tLR: 0.000010\n",
      "Training Epoch: 6 [4480/72641]\tLoss: 0.6803\tLR: 0.000010\n",
      "Training Epoch: 6 [4800/72641]\tLoss: 0.6660\tLR: 0.000010\n",
      "Training Epoch: 6 [5120/72641]\tLoss: 0.6736\tLR: 0.000010\n",
      "Training Epoch: 6 [5440/72641]\tLoss: 0.7141\tLR: 0.000010\n",
      "Training Epoch: 6 [5760/72641]\tLoss: 0.6921\tLR: 0.000010\n",
      "Training Epoch: 6 [6080/72641]\tLoss: 0.6573\tLR: 0.000010\n",
      "Training Epoch: 6 [6400/72641]\tLoss: 0.6592\tLR: 0.000010\n",
      "Training Epoch: 6 [6720/72641]\tLoss: 0.6802\tLR: 0.000010\n",
      "Training Epoch: 6 [7040/72641]\tLoss: 0.7102\tLR: 0.000010\n",
      "Training Epoch: 6 [7360/72641]\tLoss: 0.6846\tLR: 0.000010\n",
      "Training Epoch: 6 [7680/72641]\tLoss: 0.6816\tLR: 0.000010\n",
      "Training Epoch: 6 [8000/72641]\tLoss: 0.6504\tLR: 0.000010\n",
      "Training Epoch: 6 [8320/72641]\tLoss: 0.6903\tLR: 0.000010\n",
      "Training Epoch: 6 [8640/72641]\tLoss: 0.6536\tLR: 0.000010\n",
      "Training Epoch: 6 [8960/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 6 [9280/72641]\tLoss: 0.6776\tLR: 0.000010\n",
      "Training Epoch: 6 [9600/72641]\tLoss: 0.7049\tLR: 0.000010\n",
      "Training Epoch: 6 [9920/72641]\tLoss: 0.7361\tLR: 0.000010\n",
      "Training Epoch: 6 [10240/72641]\tLoss: 0.6673\tLR: 0.000010\n",
      "Training Epoch: 6 [10560/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 6 [10880/72641]\tLoss: 0.6930\tLR: 0.000010\n",
      "Training Epoch: 6 [11200/72641]\tLoss: 0.6464\tLR: 0.000010\n",
      "Training Epoch: 6 [11520/72641]\tLoss: 0.6989\tLR: 0.000010\n",
      "Training Epoch: 6 [11840/72641]\tLoss: 0.6783\tLR: 0.000010\n",
      "Training Epoch: 6 [12160/72641]\tLoss: 0.6714\tLR: 0.000010\n",
      "Training Epoch: 6 [12480/72641]\tLoss: 0.6571\tLR: 0.000010\n",
      "Training Epoch: 6 [12800/72641]\tLoss: 0.6725\tLR: 0.000010\n",
      "Training Epoch: 6 [13120/72641]\tLoss: 0.6921\tLR: 0.000010\n",
      "Training Epoch: 6 [13440/72641]\tLoss: 0.6902\tLR: 0.000010\n",
      "Training Epoch: 6 [13760/72641]\tLoss: 0.7034\tLR: 0.000010\n",
      "Training Epoch: 6 [14080/72641]\tLoss: 0.6871\tLR: 0.000010\n",
      "Training Epoch: 6 [14400/72641]\tLoss: 0.7042\tLR: 0.000010\n",
      "Training Epoch: 6 [14720/72641]\tLoss: 0.7094\tLR: 0.000010\n",
      "Training Epoch: 6 [15040/72641]\tLoss: 0.6560\tLR: 0.000010\n",
      "Training Epoch: 6 [15360/72641]\tLoss: 0.7214\tLR: 0.000010\n",
      "Training Epoch: 6 [15680/72641]\tLoss: 0.6666\tLR: 0.000010\n",
      "Training Epoch: 6 [16000/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 6 [16320/72641]\tLoss: 0.7040\tLR: 0.000010\n",
      "Training Epoch: 6 [16640/72641]\tLoss: 0.6871\tLR: 0.000010\n",
      "Training Epoch: 6 [16960/72641]\tLoss: 0.7057\tLR: 0.000010\n",
      "Training Epoch: 6 [17280/72641]\tLoss: 0.6849\tLR: 0.000010\n",
      "Training Epoch: 6 [17600/72641]\tLoss: 0.6720\tLR: 0.000010\n",
      "Training Epoch: 6 [17920/72641]\tLoss: 0.6692\tLR: 0.000010\n",
      "Training Epoch: 6 [18240/72641]\tLoss: 0.6903\tLR: 0.000010\n",
      "Training Epoch: 6 [18560/72641]\tLoss: 0.6628\tLR: 0.000010\n",
      "Training Epoch: 6 [18880/72641]\tLoss: 0.7054\tLR: 0.000010\n",
      "Training Epoch: 6 [19200/72641]\tLoss: 0.6607\tLR: 0.000010\n",
      "Training Epoch: 6 [19520/72641]\tLoss: 0.6640\tLR: 0.000010\n",
      "Training Epoch: 6 [19840/72641]\tLoss: 0.6556\tLR: 0.000010\n",
      "Training Epoch: 6 [20160/72641]\tLoss: 0.6765\tLR: 0.000010\n",
      "Training Epoch: 6 [20480/72641]\tLoss: 0.6515\tLR: 0.000010\n",
      "Training Epoch: 6 [20800/72641]\tLoss: 0.6916\tLR: 0.000010\n",
      "Training Epoch: 6 [21120/72641]\tLoss: 0.6822\tLR: 0.000010\n",
      "Training Epoch: 6 [21440/72641]\tLoss: 0.6897\tLR: 0.000010\n",
      "Training Epoch: 6 [21760/72641]\tLoss: 0.6734\tLR: 0.000010\n",
      "Training Epoch: 6 [22080/72641]\tLoss: 0.6877\tLR: 0.000010\n",
      "Training Epoch: 6 [22400/72641]\tLoss: 0.6467\tLR: 0.000010\n",
      "Training Epoch: 6 [22720/72641]\tLoss: 0.6533\tLR: 0.000010\n",
      "Training Epoch: 6 [23040/72641]\tLoss: 0.6877\tLR: 0.000010\n",
      "Training Epoch: 6 [23360/72641]\tLoss: 0.6448\tLR: 0.000010\n",
      "Training Epoch: 6 [23680/72641]\tLoss: 0.6953\tLR: 0.000010\n",
      "Training Epoch: 6 [24000/72641]\tLoss: 0.6259\tLR: 0.000010\n",
      "Training Epoch: 6 [24320/72641]\tLoss: 0.6803\tLR: 0.000010\n",
      "Training Epoch: 6 [24640/72641]\tLoss: 0.6683\tLR: 0.000010\n",
      "Training Epoch: 6 [24960/72641]\tLoss: 0.7036\tLR: 0.000010\n",
      "Training Epoch: 6 [25280/72641]\tLoss: 0.6780\tLR: 0.000010\n",
      "Training Epoch: 6 [25600/72641]\tLoss: 0.7039\tLR: 0.000010\n",
      "Training Epoch: 6 [25920/72641]\tLoss: 0.6824\tLR: 0.000010\n",
      "Training Epoch: 6 [26240/72641]\tLoss: 0.6940\tLR: 0.000010\n",
      "Training Epoch: 6 [26560/72641]\tLoss: 0.6670\tLR: 0.000010\n",
      "Training Epoch: 6 [26880/72641]\tLoss: 0.6721\tLR: 0.000010\n",
      "Training Epoch: 6 [27200/72641]\tLoss: 0.6555\tLR: 0.000010\n",
      "Training Epoch: 6 [27520/72641]\tLoss: 0.7085\tLR: 0.000010\n",
      "Training Epoch: 6 [27840/72641]\tLoss: 0.6748\tLR: 0.000010\n",
      "Training Epoch: 6 [28160/72641]\tLoss: 0.6897\tLR: 0.000010\n",
      "Training Epoch: 6 [28480/72641]\tLoss: 0.6735\tLR: 0.000010\n",
      "Training Epoch: 6 [28800/72641]\tLoss: 0.6728\tLR: 0.000010\n",
      "Training Epoch: 6 [29120/72641]\tLoss: 0.6979\tLR: 0.000010\n",
      "Training Epoch: 6 [29440/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 6 [29760/72641]\tLoss: 0.6195\tLR: 0.000010\n",
      "Training Epoch: 6 [30080/72641]\tLoss: 0.7128\tLR: 0.000010\n",
      "Training Epoch: 6 [30400/72641]\tLoss: 0.7184\tLR: 0.000010\n",
      "Training Epoch: 6 [30720/72641]\tLoss: 0.6792\tLR: 0.000010\n",
      "Training Epoch: 6 [31040/72641]\tLoss: 0.6544\tLR: 0.000010\n",
      "Training Epoch: 6 [31360/72641]\tLoss: 0.6734\tLR: 0.000010\n",
      "Training Epoch: 6 [31680/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 6 [32000/72641]\tLoss: 0.6881\tLR: 0.000010\n",
      "Training Epoch: 6 [32320/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 6 [32640/72641]\tLoss: 0.6873\tLR: 0.000010\n",
      "Training Epoch: 6 [32960/72641]\tLoss: 0.7057\tLR: 0.000010\n",
      "Training Epoch: 6 [33280/72641]\tLoss: 0.6783\tLR: 0.000010\n",
      "Training Epoch: 6 [33600/72641]\tLoss: 0.6344\tLR: 0.000010\n",
      "Training Epoch: 6 [33920/72641]\tLoss: 0.6787\tLR: 0.000010\n",
      "Training Epoch: 6 [34240/72641]\tLoss: 0.6991\tLR: 0.000010\n",
      "Training Epoch: 6 [34560/72641]\tLoss: 0.7132\tLR: 0.000010\n",
      "Training Epoch: 6 [34880/72641]\tLoss: 0.6708\tLR: 0.000010\n",
      "Training Epoch: 6 [35200/72641]\tLoss: 0.6795\tLR: 0.000010\n",
      "Training Epoch: 6 [35520/72641]\tLoss: 0.6446\tLR: 0.000010\n",
      "Training Epoch: 6 [35840/72641]\tLoss: 0.7170\tLR: 0.000010\n",
      "Training Epoch: 6 [36160/72641]\tLoss: 0.6538\tLR: 0.000010\n",
      "Training Epoch: 6 [36480/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 6 [36800/72641]\tLoss: 0.6817\tLR: 0.000010\n",
      "Training Epoch: 6 [37120/72641]\tLoss: 0.6686\tLR: 0.000010\n",
      "Training Epoch: 6 [37440/72641]\tLoss: 0.6866\tLR: 0.000010\n",
      "Training Epoch: 6 [37760/72641]\tLoss: 0.6892\tLR: 0.000010\n",
      "Training Epoch: 6 [38080/72641]\tLoss: 0.6708\tLR: 0.000010\n",
      "Training Epoch: 6 [38400/72641]\tLoss: 0.6991\tLR: 0.000010\n",
      "Training Epoch: 6 [38720/72641]\tLoss: 0.6956\tLR: 0.000010\n",
      "Training Epoch: 6 [39040/72641]\tLoss: 0.6761\tLR: 0.000010\n",
      "Training Epoch: 6 [39360/72641]\tLoss: 0.7108\tLR: 0.000010\n",
      "Training Epoch: 6 [39680/72641]\tLoss: 0.6944\tLR: 0.000010\n",
      "Training Epoch: 6 [40000/72641]\tLoss: 0.6874\tLR: 0.000010\n",
      "Training Epoch: 6 [40320/72641]\tLoss: 0.6435\tLR: 0.000010\n",
      "Training Epoch: 6 [40640/72641]\tLoss: 0.6763\tLR: 0.000010\n",
      "Training Epoch: 6 [40960/72641]\tLoss: 0.6481\tLR: 0.000010\n",
      "Training Epoch: 6 [41280/72641]\tLoss: 0.7290\tLR: 0.000010\n",
      "Training Epoch: 6 [41600/72641]\tLoss: 0.6895\tLR: 0.000010\n",
      "Training Epoch: 6 [41920/72641]\tLoss: 0.6888\tLR: 0.000010\n",
      "Training Epoch: 6 [42240/72641]\tLoss: 0.6724\tLR: 0.000010\n",
      "Training Epoch: 6 [42560/72641]\tLoss: 0.6473\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [42880/72641]\tLoss: 0.7055\tLR: 0.000010\n",
      "Training Epoch: 6 [43200/72641]\tLoss: 0.7081\tLR: 0.000010\n",
      "Training Epoch: 6 [43520/72641]\tLoss: 0.6911\tLR: 0.000010\n",
      "Training Epoch: 6 [43840/72641]\tLoss: 0.6972\tLR: 0.000010\n",
      "Training Epoch: 6 [44160/72641]\tLoss: 0.7235\tLR: 0.000010\n",
      "Training Epoch: 6 [44480/72641]\tLoss: 0.6620\tLR: 0.000010\n",
      "Training Epoch: 6 [44800/72641]\tLoss: 0.6534\tLR: 0.000010\n",
      "Training Epoch: 6 [45120/72641]\tLoss: 0.6853\tLR: 0.000010\n",
      "Training Epoch: 6 [45440/72641]\tLoss: 0.6651\tLR: 0.000010\n",
      "Training Epoch: 6 [45760/72641]\tLoss: 0.6876\tLR: 0.000010\n",
      "Training Epoch: 6 [46080/72641]\tLoss: 0.6903\tLR: 0.000010\n",
      "Training Epoch: 6 [46400/72641]\tLoss: 0.7010\tLR: 0.000010\n",
      "Training Epoch: 6 [46720/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 6 [47040/72641]\tLoss: 0.6943\tLR: 0.000010\n",
      "Training Epoch: 6 [47360/72641]\tLoss: 0.6676\tLR: 0.000010\n",
      "Training Epoch: 6 [47680/72641]\tLoss: 0.6680\tLR: 0.000010\n",
      "Training Epoch: 6 [48000/72641]\tLoss: 0.6833\tLR: 0.000010\n",
      "Training Epoch: 6 [48320/72641]\tLoss: 0.6962\tLR: 0.000010\n",
      "Training Epoch: 6 [48640/72641]\tLoss: 0.6880\tLR: 0.000010\n",
      "Training Epoch: 6 [48960/72641]\tLoss: 0.6803\tLR: 0.000010\n",
      "Training Epoch: 6 [49280/72641]\tLoss: 0.6826\tLR: 0.000010\n",
      "Training Epoch: 6 [49600/72641]\tLoss: 0.6530\tLR: 0.000010\n",
      "Training Epoch: 6 [49920/72641]\tLoss: 0.6388\tLR: 0.000010\n",
      "Training Epoch: 6 [50240/72641]\tLoss: 0.6930\tLR: 0.000010\n",
      "Training Epoch: 6 [50560/72641]\tLoss: 0.6985\tLR: 0.000010\n",
      "Training Epoch: 6 [50880/72641]\tLoss: 0.6776\tLR: 0.000010\n",
      "Training Epoch: 6 [51200/72641]\tLoss: 0.7089\tLR: 0.000010\n",
      "Training Epoch: 6 [51520/72641]\tLoss: 0.6448\tLR: 0.000010\n",
      "Training Epoch: 6 [51840/72641]\tLoss: 0.6422\tLR: 0.000010\n",
      "Training Epoch: 6 [52160/72641]\tLoss: 0.7036\tLR: 0.000010\n",
      "Training Epoch: 6 [52480/72641]\tLoss: 0.6845\tLR: 0.000010\n",
      "Training Epoch: 6 [52800/72641]\tLoss: 0.6664\tLR: 0.000010\n",
      "Training Epoch: 6 [53120/72641]\tLoss: 0.6921\tLR: 0.000010\n",
      "Training Epoch: 6 [53440/72641]\tLoss: 0.6760\tLR: 0.000010\n",
      "Training Epoch: 6 [53760/72641]\tLoss: 0.7027\tLR: 0.000010\n",
      "Training Epoch: 6 [54080/72641]\tLoss: 0.6765\tLR: 0.000010\n",
      "Training Epoch: 6 [54400/72641]\tLoss: 0.6584\tLR: 0.000010\n",
      "Training Epoch: 6 [54720/72641]\tLoss: 0.7099\tLR: 0.000010\n",
      "Training Epoch: 6 [55040/72641]\tLoss: 0.7084\tLR: 0.000010\n",
      "Training Epoch: 6 [55360/72641]\tLoss: 0.7083\tLR: 0.000010\n",
      "Training Epoch: 6 [55680/72641]\tLoss: 0.6697\tLR: 0.000010\n",
      "Training Epoch: 6 [56000/72641]\tLoss: 0.6619\tLR: 0.000010\n",
      "Training Epoch: 6 [56320/72641]\tLoss: 0.6581\tLR: 0.000010\n",
      "Training Epoch: 6 [56640/72641]\tLoss: 0.6942\tLR: 0.000010\n",
      "Training Epoch: 6 [56960/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 6 [57280/72641]\tLoss: 0.6980\tLR: 0.000010\n",
      "Training Epoch: 6 [57600/72641]\tLoss: 0.6794\tLR: 0.000010\n",
      "Training Epoch: 6 [57920/72641]\tLoss: 0.6721\tLR: 0.000010\n",
      "Training Epoch: 6 [58240/72641]\tLoss: 0.6537\tLR: 0.000010\n",
      "Training Epoch: 6 [58560/72641]\tLoss: 0.6709\tLR: 0.000010\n",
      "Training Epoch: 6 [58880/72641]\tLoss: 0.6867\tLR: 0.000010\n",
      "Training Epoch: 6 [59200/72641]\tLoss: 0.6670\tLR: 0.000010\n",
      "Training Epoch: 6 [59520/72641]\tLoss: 0.6864\tLR: 0.000010\n",
      "Training Epoch: 6 [59840/72641]\tLoss: 0.6571\tLR: 0.000010\n",
      "Training Epoch: 6 [60160/72641]\tLoss: 0.6632\tLR: 0.000010\n",
      "Training Epoch: 6 [60480/72641]\tLoss: 0.7293\tLR: 0.000010\n",
      "Training Epoch: 6 [60800/72641]\tLoss: 0.6767\tLR: 0.000010\n",
      "Training Epoch: 6 [61120/72641]\tLoss: 0.6738\tLR: 0.000010\n",
      "Training Epoch: 6 [61440/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 6 [61760/72641]\tLoss: 0.6822\tLR: 0.000010\n",
      "Training Epoch: 6 [62080/72641]\tLoss: 0.7109\tLR: 0.000010\n",
      "Training Epoch: 6 [62400/72641]\tLoss: 0.6593\tLR: 0.000010\n",
      "Training Epoch: 6 [62720/72641]\tLoss: 0.6466\tLR: 0.000010\n",
      "Training Epoch: 6 [63040/72641]\tLoss: 0.6858\tLR: 0.000010\n",
      "Training Epoch: 6 [63360/72641]\tLoss: 0.6478\tLR: 0.000010\n",
      "Training Epoch: 6 [63680/72641]\tLoss: 0.6398\tLR: 0.000010\n",
      "Training Epoch: 6 [64000/72641]\tLoss: 0.6650\tLR: 0.000010\n",
      "Training Epoch: 6 [64320/72641]\tLoss: 0.6591\tLR: 0.000010\n",
      "Training Epoch: 6 [64640/72641]\tLoss: 0.6742\tLR: 0.000010\n",
      "Training Epoch: 6 [64960/72641]\tLoss: 0.6767\tLR: 0.000010\n",
      "Training Epoch: 6 [65280/72641]\tLoss: 0.7048\tLR: 0.000010\n",
      "Training Epoch: 6 [65600/72641]\tLoss: 0.6525\tLR: 0.000010\n",
      "Training Epoch: 6 [65920/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 6 [66240/72641]\tLoss: 0.6991\tLR: 0.000010\n",
      "Training Epoch: 6 [66560/72641]\tLoss: 0.7191\tLR: 0.000010\n",
      "Training Epoch: 6 [66880/72641]\tLoss: 0.6324\tLR: 0.000010\n",
      "Training Epoch: 6 [67200/72641]\tLoss: 0.6652\tLR: 0.000010\n",
      "Training Epoch: 6 [67520/72641]\tLoss: 0.7118\tLR: 0.000010\n",
      "Training Epoch: 6 [67840/72641]\tLoss: 0.6949\tLR: 0.000010\n",
      "Training Epoch: 6 [68160/72641]\tLoss: 0.6805\tLR: 0.000010\n",
      "Training Epoch: 6 [68480/72641]\tLoss: 0.6630\tLR: 0.000010\n",
      "Training Epoch: 6 [68800/72641]\tLoss: 0.7290\tLR: 0.000010\n",
      "Training Epoch: 6 [69120/72641]\tLoss: 0.6584\tLR: 0.000010\n",
      "Training Epoch: 6 [69440/72641]\tLoss: 0.6649\tLR: 0.000010\n",
      "Training Epoch: 6 [69760/72641]\tLoss: 0.6791\tLR: 0.000010\n",
      "Training Epoch: 6 [70080/72641]\tLoss: 0.6423\tLR: 0.000010\n",
      "Training Epoch: 6 [70400/72641]\tLoss: 0.6947\tLR: 0.000010\n",
      "Training Epoch: 6 [70720/72641]\tLoss: 0.6772\tLR: 0.000010\n",
      "Training Epoch: 6 [71040/72641]\tLoss: 0.7007\tLR: 0.000010\n",
      "Training Epoch: 6 [71360/72641]\tLoss: 0.6990\tLR: 0.000010\n",
      "Training Epoch: 6 [71680/72641]\tLoss: 0.6946\tLR: 0.000010\n",
      "Training Epoch: 6 [72000/72641]\tLoss: 0.6549\tLR: 0.000010\n",
      "Training Epoch: 6 [72320/72641]\tLoss: 0.6565\tLR: 0.000010\n",
      "Training Epoch: 6 [72640/72641]\tLoss: 0.6390\tLR: 0.000010\n",
      "Val Result: Acc: 0.1416, C_ACC: 0.6905, DOA: 88.7745, ACC_k: 0.0954\n",
      "ext:0.0, cls:0.591826, coar:0.0, fine:0.0,\n",
      "Training Epoch: 7 [320/72641]\tLoss: 0.6419\tLR: 0.000010\n",
      "Training Epoch: 7 [640/72641]\tLoss: 0.6846\tLR: 0.000010\n",
      "Training Epoch: 7 [960/72641]\tLoss: 0.6855\tLR: 0.000010\n",
      "Training Epoch: 7 [1280/72641]\tLoss: 0.6397\tLR: 0.000010\n",
      "Training Epoch: 7 [1600/72641]\tLoss: 0.7245\tLR: 0.000010\n",
      "Training Epoch: 7 [1920/72641]\tLoss: 0.6799\tLR: 0.000010\n",
      "Training Epoch: 7 [2240/72641]\tLoss: 0.6958\tLR: 0.000010\n",
      "Training Epoch: 7 [2560/72641]\tLoss: 0.6661\tLR: 0.000010\n",
      "Training Epoch: 7 [2880/72641]\tLoss: 0.6773\tLR: 0.000010\n",
      "Training Epoch: 7 [3200/72641]\tLoss: 0.6996\tLR: 0.000010\n",
      "Training Epoch: 7 [3520/72641]\tLoss: 0.6553\tLR: 0.000010\n",
      "Training Epoch: 7 [3840/72641]\tLoss: 0.6689\tLR: 0.000010\n",
      "Training Epoch: 7 [4160/72641]\tLoss: 0.6716\tLR: 0.000010\n",
      "Training Epoch: 7 [4480/72641]\tLoss: 0.6248\tLR: 0.000010\n",
      "Training Epoch: 7 [4800/72641]\tLoss: 0.6932\tLR: 0.000010\n",
      "Training Epoch: 7 [5120/72641]\tLoss: 0.6717\tLR: 0.000010\n",
      "Training Epoch: 7 [5440/72641]\tLoss: 0.6504\tLR: 0.000010\n",
      "Training Epoch: 7 [5760/72641]\tLoss: 0.6941\tLR: 0.000010\n",
      "Training Epoch: 7 [6080/72641]\tLoss: 0.6999\tLR: 0.000010\n",
      "Training Epoch: 7 [6400/72641]\tLoss: 0.7056\tLR: 0.000010\n",
      "Training Epoch: 7 [6720/72641]\tLoss: 0.7062\tLR: 0.000010\n",
      "Training Epoch: 7 [7040/72641]\tLoss: 0.6660\tLR: 0.000010\n",
      "Training Epoch: 7 [7360/72641]\tLoss: 0.6528\tLR: 0.000010\n",
      "Training Epoch: 7 [7680/72641]\tLoss: 0.7152\tLR: 0.000010\n",
      "Training Epoch: 7 [8000/72641]\tLoss: 0.6893\tLR: 0.000010\n",
      "Training Epoch: 7 [8320/72641]\tLoss: 0.6631\tLR: 0.000010\n",
      "Training Epoch: 7 [8640/72641]\tLoss: 0.6892\tLR: 0.000010\n",
      "Training Epoch: 7 [8960/72641]\tLoss: 0.6691\tLR: 0.000010\n",
      "Training Epoch: 7 [9280/72641]\tLoss: 0.6418\tLR: 0.000010\n",
      "Training Epoch: 7 [9600/72641]\tLoss: 0.6856\tLR: 0.000010\n",
      "Training Epoch: 7 [9920/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 7 [10240/72641]\tLoss: 0.6500\tLR: 0.000010\n",
      "Training Epoch: 7 [10560/72641]\tLoss: 0.6458\tLR: 0.000010\n",
      "Training Epoch: 7 [10880/72641]\tLoss: 0.6629\tLR: 0.000010\n",
      "Training Epoch: 7 [11200/72641]\tLoss: 0.6519\tLR: 0.000010\n",
      "Training Epoch: 7 [11520/72641]\tLoss: 0.6489\tLR: 0.000010\n",
      "Training Epoch: 7 [11840/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 7 [12160/72641]\tLoss: 0.6617\tLR: 0.000010\n",
      "Training Epoch: 7 [12480/72641]\tLoss: 0.6558\tLR: 0.000010\n",
      "Training Epoch: 7 [12800/72641]\tLoss: 0.6501\tLR: 0.000010\n",
      "Training Epoch: 7 [13120/72641]\tLoss: 0.6591\tLR: 0.000010\n",
      "Training Epoch: 7 [13440/72641]\tLoss: 0.6861\tLR: 0.000010\n",
      "Training Epoch: 7 [13760/72641]\tLoss: 0.6464\tLR: 0.000010\n",
      "Training Epoch: 7 [14080/72641]\tLoss: 0.6647\tLR: 0.000010\n",
      "Training Epoch: 7 [14400/72641]\tLoss: 0.6611\tLR: 0.000010\n",
      "Training Epoch: 7 [14720/72641]\tLoss: 0.6471\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [15040/72641]\tLoss: 0.6455\tLR: 0.000010\n",
      "Training Epoch: 7 [15360/72641]\tLoss: 0.6566\tLR: 0.000010\n",
      "Training Epoch: 7 [15680/72641]\tLoss: 0.6523\tLR: 0.000010\n",
      "Training Epoch: 7 [16000/72641]\tLoss: 0.6684\tLR: 0.000010\n",
      "Training Epoch: 7 [16320/72641]\tLoss: 0.6287\tLR: 0.000010\n",
      "Training Epoch: 7 [16640/72641]\tLoss: 0.6713\tLR: 0.000010\n",
      "Training Epoch: 7 [16960/72641]\tLoss: 0.6722\tLR: 0.000010\n",
      "Training Epoch: 7 [17280/72641]\tLoss: 0.6332\tLR: 0.000010\n",
      "Training Epoch: 7 [17600/72641]\tLoss: 0.6436\tLR: 0.000010\n",
      "Training Epoch: 7 [17920/72641]\tLoss: 0.7288\tLR: 0.000010\n",
      "Training Epoch: 7 [18240/72641]\tLoss: 0.6773\tLR: 0.000010\n",
      "Training Epoch: 7 [18560/72641]\tLoss: 0.6772\tLR: 0.000010\n",
      "Training Epoch: 7 [18880/72641]\tLoss: 0.6972\tLR: 0.000010\n",
      "Training Epoch: 7 [19200/72641]\tLoss: 0.6662\tLR: 0.000010\n",
      "Training Epoch: 7 [19520/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 7 [19840/72641]\tLoss: 0.6709\tLR: 0.000010\n",
      "Training Epoch: 7 [20160/72641]\tLoss: 0.6491\tLR: 0.000010\n",
      "Training Epoch: 7 [20480/72641]\tLoss: 0.6734\tLR: 0.000010\n",
      "Training Epoch: 7 [20800/72641]\tLoss: 0.6950\tLR: 0.000010\n",
      "Training Epoch: 7 [21120/72641]\tLoss: 0.6521\tLR: 0.000010\n",
      "Training Epoch: 7 [21440/72641]\tLoss: 0.6426\tLR: 0.000010\n",
      "Training Epoch: 7 [21760/72641]\tLoss: 0.6723\tLR: 0.000010\n",
      "Training Epoch: 7 [22080/72641]\tLoss: 0.7021\tLR: 0.000010\n",
      "Training Epoch: 7 [22400/72641]\tLoss: 0.6930\tLR: 0.000010\n",
      "Training Epoch: 7 [22720/72641]\tLoss: 0.6574\tLR: 0.000010\n",
      "Training Epoch: 7 [23040/72641]\tLoss: 0.6970\tLR: 0.000010\n",
      "Training Epoch: 7 [23360/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 7 [23680/72641]\tLoss: 0.6882\tLR: 0.000010\n",
      "Training Epoch: 7 [24000/72641]\tLoss: 0.7036\tLR: 0.000010\n",
      "Training Epoch: 7 [24320/72641]\tLoss: 0.6843\tLR: 0.000010\n",
      "Training Epoch: 7 [24640/72641]\tLoss: 0.6770\tLR: 0.000010\n",
      "Training Epoch: 7 [24960/72641]\tLoss: 0.6659\tLR: 0.000010\n",
      "Training Epoch: 7 [25280/72641]\tLoss: 0.6708\tLR: 0.000010\n",
      "Training Epoch: 7 [25600/72641]\tLoss: 0.6578\tLR: 0.000010\n",
      "Training Epoch: 7 [25920/72641]\tLoss: 0.6464\tLR: 0.000010\n",
      "Training Epoch: 7 [26240/72641]\tLoss: 0.7153\tLR: 0.000010\n",
      "Training Epoch: 7 [26560/72641]\tLoss: 0.6966\tLR: 0.000010\n",
      "Training Epoch: 7 [26880/72641]\tLoss: 0.6603\tLR: 0.000010\n",
      "Training Epoch: 7 [27200/72641]\tLoss: 0.6632\tLR: 0.000010\n",
      "Training Epoch: 7 [27520/72641]\tLoss: 0.6717\tLR: 0.000010\n",
      "Training Epoch: 7 [27840/72641]\tLoss: 0.6718\tLR: 0.000010\n",
      "Training Epoch: 7 [28160/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 7 [28480/72641]\tLoss: 0.7014\tLR: 0.000010\n",
      "Training Epoch: 7 [28800/72641]\tLoss: 0.6422\tLR: 0.000010\n",
      "Training Epoch: 7 [29120/72641]\tLoss: 0.6563\tLR: 0.000010\n",
      "Training Epoch: 7 [29440/72641]\tLoss: 0.6463\tLR: 0.000010\n",
      "Training Epoch: 7 [29760/72641]\tLoss: 0.6514\tLR: 0.000010\n",
      "Training Epoch: 7 [30080/72641]\tLoss: 0.6801\tLR: 0.000010\n",
      "Training Epoch: 7 [30400/72641]\tLoss: 0.6969\tLR: 0.000010\n",
      "Training Epoch: 7 [30720/72641]\tLoss: 0.6612\tLR: 0.000010\n",
      "Training Epoch: 7 [31040/72641]\tLoss: 0.6484\tLR: 0.000010\n",
      "Training Epoch: 7 [31360/72641]\tLoss: 0.6797\tLR: 0.000010\n",
      "Training Epoch: 7 [31680/72641]\tLoss: 0.7069\tLR: 0.000010\n",
      "Training Epoch: 7 [32000/72641]\tLoss: 0.6636\tLR: 0.000010\n",
      "Training Epoch: 7 [32320/72641]\tLoss: 0.6598\tLR: 0.000010\n",
      "Training Epoch: 7 [32640/72641]\tLoss: 0.6415\tLR: 0.000010\n",
      "Training Epoch: 7 [32960/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 7 [33280/72641]\tLoss: 0.6693\tLR: 0.000010\n",
      "Training Epoch: 7 [33600/72641]\tLoss: 0.6469\tLR: 0.000010\n",
      "Training Epoch: 7 [33920/72641]\tLoss: 0.6654\tLR: 0.000010\n",
      "Training Epoch: 7 [34240/72641]\tLoss: 0.6604\tLR: 0.000010\n",
      "Training Epoch: 7 [34560/72641]\tLoss: 0.6537\tLR: 0.000010\n",
      "Training Epoch: 7 [34880/72641]\tLoss: 0.6926\tLR: 0.000010\n",
      "Training Epoch: 7 [35200/72641]\tLoss: 0.6739\tLR: 0.000010\n",
      "Training Epoch: 7 [35520/72641]\tLoss: 0.6766\tLR: 0.000010\n",
      "Training Epoch: 7 [35840/72641]\tLoss: 0.6881\tLR: 0.000010\n",
      "Training Epoch: 7 [36160/72641]\tLoss: 0.6764\tLR: 0.000010\n",
      "Training Epoch: 7 [36480/72641]\tLoss: 0.6878\tLR: 0.000010\n",
      "Training Epoch: 7 [36800/72641]\tLoss: 0.6815\tLR: 0.000010\n",
      "Training Epoch: 7 [37120/72641]\tLoss: 0.6575\tLR: 0.000010\n",
      "Training Epoch: 7 [37440/72641]\tLoss: 0.6934\tLR: 0.000010\n",
      "Training Epoch: 7 [37760/72641]\tLoss: 0.7032\tLR: 0.000010\n",
      "Training Epoch: 7 [38080/72641]\tLoss: 0.6994\tLR: 0.000010\n",
      "Training Epoch: 7 [38400/72641]\tLoss: 0.6732\tLR: 0.000010\n",
      "Training Epoch: 7 [38720/72641]\tLoss: 0.6707\tLR: 0.000010\n",
      "Training Epoch: 7 [39040/72641]\tLoss: 0.6967\tLR: 0.000010\n",
      "Training Epoch: 7 [39360/72641]\tLoss: 0.6508\tLR: 0.000010\n",
      "Training Epoch: 7 [39680/72641]\tLoss: 0.6352\tLR: 0.000010\n",
      "Training Epoch: 7 [40000/72641]\tLoss: 0.6899\tLR: 0.000010\n",
      "Training Epoch: 7 [40320/72641]\tLoss: 0.6606\tLR: 0.000010\n",
      "Training Epoch: 7 [40640/72641]\tLoss: 0.6486\tLR: 0.000010\n",
      "Training Epoch: 7 [40960/72641]\tLoss: 0.6605\tLR: 0.000010\n",
      "Training Epoch: 7 [41280/72641]\tLoss: 0.6936\tLR: 0.000010\n",
      "Training Epoch: 7 [41600/72641]\tLoss: 0.6331\tLR: 0.000010\n",
      "Training Epoch: 7 [41920/72641]\tLoss: 0.6751\tLR: 0.000010\n",
      "Training Epoch: 7 [42240/72641]\tLoss: 0.6393\tLR: 0.000010\n",
      "Training Epoch: 7 [42560/72641]\tLoss: 0.6868\tLR: 0.000010\n",
      "Training Epoch: 7 [42880/72641]\tLoss: 0.6815\tLR: 0.000010\n",
      "Training Epoch: 7 [43200/72641]\tLoss: 0.6709\tLR: 0.000010\n",
      "Training Epoch: 7 [43520/72641]\tLoss: 0.6558\tLR: 0.000010\n",
      "Training Epoch: 7 [43840/72641]\tLoss: 0.6872\tLR: 0.000010\n",
      "Training Epoch: 7 [44160/72641]\tLoss: 0.6721\tLR: 0.000010\n",
      "Training Epoch: 7 [44480/72641]\tLoss: 0.6742\tLR: 0.000010\n",
      "Training Epoch: 7 [44800/72641]\tLoss: 0.6771\tLR: 0.000010\n",
      "Training Epoch: 7 [45120/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 7 [45440/72641]\tLoss: 0.6700\tLR: 0.000010\n",
      "Training Epoch: 7 [45760/72641]\tLoss: 0.6511\tLR: 0.000010\n",
      "Training Epoch: 7 [46080/72641]\tLoss: 0.6956\tLR: 0.000010\n",
      "Training Epoch: 7 [46400/72641]\tLoss: 0.6776\tLR: 0.000010\n",
      "Training Epoch: 7 [46720/72641]\tLoss: 0.6596\tLR: 0.000010\n",
      "Training Epoch: 7 [47040/72641]\tLoss: 0.6545\tLR: 0.000010\n",
      "Training Epoch: 7 [47360/72641]\tLoss: 0.6825\tLR: 0.000010\n",
      "Training Epoch: 7 [47680/72641]\tLoss: 0.6819\tLR: 0.000010\n",
      "Training Epoch: 7 [48000/72641]\tLoss: 0.7037\tLR: 0.000010\n",
      "Training Epoch: 7 [48320/72641]\tLoss: 0.7040\tLR: 0.000010\n",
      "Training Epoch: 7 [48640/72641]\tLoss: 0.6678\tLR: 0.000010\n",
      "Training Epoch: 7 [48960/72641]\tLoss: 0.6786\tLR: 0.000010\n",
      "Training Epoch: 7 [49280/72641]\tLoss: 0.6750\tLR: 0.000010\n",
      "Training Epoch: 7 [49600/72641]\tLoss: 0.6963\tLR: 0.000010\n",
      "Training Epoch: 7 [49920/72641]\tLoss: 0.6638\tLR: 0.000010\n",
      "Training Epoch: 7 [50240/72641]\tLoss: 0.7005\tLR: 0.000010\n",
      "Training Epoch: 7 [50560/72641]\tLoss: 0.6394\tLR: 0.000010\n",
      "Training Epoch: 7 [50880/72641]\tLoss: 0.7067\tLR: 0.000010\n",
      "Training Epoch: 7 [51200/72641]\tLoss: 0.6992\tLR: 0.000010\n",
      "Training Epoch: 7 [51520/72641]\tLoss: 0.6459\tLR: 0.000010\n",
      "Training Epoch: 7 [51840/72641]\tLoss: 0.6696\tLR: 0.000010\n",
      "Training Epoch: 7 [52160/72641]\tLoss: 0.6846\tLR: 0.000010\n",
      "Training Epoch: 7 [52480/72641]\tLoss: 0.6489\tLR: 0.000010\n",
      "Training Epoch: 7 [52800/72641]\tLoss: 0.6926\tLR: 0.000010\n",
      "Training Epoch: 7 [53120/72641]\tLoss: 0.6731\tLR: 0.000010\n",
      "Training Epoch: 7 [53440/72641]\tLoss: 0.6605\tLR: 0.000010\n",
      "Training Epoch: 7 [53760/72641]\tLoss: 0.6876\tLR: 0.000010\n",
      "Training Epoch: 7 [54080/72641]\tLoss: 0.7008\tLR: 0.000010\n",
      "Training Epoch: 7 [54400/72641]\tLoss: 0.6634\tLR: 0.000010\n",
      "Training Epoch: 7 [54720/72641]\tLoss: 0.6791\tLR: 0.000010\n",
      "Training Epoch: 7 [55040/72641]\tLoss: 0.6709\tLR: 0.000010\n",
      "Training Epoch: 7 [55360/72641]\tLoss: 0.6825\tLR: 0.000010\n",
      "Training Epoch: 7 [55680/72641]\tLoss: 0.6512\tLR: 0.000010\n",
      "Training Epoch: 7 [56000/72641]\tLoss: 0.6695\tLR: 0.000010\n",
      "Training Epoch: 7 [56320/72641]\tLoss: 0.6699\tLR: 0.000010\n",
      "Training Epoch: 7 [56640/72641]\tLoss: 0.6813\tLR: 0.000010\n",
      "Training Epoch: 7 [56960/72641]\tLoss: 0.6550\tLR: 0.000010\n",
      "Training Epoch: 7 [57280/72641]\tLoss: 0.6699\tLR: 0.000010\n",
      "Training Epoch: 7 [57600/72641]\tLoss: 0.6890\tLR: 0.000010\n",
      "Training Epoch: 7 [57920/72641]\tLoss: 0.6861\tLR: 0.000010\n",
      "Training Epoch: 7 [58240/72641]\tLoss: 0.6254\tLR: 0.000010\n",
      "Training Epoch: 7 [58560/72641]\tLoss: 0.6537\tLR: 0.000010\n",
      "Training Epoch: 7 [58880/72641]\tLoss: 0.6361\tLR: 0.000010\n",
      "Training Epoch: 7 [59200/72641]\tLoss: 0.6823\tLR: 0.000010\n",
      "Training Epoch: 7 [59520/72641]\tLoss: 0.7172\tLR: 0.000010\n",
      "Training Epoch: 7 [59840/72641]\tLoss: 0.6692\tLR: 0.000010\n",
      "Training Epoch: 7 [60160/72641]\tLoss: 0.6811\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [60480/72641]\tLoss: 0.6848\tLR: 0.000010\n",
      "Training Epoch: 7 [60800/72641]\tLoss: 0.6659\tLR: 0.000010\n",
      "Training Epoch: 7 [61120/72641]\tLoss: 0.6819\tLR: 0.000010\n",
      "Training Epoch: 7 [61440/72641]\tLoss: 0.6495\tLR: 0.000010\n",
      "Training Epoch: 7 [61760/72641]\tLoss: 0.7026\tLR: 0.000010\n",
      "Training Epoch: 7 [62080/72641]\tLoss: 0.6962\tLR: 0.000010\n",
      "Training Epoch: 7 [62400/72641]\tLoss: 0.6622\tLR: 0.000010\n",
      "Training Epoch: 7 [62720/72641]\tLoss: 0.6470\tLR: 0.000010\n",
      "Training Epoch: 7 [63040/72641]\tLoss: 0.6511\tLR: 0.000010\n",
      "Training Epoch: 7 [63360/72641]\tLoss: 0.7185\tLR: 0.000010\n",
      "Training Epoch: 7 [63680/72641]\tLoss: 0.6884\tLR: 0.000010\n",
      "Training Epoch: 7 [64000/72641]\tLoss: 0.6763\tLR: 0.000010\n",
      "Training Epoch: 7 [64320/72641]\tLoss: 0.6334\tLR: 0.000010\n",
      "Training Epoch: 7 [64640/72641]\tLoss: 0.6463\tLR: 0.000010\n",
      "Training Epoch: 7 [64960/72641]\tLoss: 0.6470\tLR: 0.000010\n",
      "Training Epoch: 7 [65280/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 7 [65600/72641]\tLoss: 0.6625\tLR: 0.000010\n",
      "Training Epoch: 7 [65920/72641]\tLoss: 0.6632\tLR: 0.000010\n",
      "Training Epoch: 7 [66240/72641]\tLoss: 0.6817\tLR: 0.000010\n",
      "Training Epoch: 7 [66560/72641]\tLoss: 0.6424\tLR: 0.000010\n",
      "Training Epoch: 7 [66880/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 7 [67200/72641]\tLoss: 0.6697\tLR: 0.000010\n",
      "Training Epoch: 7 [67520/72641]\tLoss: 0.6768\tLR: 0.000010\n",
      "Training Epoch: 7 [67840/72641]\tLoss: 0.6874\tLR: 0.000010\n",
      "Training Epoch: 7 [68160/72641]\tLoss: 0.6605\tLR: 0.000010\n",
      "Training Epoch: 7 [68480/72641]\tLoss: 0.6895\tLR: 0.000010\n",
      "Training Epoch: 7 [68800/72641]\tLoss: 0.7042\tLR: 0.000010\n",
      "Training Epoch: 7 [69120/72641]\tLoss: 0.7182\tLR: 0.000010\n",
      "Training Epoch: 7 [69440/72641]\tLoss: 0.6620\tLR: 0.000010\n",
      "Training Epoch: 7 [69760/72641]\tLoss: 0.6801\tLR: 0.000010\n",
      "Training Epoch: 7 [70080/72641]\tLoss: 0.6772\tLR: 0.000010\n",
      "Training Epoch: 7 [70400/72641]\tLoss: 0.7197\tLR: 0.000010\n",
      "Training Epoch: 7 [70720/72641]\tLoss: 0.6658\tLR: 0.000010\n",
      "Training Epoch: 7 [71040/72641]\tLoss: 0.6620\tLR: 0.000010\n",
      "Training Epoch: 7 [71360/72641]\tLoss: 0.7323\tLR: 0.000010\n",
      "Training Epoch: 7 [71680/72641]\tLoss: 0.6685\tLR: 0.000010\n",
      "Training Epoch: 7 [72000/72641]\tLoss: 0.6526\tLR: 0.000010\n",
      "Training Epoch: 7 [72320/72641]\tLoss: 0.6717\tLR: 0.000010\n",
      "Training Epoch: 7 [72640/72641]\tLoss: 0.6214\tLR: 0.000010\n",
      "Val Result: Acc: 0.1462, C_ACC: 0.6700, DOA: 89.4625, ACC_k: 0.0994\n",
      "ext:0.0, cls:0.611854, coar:0.0, fine:0.0,\n",
      "Training Epoch: 8 [320/72641]\tLoss: 0.6452\tLR: 0.000010\n",
      "Training Epoch: 8 [640/72641]\tLoss: 0.6547\tLR: 0.000010\n",
      "Training Epoch: 8 [960/72641]\tLoss: 0.6763\tLR: 0.000010\n",
      "Training Epoch: 8 [1280/72641]\tLoss: 0.6477\tLR: 0.000010\n",
      "Training Epoch: 8 [1600/72641]\tLoss: 0.6300\tLR: 0.000010\n",
      "Training Epoch: 8 [1920/72641]\tLoss: 0.6571\tLR: 0.000010\n",
      "Training Epoch: 8 [2240/72641]\tLoss: 0.6644\tLR: 0.000010\n",
      "Training Epoch: 8 [2560/72641]\tLoss: 0.6397\tLR: 0.000010\n",
      "Training Epoch: 8 [2880/72641]\tLoss: 0.6692\tLR: 0.000010\n",
      "Training Epoch: 8 [3200/72641]\tLoss: 0.7030\tLR: 0.000010\n",
      "Training Epoch: 8 [3520/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 8 [3840/72641]\tLoss: 0.6332\tLR: 0.000010\n",
      "Training Epoch: 8 [4160/72641]\tLoss: 0.6989\tLR: 0.000010\n",
      "Training Epoch: 8 [4480/72641]\tLoss: 0.6825\tLR: 0.000010\n",
      "Training Epoch: 8 [4800/72641]\tLoss: 0.6667\tLR: 0.000010\n",
      "Training Epoch: 8 [5120/72641]\tLoss: 0.6535\tLR: 0.000010\n",
      "Training Epoch: 8 [5440/72641]\tLoss: 0.6892\tLR: 0.000010\n",
      "Training Epoch: 8 [5760/72641]\tLoss: 0.6683\tLR: 0.000010\n",
      "Training Epoch: 8 [6080/72641]\tLoss: 0.6533\tLR: 0.000010\n",
      "Training Epoch: 8 [6400/72641]\tLoss: 0.6742\tLR: 0.000010\n",
      "Training Epoch: 8 [6720/72641]\tLoss: 0.6351\tLR: 0.000010\n",
      "Training Epoch: 8 [7040/72641]\tLoss: 0.6513\tLR: 0.000010\n",
      "Training Epoch: 8 [7360/72641]\tLoss: 0.7212\tLR: 0.000010\n",
      "Training Epoch: 8 [7680/72641]\tLoss: 0.6814\tLR: 0.000010\n",
      "Training Epoch: 8 [8000/72641]\tLoss: 0.6882\tLR: 0.000010\n",
      "Training Epoch: 8 [8320/72641]\tLoss: 0.6679\tLR: 0.000010\n",
      "Training Epoch: 8 [8640/72641]\tLoss: 0.6530\tLR: 0.000010\n",
      "Training Epoch: 8 [8960/72641]\tLoss: 0.6596\tLR: 0.000010\n",
      "Training Epoch: 8 [9280/72641]\tLoss: 0.6782\tLR: 0.000010\n",
      "Training Epoch: 8 [9600/72641]\tLoss: 0.7359\tLR: 0.000010\n",
      "Training Epoch: 8 [9920/72641]\tLoss: 0.7095\tLR: 0.000010\n",
      "Training Epoch: 8 [10240/72641]\tLoss: 0.6626\tLR: 0.000010\n",
      "Training Epoch: 8 [10560/72641]\tLoss: 0.6237\tLR: 0.000010\n",
      "Training Epoch: 8 [10880/72641]\tLoss: 0.7025\tLR: 0.000010\n",
      "Training Epoch: 8 [11200/72641]\tLoss: 0.6604\tLR: 0.000010\n",
      "Training Epoch: 8 [11520/72641]\tLoss: 0.6415\tLR: 0.000010\n",
      "Training Epoch: 8 [11840/72641]\tLoss: 0.6861\tLR: 0.000010\n",
      "Training Epoch: 8 [12160/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 8 [12480/72641]\tLoss: 0.6644\tLR: 0.000010\n",
      "Training Epoch: 8 [12800/72641]\tLoss: 0.6354\tLR: 0.000010\n",
      "Training Epoch: 8 [13120/72641]\tLoss: 0.6741\tLR: 0.000010\n",
      "Training Epoch: 8 [13440/72641]\tLoss: 0.6579\tLR: 0.000010\n",
      "Training Epoch: 8 [13760/72641]\tLoss: 0.6874\tLR: 0.000010\n",
      "Training Epoch: 8 [14080/72641]\tLoss: 0.6813\tLR: 0.000010\n",
      "Training Epoch: 8 [14400/72641]\tLoss: 0.6544\tLR: 0.000010\n",
      "Training Epoch: 8 [14720/72641]\tLoss: 0.6368\tLR: 0.000010\n",
      "Training Epoch: 8 [15040/72641]\tLoss: 0.6717\tLR: 0.000010\n",
      "Training Epoch: 8 [15360/72641]\tLoss: 0.6847\tLR: 0.000010\n",
      "Training Epoch: 8 [15680/72641]\tLoss: 0.6488\tLR: 0.000010\n",
      "Training Epoch: 8 [16000/72641]\tLoss: 0.6647\tLR: 0.000010\n",
      "Training Epoch: 8 [16320/72641]\tLoss: 0.6688\tLR: 0.000010\n",
      "Training Epoch: 8 [16640/72641]\tLoss: 0.6678\tLR: 0.000010\n",
      "Training Epoch: 8 [16960/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 8 [17280/72641]\tLoss: 0.6603\tLR: 0.000010\n",
      "Training Epoch: 8 [17600/72641]\tLoss: 0.6322\tLR: 0.000010\n",
      "Training Epoch: 8 [17920/72641]\tLoss: 0.6961\tLR: 0.000010\n",
      "Training Epoch: 8 [18240/72641]\tLoss: 0.6617\tLR: 0.000010\n",
      "Training Epoch: 8 [18560/72641]\tLoss: 0.6969\tLR: 0.000010\n",
      "Training Epoch: 8 [18880/72641]\tLoss: 0.6838\tLR: 0.000010\n",
      "Training Epoch: 8 [19200/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 8 [19520/72641]\tLoss: 0.7032\tLR: 0.000010\n",
      "Training Epoch: 8 [19840/72641]\tLoss: 0.6473\tLR: 0.000010\n",
      "Training Epoch: 8 [20160/72641]\tLoss: 0.6341\tLR: 0.000010\n",
      "Training Epoch: 8 [20480/72641]\tLoss: 0.6659\tLR: 0.000010\n",
      "Training Epoch: 8 [20800/72641]\tLoss: 0.6818\tLR: 0.000010\n",
      "Training Epoch: 8 [21120/72641]\tLoss: 0.6828\tLR: 0.000010\n",
      "Training Epoch: 8 [21440/72641]\tLoss: 0.6626\tLR: 0.000010\n",
      "Training Epoch: 8 [21760/72641]\tLoss: 0.6748\tLR: 0.000010\n",
      "Training Epoch: 8 [22080/72641]\tLoss: 0.6508\tLR: 0.000010\n",
      "Training Epoch: 8 [22400/72641]\tLoss: 0.7096\tLR: 0.000010\n",
      "Training Epoch: 8 [22720/72641]\tLoss: 0.6507\tLR: 0.000010\n",
      "Training Epoch: 8 [23040/72641]\tLoss: 0.6491\tLR: 0.000010\n",
      "Training Epoch: 8 [23360/72641]\tLoss: 0.6692\tLR: 0.000010\n",
      "Training Epoch: 8 [23680/72641]\tLoss: 0.6559\tLR: 0.000010\n",
      "Training Epoch: 8 [24000/72641]\tLoss: 0.6627\tLR: 0.000010\n",
      "Training Epoch: 8 [24320/72641]\tLoss: 0.6733\tLR: 0.000010\n",
      "Training Epoch: 8 [24640/72641]\tLoss: 0.6528\tLR: 0.000010\n",
      "Training Epoch: 8 [24960/72641]\tLoss: 0.6670\tLR: 0.000010\n",
      "Training Epoch: 8 [25280/72641]\tLoss: 0.6301\tLR: 0.000010\n",
      "Training Epoch: 8 [25600/72641]\tLoss: 0.6658\tLR: 0.000010\n",
      "Training Epoch: 8 [25920/72641]\tLoss: 0.6417\tLR: 0.000010\n",
      "Training Epoch: 8 [26240/72641]\tLoss: 0.6828\tLR: 0.000010\n",
      "Training Epoch: 8 [26560/72641]\tLoss: 0.6693\tLR: 0.000010\n",
      "Training Epoch: 8 [26880/72641]\tLoss: 0.6681\tLR: 0.000010\n",
      "Training Epoch: 8 [27200/72641]\tLoss: 0.6395\tLR: 0.000010\n",
      "Training Epoch: 8 [27520/72641]\tLoss: 0.6450\tLR: 0.000010\n",
      "Training Epoch: 8 [27840/72641]\tLoss: 0.6927\tLR: 0.000010\n",
      "Training Epoch: 8 [28160/72641]\tLoss: 0.6867\tLR: 0.000010\n",
      "Training Epoch: 8 [28480/72641]\tLoss: 0.6769\tLR: 0.000010\n",
      "Training Epoch: 8 [28800/72641]\tLoss: 0.6855\tLR: 0.000010\n",
      "Training Epoch: 8 [29120/72641]\tLoss: 0.6863\tLR: 0.000010\n",
      "Training Epoch: 8 [29440/72641]\tLoss: 0.6891\tLR: 0.000010\n",
      "Training Epoch: 8 [29760/72641]\tLoss: 0.6811\tLR: 0.000010\n",
      "Training Epoch: 8 [30080/72641]\tLoss: 0.6825\tLR: 0.000010\n",
      "Training Epoch: 8 [30400/72641]\tLoss: 0.6715\tLR: 0.000010\n",
      "Training Epoch: 8 [30720/72641]\tLoss: 0.6908\tLR: 0.000010\n",
      "Training Epoch: 8 [31040/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 8 [31360/72641]\tLoss: 0.6631\tLR: 0.000010\n",
      "Training Epoch: 8 [31680/72641]\tLoss: 0.6884\tLR: 0.000010\n",
      "Training Epoch: 8 [32000/72641]\tLoss: 0.6320\tLR: 0.000010\n",
      "Training Epoch: 8 [32320/72641]\tLoss: 0.6772\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [32640/72641]\tLoss: 0.6455\tLR: 0.000010\n",
      "Training Epoch: 8 [32960/72641]\tLoss: 0.7032\tLR: 0.000010\n",
      "Training Epoch: 8 [33280/72641]\tLoss: 0.6830\tLR: 0.000010\n",
      "Training Epoch: 8 [33600/72641]\tLoss: 0.6467\tLR: 0.000010\n",
      "Training Epoch: 8 [33920/72641]\tLoss: 0.6703\tLR: 0.000010\n",
      "Training Epoch: 8 [34240/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 8 [34560/72641]\tLoss: 0.6952\tLR: 0.000010\n",
      "Training Epoch: 8 [34880/72641]\tLoss: 0.6723\tLR: 0.000010\n",
      "Training Epoch: 8 [35200/72641]\tLoss: 0.6399\tLR: 0.000010\n",
      "Training Epoch: 8 [35520/72641]\tLoss: 0.6477\tLR: 0.000010\n",
      "Training Epoch: 8 [35840/72641]\tLoss: 0.7113\tLR: 0.000010\n",
      "Training Epoch: 8 [36160/72641]\tLoss: 0.6656\tLR: 0.000010\n",
      "Training Epoch: 8 [36480/72641]\tLoss: 0.6378\tLR: 0.000010\n",
      "Training Epoch: 8 [36800/72641]\tLoss: 0.6511\tLR: 0.000010\n",
      "Training Epoch: 8 [37120/72641]\tLoss: 0.6887\tLR: 0.000010\n",
      "Training Epoch: 8 [37440/72641]\tLoss: 0.6840\tLR: 0.000010\n",
      "Training Epoch: 8 [37760/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 8 [38080/72641]\tLoss: 0.6617\tLR: 0.000010\n",
      "Training Epoch: 8 [38400/72641]\tLoss: 0.6998\tLR: 0.000010\n",
      "Training Epoch: 8 [38720/72641]\tLoss: 0.6771\tLR: 0.000010\n",
      "Training Epoch: 8 [39040/72641]\tLoss: 0.7072\tLR: 0.000010\n",
      "Training Epoch: 8 [39360/72641]\tLoss: 0.6316\tLR: 0.000010\n",
      "Training Epoch: 8 [39680/72641]\tLoss: 0.6064\tLR: 0.000010\n",
      "Training Epoch: 8 [40000/72641]\tLoss: 0.7014\tLR: 0.000010\n",
      "Training Epoch: 8 [40320/72641]\tLoss: 0.6383\tLR: 0.000010\n",
      "Training Epoch: 8 [40640/72641]\tLoss: 0.6639\tLR: 0.000010\n",
      "Training Epoch: 8 [40960/72641]\tLoss: 0.6718\tLR: 0.000010\n",
      "Training Epoch: 8 [41280/72641]\tLoss: 0.6521\tLR: 0.000010\n",
      "Training Epoch: 8 [41600/72641]\tLoss: 0.6697\tLR: 0.000010\n",
      "Training Epoch: 8 [41920/72641]\tLoss: 0.6676\tLR: 0.000010\n",
      "Training Epoch: 8 [42240/72641]\tLoss: 0.6068\tLR: 0.000010\n",
      "Training Epoch: 8 [42560/72641]\tLoss: 0.6530\tLR: 0.000010\n",
      "Training Epoch: 8 [42880/72641]\tLoss: 0.6944\tLR: 0.000010\n",
      "Training Epoch: 8 [43200/72641]\tLoss: 0.7094\tLR: 0.000010\n",
      "Training Epoch: 8 [43520/72641]\tLoss: 0.6808\tLR: 0.000010\n",
      "Training Epoch: 8 [43840/72641]\tLoss: 0.6867\tLR: 0.000010\n",
      "Training Epoch: 8 [44160/72641]\tLoss: 0.6798\tLR: 0.000010\n",
      "Training Epoch: 8 [44480/72641]\tLoss: 0.6928\tLR: 0.000010\n",
      "Training Epoch: 8 [44800/72641]\tLoss: 0.6430\tLR: 0.000010\n",
      "Training Epoch: 8 [45120/72641]\tLoss: 0.6882\tLR: 0.000010\n",
      "Training Epoch: 8 [45440/72641]\tLoss: 0.6336\tLR: 0.000010\n",
      "Training Epoch: 8 [45760/72641]\tLoss: 0.6669\tLR: 0.000010\n",
      "Training Epoch: 8 [46080/72641]\tLoss: 0.6584\tLR: 0.000010\n",
      "Training Epoch: 8 [46400/72641]\tLoss: 0.6375\tLR: 0.000010\n",
      "Training Epoch: 8 [46720/72641]\tLoss: 0.7044\tLR: 0.000010\n",
      "Training Epoch: 8 [47040/72641]\tLoss: 0.7190\tLR: 0.000010\n",
      "Training Epoch: 8 [47360/72641]\tLoss: 0.6846\tLR: 0.000010\n",
      "Training Epoch: 8 [47680/72641]\tLoss: 0.6836\tLR: 0.000010\n",
      "Training Epoch: 8 [48000/72641]\tLoss: 0.6475\tLR: 0.000010\n",
      "Training Epoch: 8 [48320/72641]\tLoss: 0.6795\tLR: 0.000010\n",
      "Training Epoch: 8 [48640/72641]\tLoss: 0.6474\tLR: 0.000010\n",
      "Training Epoch: 8 [48960/72641]\tLoss: 0.6360\tLR: 0.000010\n",
      "Training Epoch: 8 [49280/72641]\tLoss: 0.6537\tLR: 0.000010\n",
      "Training Epoch: 8 [49600/72641]\tLoss: 0.6822\tLR: 0.000010\n",
      "Training Epoch: 8 [49920/72641]\tLoss: 0.6425\tLR: 0.000010\n",
      "Training Epoch: 8 [50240/72641]\tLoss: 0.6168\tLR: 0.000010\n",
      "Training Epoch: 8 [50560/72641]\tLoss: 0.6355\tLR: 0.000010\n",
      "Training Epoch: 8 [50880/72641]\tLoss: 0.6524\tLR: 0.000010\n",
      "Training Epoch: 8 [51200/72641]\tLoss: 0.6744\tLR: 0.000010\n",
      "Training Epoch: 8 [51520/72641]\tLoss: 0.6478\tLR: 0.000010\n",
      "Training Epoch: 8 [51840/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 8 [52160/72641]\tLoss: 0.6878\tLR: 0.000010\n",
      "Training Epoch: 8 [52480/72641]\tLoss: 0.6274\tLR: 0.000010\n",
      "Training Epoch: 8 [52800/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 8 [53120/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 8 [53440/72641]\tLoss: 0.6610\tLR: 0.000010\n",
      "Training Epoch: 8 [53760/72641]\tLoss: 0.6806\tLR: 0.000010\n",
      "Training Epoch: 8 [54080/72641]\tLoss: 0.6393\tLR: 0.000010\n",
      "Training Epoch: 8 [54400/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 8 [54720/72641]\tLoss: 0.6462\tLR: 0.000010\n",
      "Training Epoch: 8 [55040/72641]\tLoss: 0.6890\tLR: 0.000010\n",
      "Training Epoch: 8 [55360/72641]\tLoss: 0.6455\tLR: 0.000010\n",
      "Training Epoch: 8 [55680/72641]\tLoss: 0.6807\tLR: 0.000010\n",
      "Training Epoch: 8 [56000/72641]\tLoss: 0.6704\tLR: 0.000010\n",
      "Training Epoch: 8 [56320/72641]\tLoss: 0.6696\tLR: 0.000010\n",
      "Training Epoch: 8 [56640/72641]\tLoss: 0.6715\tLR: 0.000010\n",
      "Training Epoch: 8 [56960/72641]\tLoss: 0.6258\tLR: 0.000010\n",
      "Training Epoch: 8 [57280/72641]\tLoss: 0.6468\tLR: 0.000010\n",
      "Training Epoch: 8 [57600/72641]\tLoss: 0.6890\tLR: 0.000010\n",
      "Training Epoch: 8 [57920/72641]\tLoss: 0.6626\tLR: 0.000010\n",
      "Training Epoch: 8 [58240/72641]\tLoss: 0.6185\tLR: 0.000010\n",
      "Training Epoch: 8 [58560/72641]\tLoss: 0.6320\tLR: 0.000010\n",
      "Training Epoch: 8 [58880/72641]\tLoss: 0.6656\tLR: 0.000010\n",
      "Training Epoch: 8 [59200/72641]\tLoss: 0.6681\tLR: 0.000010\n",
      "Training Epoch: 8 [59520/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 8 [59840/72641]\tLoss: 0.7413\tLR: 0.000010\n",
      "Training Epoch: 8 [60160/72641]\tLoss: 0.6468\tLR: 0.000010\n",
      "Training Epoch: 8 [60480/72641]\tLoss: 0.6647\tLR: 0.000010\n",
      "Training Epoch: 8 [60800/72641]\tLoss: 0.6182\tLR: 0.000010\n",
      "Training Epoch: 8 [61120/72641]\tLoss: 0.6603\tLR: 0.000010\n",
      "Training Epoch: 8 [61440/72641]\tLoss: 0.6599\tLR: 0.000010\n",
      "Training Epoch: 8 [61760/72641]\tLoss: 0.6600\tLR: 0.000010\n",
      "Training Epoch: 8 [62080/72641]\tLoss: 0.6914\tLR: 0.000010\n",
      "Training Epoch: 8 [62400/72641]\tLoss: 0.6974\tLR: 0.000010\n",
      "Training Epoch: 8 [62720/72641]\tLoss: 0.6981\tLR: 0.000010\n",
      "Training Epoch: 8 [63040/72641]\tLoss: 0.6658\tLR: 0.000010\n",
      "Training Epoch: 8 [63360/72641]\tLoss: 0.6919\tLR: 0.000010\n",
      "Training Epoch: 8 [63680/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 8 [64000/72641]\tLoss: 0.6974\tLR: 0.000010\n",
      "Training Epoch: 8 [64320/72641]\tLoss: 0.6697\tLR: 0.000010\n",
      "Training Epoch: 8 [64640/72641]\tLoss: 0.6370\tLR: 0.000010\n",
      "Training Epoch: 8 [64960/72641]\tLoss: 0.6575\tLR: 0.000010\n",
      "Training Epoch: 8 [65280/72641]\tLoss: 0.6520\tLR: 0.000010\n",
      "Training Epoch: 8 [65600/72641]\tLoss: 0.6644\tLR: 0.000010\n",
      "Training Epoch: 8 [65920/72641]\tLoss: 0.6823\tLR: 0.000010\n",
      "Training Epoch: 8 [66240/72641]\tLoss: 0.6661\tLR: 0.000010\n",
      "Training Epoch: 8 [66560/72641]\tLoss: 0.6658\tLR: 0.000010\n",
      "Training Epoch: 8 [66880/72641]\tLoss: 0.6938\tLR: 0.000010\n",
      "Training Epoch: 8 [67200/72641]\tLoss: 0.6317\tLR: 0.000010\n",
      "Training Epoch: 8 [67520/72641]\tLoss: 0.6636\tLR: 0.000010\n",
      "Training Epoch: 8 [67840/72641]\tLoss: 0.6631\tLR: 0.000010\n",
      "Training Epoch: 8 [68160/72641]\tLoss: 0.7073\tLR: 0.000010\n",
      "Training Epoch: 8 [68480/72641]\tLoss: 0.6154\tLR: 0.000010\n",
      "Training Epoch: 8 [68800/72641]\tLoss: 0.6948\tLR: 0.000010\n",
      "Training Epoch: 8 [69120/72641]\tLoss: 0.6571\tLR: 0.000010\n",
      "Training Epoch: 8 [69440/72641]\tLoss: 0.6448\tLR: 0.000010\n",
      "Training Epoch: 8 [69760/72641]\tLoss: 0.6767\tLR: 0.000010\n",
      "Training Epoch: 8 [70080/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 8 [70400/72641]\tLoss: 0.6859\tLR: 0.000010\n",
      "Training Epoch: 8 [70720/72641]\tLoss: 0.6625\tLR: 0.000010\n",
      "Training Epoch: 8 [71040/72641]\tLoss: 0.7029\tLR: 0.000010\n",
      "Training Epoch: 8 [71360/72641]\tLoss: 0.6758\tLR: 0.000010\n",
      "Training Epoch: 8 [71680/72641]\tLoss: 0.7432\tLR: 0.000010\n",
      "Training Epoch: 8 [72000/72641]\tLoss: 0.6449\tLR: 0.000010\n",
      "Training Epoch: 8 [72320/72641]\tLoss: 0.6767\tLR: 0.000010\n",
      "Training Epoch: 8 [72640/72641]\tLoss: 0.6604\tLR: 0.000010\n",
      "Val Result: Acc: 0.1460, C_ACC: 0.6953, DOA: 89.2701, ACC_k: 0.1044\n",
      "ext:0.0, cls:0.584983, coar:0.0, fine:0.0,\n",
      "Training Epoch: 9 [320/72641]\tLoss: 0.6628\tLR: 0.000010\n",
      "Training Epoch: 9 [640/72641]\tLoss: 0.6312\tLR: 0.000010\n",
      "Training Epoch: 9 [960/72641]\tLoss: 0.6686\tLR: 0.000010\n",
      "Training Epoch: 9 [1280/72641]\tLoss: 0.6904\tLR: 0.000010\n",
      "Training Epoch: 9 [1600/72641]\tLoss: 0.6487\tLR: 0.000010\n",
      "Training Epoch: 9 [1920/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 9 [2240/72641]\tLoss: 0.6250\tLR: 0.000010\n",
      "Training Epoch: 9 [2560/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 9 [2880/72641]\tLoss: 0.6340\tLR: 0.000010\n",
      "Training Epoch: 9 [3200/72641]\tLoss: 0.7069\tLR: 0.000010\n",
      "Training Epoch: 9 [3520/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 9 [3840/72641]\tLoss: 0.6472\tLR: 0.000010\n",
      "Training Epoch: 9 [4160/72641]\tLoss: 0.6994\tLR: 0.000010\n",
      "Training Epoch: 9 [4480/72641]\tLoss: 0.6642\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [4800/72641]\tLoss: 0.6555\tLR: 0.000010\n",
      "Training Epoch: 9 [5120/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 9 [5440/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 9 [5760/72641]\tLoss: 0.6542\tLR: 0.000010\n",
      "Training Epoch: 9 [6080/72641]\tLoss: 0.6387\tLR: 0.000010\n",
      "Training Epoch: 9 [6400/72641]\tLoss: 0.6471\tLR: 0.000010\n",
      "Training Epoch: 9 [6720/72641]\tLoss: 0.6660\tLR: 0.000010\n",
      "Training Epoch: 9 [7040/72641]\tLoss: 0.6612\tLR: 0.000010\n",
      "Training Epoch: 9 [7360/72641]\tLoss: 0.6744\tLR: 0.000010\n",
      "Training Epoch: 9 [7680/72641]\tLoss: 0.6559\tLR: 0.000010\n",
      "Training Epoch: 9 [8000/72641]\tLoss: 0.6313\tLR: 0.000010\n",
      "Training Epoch: 9 [8320/72641]\tLoss: 0.6566\tLR: 0.000010\n",
      "Training Epoch: 9 [8640/72641]\tLoss: 0.6780\tLR: 0.000010\n",
      "Training Epoch: 9 [8960/72641]\tLoss: 0.6602\tLR: 0.000010\n",
      "Training Epoch: 9 [9280/72641]\tLoss: 0.6375\tLR: 0.000010\n",
      "Training Epoch: 9 [9600/72641]\tLoss: 0.6685\tLR: 0.000010\n",
      "Training Epoch: 9 [9920/72641]\tLoss: 0.6602\tLR: 0.000010\n",
      "Training Epoch: 9 [10240/72641]\tLoss: 0.6687\tLR: 0.000010\n",
      "Training Epoch: 9 [10560/72641]\tLoss: 0.6360\tLR: 0.000010\n",
      "Training Epoch: 9 [10880/72641]\tLoss: 0.6598\tLR: 0.000010\n",
      "Training Epoch: 9 [11200/72641]\tLoss: 0.6718\tLR: 0.000010\n",
      "Training Epoch: 9 [11520/72641]\tLoss: 0.6832\tLR: 0.000010\n",
      "Training Epoch: 9 [11840/72641]\tLoss: 0.6813\tLR: 0.000010\n",
      "Training Epoch: 9 [12160/72641]\tLoss: 0.6439\tLR: 0.000010\n",
      "Training Epoch: 9 [12480/72641]\tLoss: 0.6844\tLR: 0.000010\n",
      "Training Epoch: 9 [12800/72641]\tLoss: 0.6183\tLR: 0.000010\n",
      "Training Epoch: 9 [13120/72641]\tLoss: 0.7094\tLR: 0.000010\n",
      "Training Epoch: 9 [13440/72641]\tLoss: 0.6893\tLR: 0.000010\n",
      "Training Epoch: 9 [13760/72641]\tLoss: 0.7029\tLR: 0.000010\n",
      "Training Epoch: 9 [14080/72641]\tLoss: 0.7083\tLR: 0.000010\n",
      "Training Epoch: 9 [14400/72641]\tLoss: 0.7042\tLR: 0.000010\n",
      "Training Epoch: 9 [14720/72641]\tLoss: 0.6840\tLR: 0.000010\n",
      "Training Epoch: 9 [15040/72641]\tLoss: 0.6645\tLR: 0.000010\n",
      "Training Epoch: 9 [15360/72641]\tLoss: 0.6885\tLR: 0.000010\n",
      "Training Epoch: 9 [15680/72641]\tLoss: 0.6644\tLR: 0.000010\n",
      "Training Epoch: 9 [16000/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 9 [16320/72641]\tLoss: 0.6645\tLR: 0.000010\n",
      "Training Epoch: 9 [16640/72641]\tLoss: 0.6834\tLR: 0.000010\n",
      "Training Epoch: 9 [16960/72641]\tLoss: 0.6910\tLR: 0.000010\n",
      "Training Epoch: 9 [17280/72641]\tLoss: 0.6347\tLR: 0.000010\n",
      "Training Epoch: 9 [17600/72641]\tLoss: 0.6193\tLR: 0.000010\n",
      "Training Epoch: 9 [17920/72641]\tLoss: 0.6573\tLR: 0.000010\n",
      "Training Epoch: 9 [18240/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 9 [18560/72641]\tLoss: 0.6525\tLR: 0.000010\n",
      "Training Epoch: 9 [18880/72641]\tLoss: 0.6579\tLR: 0.000010\n",
      "Training Epoch: 9 [19200/72641]\tLoss: 0.6794\tLR: 0.000010\n",
      "Training Epoch: 9 [19520/72641]\tLoss: 0.6509\tLR: 0.000010\n",
      "Training Epoch: 9 [19840/72641]\tLoss: 0.6482\tLR: 0.000010\n",
      "Training Epoch: 9 [20160/72641]\tLoss: 0.6018\tLR: 0.000010\n",
      "Training Epoch: 9 [20480/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 9 [20800/72641]\tLoss: 0.6735\tLR: 0.000010\n",
      "Training Epoch: 9 [21120/72641]\tLoss: 0.6678\tLR: 0.000010\n",
      "Training Epoch: 9 [21440/72641]\tLoss: 0.6345\tLR: 0.000010\n",
      "Training Epoch: 9 [21760/72641]\tLoss: 0.6907\tLR: 0.000010\n",
      "Training Epoch: 9 [22080/72641]\tLoss: 0.6827\tLR: 0.000010\n",
      "Training Epoch: 9 [22400/72641]\tLoss: 0.6452\tLR: 0.000010\n",
      "Training Epoch: 9 [22720/72641]\tLoss: 0.6484\tLR: 0.000010\n",
      "Training Epoch: 9 [23040/72641]\tLoss: 0.6340\tLR: 0.000010\n",
      "Training Epoch: 9 [23360/72641]\tLoss: 0.6457\tLR: 0.000010\n",
      "Training Epoch: 9 [23680/72641]\tLoss: 0.7037\tLR: 0.000010\n",
      "Training Epoch: 9 [24000/72641]\tLoss: 0.6837\tLR: 0.000010\n",
      "Training Epoch: 9 [24320/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 9 [24640/72641]\tLoss: 0.7085\tLR: 0.000010\n",
      "Training Epoch: 9 [24960/72641]\tLoss: 0.6792\tLR: 0.000010\n",
      "Training Epoch: 9 [25280/72641]\tLoss: 0.6543\tLR: 0.000010\n",
      "Training Epoch: 9 [25600/72641]\tLoss: 0.6510\tLR: 0.000010\n",
      "Training Epoch: 9 [25920/72641]\tLoss: 0.6751\tLR: 0.000010\n",
      "Training Epoch: 9 [26240/72641]\tLoss: 0.6303\tLR: 0.000010\n",
      "Training Epoch: 9 [26560/72641]\tLoss: 0.6276\tLR: 0.000010\n",
      "Training Epoch: 9 [26880/72641]\tLoss: 0.6479\tLR: 0.000010\n",
      "Training Epoch: 9 [27200/72641]\tLoss: 0.6746\tLR: 0.000010\n",
      "Training Epoch: 9 [27520/72641]\tLoss: 0.6702\tLR: 0.000010\n",
      "Training Epoch: 9 [27840/72641]\tLoss: 0.7053\tLR: 0.000010\n",
      "Training Epoch: 9 [28160/72641]\tLoss: 0.6469\tLR: 0.000010\n",
      "Training Epoch: 9 [28480/72641]\tLoss: 0.6609\tLR: 0.000010\n",
      "Training Epoch: 9 [28800/72641]\tLoss: 0.6643\tLR: 0.000010\n",
      "Training Epoch: 9 [29120/72641]\tLoss: 0.6490\tLR: 0.000010\n",
      "Training Epoch: 9 [29440/72641]\tLoss: 0.6394\tLR: 0.000010\n",
      "Training Epoch: 9 [29760/72641]\tLoss: 0.6791\tLR: 0.000010\n",
      "Training Epoch: 9 [30080/72641]\tLoss: 0.7003\tLR: 0.000010\n",
      "Training Epoch: 9 [30400/72641]\tLoss: 0.6486\tLR: 0.000010\n",
      "Training Epoch: 9 [30720/72641]\tLoss: 0.7102\tLR: 0.000010\n",
      "Training Epoch: 9 [31040/72641]\tLoss: 0.6410\tLR: 0.000010\n",
      "Training Epoch: 9 [31360/72641]\tLoss: 0.6428\tLR: 0.000010\n",
      "Training Epoch: 9 [31680/72641]\tLoss: 0.6255\tLR: 0.000010\n",
      "Training Epoch: 9 [32000/72641]\tLoss: 0.6665\tLR: 0.000010\n",
      "Training Epoch: 9 [32320/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 9 [32640/72641]\tLoss: 0.7070\tLR: 0.000010\n",
      "Training Epoch: 9 [32960/72641]\tLoss: 0.6953\tLR: 0.000010\n",
      "Training Epoch: 9 [33280/72641]\tLoss: 0.6742\tLR: 0.000010\n",
      "Training Epoch: 9 [33600/72641]\tLoss: 0.6555\tLR: 0.000010\n",
      "Training Epoch: 9 [33920/72641]\tLoss: 0.6598\tLR: 0.000010\n",
      "Training Epoch: 9 [34240/72641]\tLoss: 0.6467\tLR: 0.000010\n",
      "Training Epoch: 9 [34560/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 9 [34880/72641]\tLoss: 0.6259\tLR: 0.000010\n",
      "Training Epoch: 9 [35200/72641]\tLoss: 0.6592\tLR: 0.000010\n",
      "Training Epoch: 9 [35520/72641]\tLoss: 0.6641\tLR: 0.000010\n",
      "Training Epoch: 9 [35840/72641]\tLoss: 0.6670\tLR: 0.000010\n",
      "Training Epoch: 9 [36160/72641]\tLoss: 0.6312\tLR: 0.000010\n",
      "Training Epoch: 9 [36480/72641]\tLoss: 0.6384\tLR: 0.000010\n",
      "Training Epoch: 9 [36800/72641]\tLoss: 0.6195\tLR: 0.000010\n",
      "Training Epoch: 9 [37120/72641]\tLoss: 0.6450\tLR: 0.000010\n",
      "Training Epoch: 9 [37440/72641]\tLoss: 0.6622\tLR: 0.000010\n",
      "Training Epoch: 9 [37760/72641]\tLoss: 0.6716\tLR: 0.000010\n",
      "Training Epoch: 9 [38080/72641]\tLoss: 0.6159\tLR: 0.000010\n",
      "Training Epoch: 9 [38400/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 9 [38720/72641]\tLoss: 0.7007\tLR: 0.000010\n",
      "Training Epoch: 9 [39040/72641]\tLoss: 0.6728\tLR: 0.000010\n",
      "Training Epoch: 9 [39360/72641]\tLoss: 0.6490\tLR: 0.000010\n",
      "Training Epoch: 9 [39680/72641]\tLoss: 0.6304\tLR: 0.000010\n",
      "Training Epoch: 9 [40000/72641]\tLoss: 0.7059\tLR: 0.000010\n",
      "Training Epoch: 9 [40320/72641]\tLoss: 0.6769\tLR: 0.000010\n",
      "Training Epoch: 9 [40640/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 9 [40960/72641]\tLoss: 0.6794\tLR: 0.000010\n",
      "Training Epoch: 9 [41280/72641]\tLoss: 0.6623\tLR: 0.000010\n",
      "Training Epoch: 9 [41600/72641]\tLoss: 0.6367\tLR: 0.000010\n",
      "Training Epoch: 9 [41920/72641]\tLoss: 0.6296\tLR: 0.000010\n",
      "Training Epoch: 9 [42240/72641]\tLoss: 0.6305\tLR: 0.000010\n",
      "Training Epoch: 9 [42560/72641]\tLoss: 0.6985\tLR: 0.000010\n",
      "Training Epoch: 9 [42880/72641]\tLoss: 0.6775\tLR: 0.000010\n",
      "Training Epoch: 9 [43200/72641]\tLoss: 0.6614\tLR: 0.000010\n",
      "Training Epoch: 9 [43520/72641]\tLoss: 0.6748\tLR: 0.000010\n",
      "Training Epoch: 9 [43840/72641]\tLoss: 0.6752\tLR: 0.000010\n",
      "Training Epoch: 9 [44160/72641]\tLoss: 0.7006\tLR: 0.000010\n",
      "Training Epoch: 9 [44480/72641]\tLoss: 0.6197\tLR: 0.000010\n",
      "Training Epoch: 9 [44800/72641]\tLoss: 0.6467\tLR: 0.000010\n",
      "Training Epoch: 9 [45120/72641]\tLoss: 0.6338\tLR: 0.000010\n",
      "Training Epoch: 9 [45440/72641]\tLoss: 0.6439\tLR: 0.000010\n",
      "Training Epoch: 9 [45760/72641]\tLoss: 0.6876\tLR: 0.000010\n",
      "Training Epoch: 9 [46080/72641]\tLoss: 0.6313\tLR: 0.000010\n",
      "Training Epoch: 9 [46400/72641]\tLoss: 0.6547\tLR: 0.000010\n",
      "Training Epoch: 9 [46720/72641]\tLoss: 0.6783\tLR: 0.000010\n",
      "Training Epoch: 9 [47040/72641]\tLoss: 0.6541\tLR: 0.000010\n",
      "Training Epoch: 9 [47360/72641]\tLoss: 0.6298\tLR: 0.000010\n",
      "Training Epoch: 9 [47680/72641]\tLoss: 0.6253\tLR: 0.000010\n",
      "Training Epoch: 9 [48000/72641]\tLoss: 0.6770\tLR: 0.000010\n",
      "Training Epoch: 9 [48320/72641]\tLoss: 0.6770\tLR: 0.000010\n",
      "Training Epoch: 9 [48640/72641]\tLoss: 0.6791\tLR: 0.000010\n",
      "Training Epoch: 9 [48960/72641]\tLoss: 0.6494\tLR: 0.000010\n",
      "Training Epoch: 9 [49280/72641]\tLoss: 0.6761\tLR: 0.000010\n",
      "Training Epoch: 9 [49600/72641]\tLoss: 0.7125\tLR: 0.000010\n",
      "Training Epoch: 9 [49920/72641]\tLoss: 0.6578\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [50240/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 9 [50560/72641]\tLoss: 0.6340\tLR: 0.000010\n",
      "Training Epoch: 9 [50880/72641]\tLoss: 0.6805\tLR: 0.000010\n",
      "Training Epoch: 9 [51200/72641]\tLoss: 0.7299\tLR: 0.000010\n",
      "Training Epoch: 9 [51520/72641]\tLoss: 0.5939\tLR: 0.000010\n",
      "Training Epoch: 9 [51840/72641]\tLoss: 0.6384\tLR: 0.000010\n",
      "Training Epoch: 9 [52160/72641]\tLoss: 0.6818\tLR: 0.000010\n",
      "Training Epoch: 9 [52480/72641]\tLoss: 0.6823\tLR: 0.000010\n",
      "Training Epoch: 9 [52800/72641]\tLoss: 0.6873\tLR: 0.000010\n",
      "Training Epoch: 9 [53120/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 9 [53440/72641]\tLoss: 0.6277\tLR: 0.000010\n",
      "Training Epoch: 9 [53760/72641]\tLoss: 0.7070\tLR: 0.000010\n",
      "Training Epoch: 9 [54080/72641]\tLoss: 0.6720\tLR: 0.000010\n",
      "Training Epoch: 9 [54400/72641]\tLoss: 0.6729\tLR: 0.000010\n",
      "Training Epoch: 9 [54720/72641]\tLoss: 0.6452\tLR: 0.000010\n",
      "Training Epoch: 9 [55040/72641]\tLoss: 0.6679\tLR: 0.000010\n",
      "Training Epoch: 9 [55360/72641]\tLoss: 0.6669\tLR: 0.000010\n",
      "Training Epoch: 9 [55680/72641]\tLoss: 0.6704\tLR: 0.000010\n",
      "Training Epoch: 9 [56000/72641]\tLoss: 0.6785\tLR: 0.000010\n",
      "Training Epoch: 9 [56320/72641]\tLoss: 0.6569\tLR: 0.000010\n",
      "Training Epoch: 9 [56640/72641]\tLoss: 0.6959\tLR: 0.000010\n",
      "Training Epoch: 9 [56960/72641]\tLoss: 0.6863\tLR: 0.000010\n",
      "Training Epoch: 9 [57280/72641]\tLoss: 0.6574\tLR: 0.000010\n",
      "Training Epoch: 9 [57600/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 9 [57920/72641]\tLoss: 0.6868\tLR: 0.000010\n",
      "Training Epoch: 9 [58240/72641]\tLoss: 0.6379\tLR: 0.000010\n",
      "Training Epoch: 9 [58560/72641]\tLoss: 0.6563\tLR: 0.000010\n",
      "Training Epoch: 9 [58880/72641]\tLoss: 0.6318\tLR: 0.000010\n",
      "Training Epoch: 9 [59200/72641]\tLoss: 0.6727\tLR: 0.000010\n",
      "Training Epoch: 9 [59520/72641]\tLoss: 0.6560\tLR: 0.000010\n",
      "Training Epoch: 9 [59840/72641]\tLoss: 0.6807\tLR: 0.000010\n",
      "Training Epoch: 9 [60160/72641]\tLoss: 0.6484\tLR: 0.000010\n",
      "Training Epoch: 9 [60480/72641]\tLoss: 0.6656\tLR: 0.000010\n",
      "Training Epoch: 9 [60800/72641]\tLoss: 0.6899\tLR: 0.000010\n",
      "Training Epoch: 9 [61120/72641]\tLoss: 0.6706\tLR: 0.000010\n",
      "Training Epoch: 9 [61440/72641]\tLoss: 0.6422\tLR: 0.000010\n",
      "Training Epoch: 9 [61760/72641]\tLoss: 0.6706\tLR: 0.000010\n",
      "Training Epoch: 9 [62080/72641]\tLoss: 0.7072\tLR: 0.000010\n",
      "Training Epoch: 9 [62400/72641]\tLoss: 0.6856\tLR: 0.000010\n",
      "Training Epoch: 9 [62720/72641]\tLoss: 0.6520\tLR: 0.000010\n",
      "Training Epoch: 9 [63040/72641]\tLoss: 0.6312\tLR: 0.000010\n",
      "Training Epoch: 9 [63360/72641]\tLoss: 0.6711\tLR: 0.000010\n",
      "Training Epoch: 9 [63680/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 9 [64000/72641]\tLoss: 0.6715\tLR: 0.000010\n",
      "Training Epoch: 9 [64320/72641]\tLoss: 0.6244\tLR: 0.000010\n",
      "Training Epoch: 9 [64640/72641]\tLoss: 0.6319\tLR: 0.000010\n",
      "Training Epoch: 9 [64960/72641]\tLoss: 0.6537\tLR: 0.000010\n",
      "Training Epoch: 9 [65280/72641]\tLoss: 0.6906\tLR: 0.000010\n",
      "Training Epoch: 9 [65600/72641]\tLoss: 0.6537\tLR: 0.000010\n",
      "Training Epoch: 9 [65920/72641]\tLoss: 0.6885\tLR: 0.000010\n",
      "Training Epoch: 9 [66240/72641]\tLoss: 0.6581\tLR: 0.000010\n",
      "Training Epoch: 9 [66560/72641]\tLoss: 0.6584\tLR: 0.000010\n",
      "Training Epoch: 9 [66880/72641]\tLoss: 0.6559\tLR: 0.000010\n",
      "Training Epoch: 9 [67200/72641]\tLoss: 0.6566\tLR: 0.000010\n",
      "Training Epoch: 9 [67520/72641]\tLoss: 0.6998\tLR: 0.000010\n",
      "Training Epoch: 9 [67840/72641]\tLoss: 0.6389\tLR: 0.000010\n",
      "Training Epoch: 9 [68160/72641]\tLoss: 0.7021\tLR: 0.000010\n",
      "Training Epoch: 9 [68480/72641]\tLoss: 0.6243\tLR: 0.000010\n",
      "Training Epoch: 9 [68800/72641]\tLoss: 0.6464\tLR: 0.000010\n",
      "Training Epoch: 9 [69120/72641]\tLoss: 0.6469\tLR: 0.000010\n",
      "Training Epoch: 9 [69440/72641]\tLoss: 0.6629\tLR: 0.000010\n",
      "Training Epoch: 9 [69760/72641]\tLoss: 0.6582\tLR: 0.000010\n",
      "Training Epoch: 9 [70080/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 9 [70400/72641]\tLoss: 0.6568\tLR: 0.000010\n",
      "Training Epoch: 9 [70720/72641]\tLoss: 0.6926\tLR: 0.000010\n",
      "Training Epoch: 9 [71040/72641]\tLoss: 0.6801\tLR: 0.000010\n",
      "Training Epoch: 9 [71360/72641]\tLoss: 0.6712\tLR: 0.000010\n",
      "Training Epoch: 9 [71680/72641]\tLoss: 0.6419\tLR: 0.000010\n",
      "Training Epoch: 9 [72000/72641]\tLoss: 0.6334\tLR: 0.000010\n",
      "Training Epoch: 9 [72320/72641]\tLoss: 0.6172\tLR: 0.000010\n",
      "Training Epoch: 9 [72640/72641]\tLoss: 0.6123\tLR: 0.000010\n",
      "Val Result: Acc: 0.1450, C_ACC: 0.6761, DOA: 89.2433, ACC_k: 0.0993\n",
      "ext:0.0, cls:0.599942, coar:0.0, fine:0.0,\n",
      "Training Epoch: 10 [320/72641]\tLoss: 0.6563\tLR: 0.000010\n",
      "Training Epoch: 10 [640/72641]\tLoss: 0.6422\tLR: 0.000010\n",
      "Training Epoch: 10 [960/72641]\tLoss: 0.7102\tLR: 0.000010\n",
      "Training Epoch: 10 [1280/72641]\tLoss: 0.6759\tLR: 0.000010\n",
      "Training Epoch: 10 [1600/72641]\tLoss: 0.6807\tLR: 0.000010\n",
      "Training Epoch: 10 [1920/72641]\tLoss: 0.6474\tLR: 0.000010\n",
      "Training Epoch: 10 [2240/72641]\tLoss: 0.6479\tLR: 0.000010\n",
      "Training Epoch: 10 [2560/72641]\tLoss: 0.6999\tLR: 0.000010\n",
      "Training Epoch: 10 [2880/72641]\tLoss: 0.6617\tLR: 0.000010\n",
      "Training Epoch: 10 [3200/72641]\tLoss: 0.6728\tLR: 0.000010\n",
      "Training Epoch: 10 [3520/72641]\tLoss: 0.6306\tLR: 0.000010\n",
      "Training Epoch: 10 [3840/72641]\tLoss: 0.6740\tLR: 0.000010\n",
      "Training Epoch: 10 [4160/72641]\tLoss: 0.6939\tLR: 0.000010\n",
      "Training Epoch: 10 [4480/72641]\tLoss: 0.6863\tLR: 0.000010\n",
      "Training Epoch: 10 [4800/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 10 [5120/72641]\tLoss: 0.6707\tLR: 0.000010\n",
      "Training Epoch: 10 [5440/72641]\tLoss: 0.6701\tLR: 0.000010\n",
      "Training Epoch: 10 [5760/72641]\tLoss: 0.6559\tLR: 0.000010\n",
      "Training Epoch: 10 [6080/72641]\tLoss: 0.6553\tLR: 0.000010\n",
      "Training Epoch: 10 [6400/72641]\tLoss: 0.6565\tLR: 0.000010\n",
      "Training Epoch: 10 [6720/72641]\tLoss: 0.6337\tLR: 0.000010\n",
      "Training Epoch: 10 [7040/72641]\tLoss: 0.6906\tLR: 0.000010\n",
      "Training Epoch: 10 [7360/72641]\tLoss: 0.6280\tLR: 0.000010\n",
      "Training Epoch: 10 [7680/72641]\tLoss: 0.6577\tLR: 0.000010\n",
      "Training Epoch: 10 [8000/72641]\tLoss: 0.6793\tLR: 0.000010\n",
      "Training Epoch: 10 [8320/72641]\tLoss: 0.6450\tLR: 0.000010\n",
      "Training Epoch: 10 [8640/72641]\tLoss: 0.6596\tLR: 0.000010\n",
      "Training Epoch: 10 [8960/72641]\tLoss: 0.6392\tLR: 0.000010\n",
      "Training Epoch: 10 [9280/72641]\tLoss: 0.6396\tLR: 0.000010\n",
      "Training Epoch: 10 [9600/72641]\tLoss: 0.6798\tLR: 0.000010\n",
      "Training Epoch: 10 [9920/72641]\tLoss: 0.6492\tLR: 0.000010\n",
      "Training Epoch: 10 [10240/72641]\tLoss: 0.6382\tLR: 0.000010\n",
      "Training Epoch: 10 [10560/72641]\tLoss: 0.6350\tLR: 0.000010\n",
      "Training Epoch: 10 [10880/72641]\tLoss: 0.6433\tLR: 0.000010\n",
      "Training Epoch: 10 [11200/72641]\tLoss: 0.6978\tLR: 0.000010\n",
      "Training Epoch: 10 [11520/72641]\tLoss: 0.6457\tLR: 0.000010\n",
      "Training Epoch: 10 [11840/72641]\tLoss: 0.6320\tLR: 0.000010\n",
      "Training Epoch: 10 [12160/72641]\tLoss: 0.6678\tLR: 0.000010\n",
      "Training Epoch: 10 [12480/72641]\tLoss: 0.6624\tLR: 0.000010\n",
      "Training Epoch: 10 [12800/72641]\tLoss: 0.7128\tLR: 0.000010\n",
      "Training Epoch: 10 [13120/72641]\tLoss: 0.6293\tLR: 0.000010\n",
      "Training Epoch: 10 [13440/72641]\tLoss: 0.6506\tLR: 0.000010\n",
      "Training Epoch: 10 [13760/72641]\tLoss: 0.6553\tLR: 0.000010\n",
      "Training Epoch: 10 [14080/72641]\tLoss: 0.6785\tLR: 0.000010\n",
      "Training Epoch: 10 [14400/72641]\tLoss: 0.6348\tLR: 0.000010\n",
      "Training Epoch: 10 [14720/72641]\tLoss: 0.6515\tLR: 0.000010\n",
      "Training Epoch: 10 [15040/72641]\tLoss: 0.6348\tLR: 0.000010\n",
      "Training Epoch: 10 [15360/72641]\tLoss: 0.6777\tLR: 0.000010\n",
      "Training Epoch: 10 [15680/72641]\tLoss: 0.6507\tLR: 0.000010\n",
      "Training Epoch: 10 [16000/72641]\tLoss: 0.6868\tLR: 0.000010\n",
      "Training Epoch: 10 [16320/72641]\tLoss: 0.7159\tLR: 0.000010\n",
      "Training Epoch: 10 [16640/72641]\tLoss: 0.6620\tLR: 0.000010\n",
      "Training Epoch: 10 [16960/72641]\tLoss: 0.6374\tLR: 0.000010\n",
      "Training Epoch: 10 [17280/72641]\tLoss: 0.6657\tLR: 0.000010\n",
      "Training Epoch: 10 [17600/72641]\tLoss: 0.6289\tLR: 0.000010\n",
      "Training Epoch: 10 [17920/72641]\tLoss: 0.6499\tLR: 0.000010\n",
      "Training Epoch: 10 [18240/72641]\tLoss: 0.6149\tLR: 0.000010\n",
      "Training Epoch: 10 [18560/72641]\tLoss: 0.6609\tLR: 0.000010\n",
      "Training Epoch: 10 [18880/72641]\tLoss: 0.6585\tLR: 0.000010\n",
      "Training Epoch: 10 [19200/72641]\tLoss: 0.6425\tLR: 0.000010\n",
      "Training Epoch: 10 [19520/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 10 [19840/72641]\tLoss: 0.6616\tLR: 0.000010\n",
      "Training Epoch: 10 [20160/72641]\tLoss: 0.6190\tLR: 0.000010\n",
      "Training Epoch: 10 [20480/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 10 [20800/72641]\tLoss: 0.6725\tLR: 0.000010\n",
      "Training Epoch: 10 [21120/72641]\tLoss: 0.6820\tLR: 0.000010\n",
      "Training Epoch: 10 [21440/72641]\tLoss: 0.6372\tLR: 0.000010\n",
      "Training Epoch: 10 [21760/72641]\tLoss: 0.6573\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [22080/72641]\tLoss: 0.7005\tLR: 0.000010\n",
      "Training Epoch: 10 [22400/72641]\tLoss: 0.6759\tLR: 0.000010\n",
      "Training Epoch: 10 [22720/72641]\tLoss: 0.6539\tLR: 0.000010\n",
      "Training Epoch: 10 [23040/72641]\tLoss: 0.6697\tLR: 0.000010\n",
      "Training Epoch: 10 [23360/72641]\tLoss: 0.6936\tLR: 0.000010\n",
      "Training Epoch: 10 [23680/72641]\tLoss: 0.6640\tLR: 0.000010\n",
      "Training Epoch: 10 [24000/72641]\tLoss: 0.6411\tLR: 0.000010\n",
      "Training Epoch: 10 [24320/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 10 [24640/72641]\tLoss: 0.6618\tLR: 0.000010\n",
      "Training Epoch: 10 [24960/72641]\tLoss: 0.6871\tLR: 0.000010\n",
      "Training Epoch: 10 [25280/72641]\tLoss: 0.6655\tLR: 0.000010\n",
      "Training Epoch: 10 [25600/72641]\tLoss: 0.6235\tLR: 0.000010\n",
      "Training Epoch: 10 [25920/72641]\tLoss: 0.6702\tLR: 0.000010\n",
      "Training Epoch: 10 [26240/72641]\tLoss: 0.6801\tLR: 0.000010\n",
      "Training Epoch: 10 [26560/72641]\tLoss: 0.6554\tLR: 0.000010\n",
      "Training Epoch: 10 [26880/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 10 [27200/72641]\tLoss: 0.6807\tLR: 0.000010\n",
      "Training Epoch: 10 [27520/72641]\tLoss: 0.6642\tLR: 0.000010\n",
      "Training Epoch: 10 [27840/72641]\tLoss: 0.6717\tLR: 0.000010\n",
      "Training Epoch: 10 [28160/72641]\tLoss: 0.6455\tLR: 0.000010\n",
      "Training Epoch: 10 [28480/72641]\tLoss: 0.6788\tLR: 0.000010\n",
      "Training Epoch: 10 [28800/72641]\tLoss: 0.6467\tLR: 0.000010\n",
      "Training Epoch: 10 [29120/72641]\tLoss: 0.6555\tLR: 0.000010\n",
      "Training Epoch: 10 [29440/72641]\tLoss: 0.6656\tLR: 0.000010\n",
      "Training Epoch: 10 [29760/72641]\tLoss: 0.6310\tLR: 0.000010\n",
      "Training Epoch: 10 [30080/72641]\tLoss: 0.6780\tLR: 0.000010\n",
      "Training Epoch: 10 [30400/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 10 [30720/72641]\tLoss: 0.6623\tLR: 0.000010\n",
      "Training Epoch: 10 [31040/72641]\tLoss: 0.6246\tLR: 0.000010\n",
      "Training Epoch: 10 [31360/72641]\tLoss: 0.6303\tLR: 0.000010\n",
      "Training Epoch: 10 [31680/72641]\tLoss: 0.6425\tLR: 0.000010\n",
      "Training Epoch: 10 [32000/72641]\tLoss: 0.6382\tLR: 0.000010\n",
      "Training Epoch: 10 [32320/72641]\tLoss: 0.6883\tLR: 0.000010\n",
      "Training Epoch: 10 [32640/72641]\tLoss: 0.6204\tLR: 0.000010\n",
      "Training Epoch: 10 [32960/72641]\tLoss: 0.7059\tLR: 0.000010\n",
      "Training Epoch: 10 [33280/72641]\tLoss: 0.6806\tLR: 0.000010\n",
      "Training Epoch: 10 [33600/72641]\tLoss: 0.6278\tLR: 0.000010\n",
      "Training Epoch: 10 [33920/72641]\tLoss: 0.6287\tLR: 0.000010\n",
      "Training Epoch: 10 [34240/72641]\tLoss: 0.6301\tLR: 0.000010\n",
      "Training Epoch: 10 [34560/72641]\tLoss: 0.6742\tLR: 0.000010\n",
      "Training Epoch: 10 [34880/72641]\tLoss: 0.6434\tLR: 0.000010\n",
      "Training Epoch: 10 [35200/72641]\tLoss: 0.6316\tLR: 0.000010\n",
      "Training Epoch: 10 [35520/72641]\tLoss: 0.6418\tLR: 0.000010\n",
      "Training Epoch: 10 [35840/72641]\tLoss: 0.6659\tLR: 0.000010\n",
      "Training Epoch: 10 [36160/72641]\tLoss: 0.6691\tLR: 0.000010\n",
      "Training Epoch: 10 [36480/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 10 [36800/72641]\tLoss: 0.6117\tLR: 0.000010\n",
      "Training Epoch: 10 [37120/72641]\tLoss: 0.6544\tLR: 0.000010\n",
      "Training Epoch: 10 [37440/72641]\tLoss: 0.6807\tLR: 0.000010\n",
      "Training Epoch: 10 [37760/72641]\tLoss: 0.6672\tLR: 0.000010\n",
      "Training Epoch: 10 [38080/72641]\tLoss: 0.6719\tLR: 0.000010\n",
      "Training Epoch: 10 [38400/72641]\tLoss: 0.6731\tLR: 0.000010\n",
      "Training Epoch: 10 [38720/72641]\tLoss: 0.6575\tLR: 0.000010\n",
      "Training Epoch: 10 [39040/72641]\tLoss: 0.6761\tLR: 0.000010\n",
      "Training Epoch: 10 [39360/72641]\tLoss: 0.6465\tLR: 0.000010\n",
      "Training Epoch: 10 [39680/72641]\tLoss: 0.6470\tLR: 0.000010\n",
      "Training Epoch: 10 [40000/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 10 [40320/72641]\tLoss: 0.6638\tLR: 0.000010\n",
      "Training Epoch: 10 [40640/72641]\tLoss: 0.6436\tLR: 0.000010\n",
      "Training Epoch: 10 [40960/72641]\tLoss: 0.6394\tLR: 0.000010\n",
      "Training Epoch: 10 [41280/72641]\tLoss: 0.6941\tLR: 0.000010\n",
      "Training Epoch: 10 [41600/72641]\tLoss: 0.7066\tLR: 0.000010\n",
      "Training Epoch: 10 [41920/72641]\tLoss: 0.6233\tLR: 0.000010\n",
      "Training Epoch: 10 [42240/72641]\tLoss: 0.6591\tLR: 0.000010\n",
      "Training Epoch: 10 [42560/72641]\tLoss: 0.6543\tLR: 0.000010\n",
      "Training Epoch: 10 [42880/72641]\tLoss: 0.6435\tLR: 0.000010\n",
      "Training Epoch: 10 [43200/72641]\tLoss: 0.6577\tLR: 0.000010\n",
      "Training Epoch: 10 [43520/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 10 [43840/72641]\tLoss: 0.7217\tLR: 0.000010\n",
      "Training Epoch: 10 [44160/72641]\tLoss: 0.7293\tLR: 0.000010\n",
      "Training Epoch: 10 [44480/72641]\tLoss: 0.6364\tLR: 0.000010\n",
      "Training Epoch: 10 [44800/72641]\tLoss: 0.6477\tLR: 0.000010\n",
      "Training Epoch: 10 [45120/72641]\tLoss: 0.6240\tLR: 0.000010\n",
      "Training Epoch: 10 [45440/72641]\tLoss: 0.6947\tLR: 0.000010\n",
      "Training Epoch: 10 [45760/72641]\tLoss: 0.6661\tLR: 0.000010\n",
      "Training Epoch: 10 [46080/72641]\tLoss: 0.6642\tLR: 0.000010\n",
      "Training Epoch: 10 [46400/72641]\tLoss: 0.6410\tLR: 0.000010\n",
      "Training Epoch: 10 [46720/72641]\tLoss: 0.6688\tLR: 0.000010\n",
      "Training Epoch: 10 [47040/72641]\tLoss: 0.6570\tLR: 0.000010\n",
      "Training Epoch: 10 [47360/72641]\tLoss: 0.6332\tLR: 0.000010\n",
      "Training Epoch: 10 [47680/72641]\tLoss: 0.6630\tLR: 0.000010\n",
      "Training Epoch: 10 [48000/72641]\tLoss: 0.6599\tLR: 0.000010\n",
      "Training Epoch: 10 [48320/72641]\tLoss: 0.6511\tLR: 0.000010\n",
      "Training Epoch: 10 [48640/72641]\tLoss: 0.6323\tLR: 0.000010\n",
      "Training Epoch: 10 [48960/72641]\tLoss: 0.6454\tLR: 0.000010\n",
      "Training Epoch: 10 [49280/72641]\tLoss: 0.6775\tLR: 0.000010\n",
      "Training Epoch: 10 [49600/72641]\tLoss: 0.6953\tLR: 0.000010\n",
      "Training Epoch: 10 [49920/72641]\tLoss: 0.6494\tLR: 0.000010\n",
      "Training Epoch: 10 [50240/72641]\tLoss: 0.6664\tLR: 0.000010\n",
      "Training Epoch: 10 [50560/72641]\tLoss: 0.6546\tLR: 0.000010\n",
      "Training Epoch: 10 [50880/72641]\tLoss: 0.6549\tLR: 0.000010\n",
      "Training Epoch: 10 [51200/72641]\tLoss: 0.6964\tLR: 0.000010\n",
      "Training Epoch: 10 [51520/72641]\tLoss: 0.6679\tLR: 0.000010\n",
      "Training Epoch: 10 [51840/72641]\tLoss: 0.7037\tLR: 0.000010\n",
      "Training Epoch: 10 [52160/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 10 [52480/72641]\tLoss: 0.6598\tLR: 0.000010\n",
      "Training Epoch: 10 [52800/72641]\tLoss: 0.6651\tLR: 0.000010\n",
      "Training Epoch: 10 [53120/72641]\tLoss: 0.6380\tLR: 0.000010\n",
      "Training Epoch: 10 [53440/72641]\tLoss: 0.6558\tLR: 0.000010\n",
      "Training Epoch: 10 [53760/72641]\tLoss: 0.6923\tLR: 0.000010\n",
      "Training Epoch: 10 [54080/72641]\tLoss: 0.6491\tLR: 0.000010\n",
      "Training Epoch: 10 [54400/72641]\tLoss: 0.6523\tLR: 0.000010\n",
      "Training Epoch: 10 [54720/72641]\tLoss: 0.6747\tLR: 0.000010\n",
      "Training Epoch: 10 [55040/72641]\tLoss: 0.7070\tLR: 0.000010\n",
      "Training Epoch: 10 [55360/72641]\tLoss: 0.6882\tLR: 0.000010\n",
      "Training Epoch: 10 [55680/72641]\tLoss: 0.6628\tLR: 0.000010\n",
      "Training Epoch: 10 [56000/72641]\tLoss: 0.6628\tLR: 0.000010\n",
      "Training Epoch: 10 [56320/72641]\tLoss: 0.6515\tLR: 0.000010\n",
      "Training Epoch: 10 [56640/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 10 [56960/72641]\tLoss: 0.6683\tLR: 0.000010\n",
      "Training Epoch: 10 [57280/72641]\tLoss: 0.6332\tLR: 0.000010\n",
      "Training Epoch: 10 [57600/72641]\tLoss: 0.6898\tLR: 0.000010\n",
      "Training Epoch: 10 [57920/72641]\tLoss: 0.6896\tLR: 0.000010\n",
      "Training Epoch: 10 [58240/72641]\tLoss: 0.5994\tLR: 0.000010\n",
      "Training Epoch: 10 [58560/72641]\tLoss: 0.6656\tLR: 0.000010\n",
      "Training Epoch: 10 [58880/72641]\tLoss: 0.6698\tLR: 0.000010\n",
      "Training Epoch: 10 [59200/72641]\tLoss: 0.6528\tLR: 0.000010\n",
      "Training Epoch: 10 [59520/72641]\tLoss: 0.6412\tLR: 0.000010\n",
      "Training Epoch: 10 [59840/72641]\tLoss: 0.6654\tLR: 0.000010\n",
      "Training Epoch: 10 [60160/72641]\tLoss: 0.6559\tLR: 0.000010\n",
      "Training Epoch: 10 [60480/72641]\tLoss: 0.6333\tLR: 0.000010\n",
      "Training Epoch: 10 [60800/72641]\tLoss: 0.6851\tLR: 0.000010\n",
      "Training Epoch: 10 [61120/72641]\tLoss: 0.5818\tLR: 0.000010\n",
      "Training Epoch: 10 [61440/72641]\tLoss: 0.6062\tLR: 0.000010\n",
      "Training Epoch: 10 [61760/72641]\tLoss: 0.6276\tLR: 0.000010\n",
      "Training Epoch: 10 [62080/72641]\tLoss: 0.7016\tLR: 0.000010\n",
      "Training Epoch: 10 [62400/72641]\tLoss: 0.6476\tLR: 0.000010\n",
      "Training Epoch: 10 [62720/72641]\tLoss: 0.6517\tLR: 0.000010\n",
      "Training Epoch: 10 [63040/72641]\tLoss: 0.6543\tLR: 0.000010\n",
      "Training Epoch: 10 [63360/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 10 [63680/72641]\tLoss: 0.6087\tLR: 0.000010\n",
      "Training Epoch: 10 [64000/72641]\tLoss: 0.6198\tLR: 0.000010\n",
      "Training Epoch: 10 [64320/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 10 [64640/72641]\tLoss: 0.6857\tLR: 0.000010\n",
      "Training Epoch: 10 [64960/72641]\tLoss: 0.6585\tLR: 0.000010\n",
      "Training Epoch: 10 [65280/72641]\tLoss: 0.6555\tLR: 0.000010\n",
      "Training Epoch: 10 [65600/72641]\tLoss: 0.6138\tLR: 0.000010\n",
      "Training Epoch: 10 [65920/72641]\tLoss: 0.7026\tLR: 0.000010\n",
      "Training Epoch: 10 [66240/72641]\tLoss: 0.6842\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [66560/72641]\tLoss: 0.6514\tLR: 0.000010\n",
      "Training Epoch: 10 [66880/72641]\tLoss: 0.6504\tLR: 0.000010\n",
      "Training Epoch: 10 [67200/72641]\tLoss: 0.6354\tLR: 0.000010\n",
      "Training Epoch: 10 [67520/72641]\tLoss: 0.6899\tLR: 0.000010\n",
      "Training Epoch: 10 [67840/72641]\tLoss: 0.6386\tLR: 0.000010\n",
      "Training Epoch: 10 [68160/72641]\tLoss: 0.6498\tLR: 0.000010\n",
      "Training Epoch: 10 [68480/72641]\tLoss: 0.6039\tLR: 0.000010\n",
      "Training Epoch: 10 [68800/72641]\tLoss: 0.7215\tLR: 0.000010\n",
      "Training Epoch: 10 [69120/72641]\tLoss: 0.7053\tLR: 0.000010\n",
      "Training Epoch: 10 [69440/72641]\tLoss: 0.6330\tLR: 0.000010\n",
      "Training Epoch: 10 [69760/72641]\tLoss: 0.6300\tLR: 0.000010\n",
      "Training Epoch: 10 [70080/72641]\tLoss: 0.6268\tLR: 0.000010\n",
      "Training Epoch: 10 [70400/72641]\tLoss: 0.6598\tLR: 0.000010\n",
      "Training Epoch: 10 [70720/72641]\tLoss: 0.6933\tLR: 0.000010\n",
      "Training Epoch: 10 [71040/72641]\tLoss: 0.6167\tLR: 0.000010\n",
      "Training Epoch: 10 [71360/72641]\tLoss: 0.6404\tLR: 0.000010\n",
      "Training Epoch: 10 [71680/72641]\tLoss: 0.6892\tLR: 0.000010\n",
      "Training Epoch: 10 [72000/72641]\tLoss: 0.6400\tLR: 0.000010\n",
      "Training Epoch: 10 [72320/72641]\tLoss: 0.6244\tLR: 0.000010\n",
      "Training Epoch: 10 [72640/72641]\tLoss: 0.6624\tLR: 0.000010\n",
      "Val Result: Acc: 0.1478, C_ACC: 0.6995, DOA: 88.3284, ACC_k: 0.1015\n",
      "ext:0.0, cls:0.583044, coar:0.0, fine:0.0,\n",
      "Training Epoch: 11 [320/72641]\tLoss: 0.6391\tLR: 0.000010\n",
      "Training Epoch: 11 [640/72641]\tLoss: 0.6593\tLR: 0.000010\n",
      "Training Epoch: 11 [960/72641]\tLoss: 0.6425\tLR: 0.000010\n",
      "Training Epoch: 11 [1280/72641]\tLoss: 0.6592\tLR: 0.000010\n",
      "Training Epoch: 11 [1600/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 11 [1920/72641]\tLoss: 0.6667\tLR: 0.000010\n",
      "Training Epoch: 11 [2240/72641]\tLoss: 0.6154\tLR: 0.000010\n",
      "Training Epoch: 11 [2560/72641]\tLoss: 0.6476\tLR: 0.000010\n",
      "Training Epoch: 11 [2880/72641]\tLoss: 0.6548\tLR: 0.000010\n",
      "Training Epoch: 11 [3200/72641]\tLoss: 0.6694\tLR: 0.000010\n",
      "Training Epoch: 11 [3520/72641]\tLoss: 0.6458\tLR: 0.000010\n",
      "Training Epoch: 11 [3840/72641]\tLoss: 0.6330\tLR: 0.000010\n",
      "Training Epoch: 11 [4160/72641]\tLoss: 0.6620\tLR: 0.000010\n",
      "Training Epoch: 11 [4480/72641]\tLoss: 0.6413\tLR: 0.000010\n",
      "Training Epoch: 11 [4800/72641]\tLoss: 0.6291\tLR: 0.000010\n",
      "Training Epoch: 11 [5120/72641]\tLoss: 0.6455\tLR: 0.000010\n",
      "Training Epoch: 11 [5440/72641]\tLoss: 0.6803\tLR: 0.000010\n",
      "Training Epoch: 11 [5760/72641]\tLoss: 0.6820\tLR: 0.000010\n",
      "Training Epoch: 11 [6080/72641]\tLoss: 0.6158\tLR: 0.000010\n",
      "Training Epoch: 11 [6400/72641]\tLoss: 0.6290\tLR: 0.000010\n",
      "Training Epoch: 11 [6720/72641]\tLoss: 0.6045\tLR: 0.000010\n",
      "Training Epoch: 11 [7040/72641]\tLoss: 0.6496\tLR: 0.000010\n",
      "Training Epoch: 11 [7360/72641]\tLoss: 0.6204\tLR: 0.000010\n",
      "Training Epoch: 11 [7680/72641]\tLoss: 0.6612\tLR: 0.000010\n",
      "Training Epoch: 11 [8000/72641]\tLoss: 0.6263\tLR: 0.000010\n",
      "Training Epoch: 11 [8320/72641]\tLoss: 0.6562\tLR: 0.000010\n",
      "Training Epoch: 11 [8640/72641]\tLoss: 0.6536\tLR: 0.000010\n",
      "Training Epoch: 11 [8960/72641]\tLoss: 0.6007\tLR: 0.000010\n",
      "Training Epoch: 11 [9280/72641]\tLoss: 0.6508\tLR: 0.000010\n",
      "Training Epoch: 11 [9600/72641]\tLoss: 0.6775\tLR: 0.000010\n",
      "Training Epoch: 11 [9920/72641]\tLoss: 0.6665\tLR: 0.000010\n",
      "Training Epoch: 11 [10240/72641]\tLoss: 0.6607\tLR: 0.000010\n",
      "Training Epoch: 11 [10560/72641]\tLoss: 0.6117\tLR: 0.000010\n",
      "Training Epoch: 11 [10880/72641]\tLoss: 0.7058\tLR: 0.000010\n",
      "Training Epoch: 11 [11200/72641]\tLoss: 0.6387\tLR: 0.000010\n",
      "Training Epoch: 11 [11520/72641]\tLoss: 0.6199\tLR: 0.000010\n",
      "Training Epoch: 11 [11840/72641]\tLoss: 0.6353\tLR: 0.000010\n",
      "Training Epoch: 11 [12160/72641]\tLoss: 0.6533\tLR: 0.000010\n",
      "Training Epoch: 11 [12480/72641]\tLoss: 0.6980\tLR: 0.000010\n",
      "Training Epoch: 11 [12800/72641]\tLoss: 0.6265\tLR: 0.000010\n",
      "Training Epoch: 11 [13120/72641]\tLoss: 0.6394\tLR: 0.000010\n",
      "Training Epoch: 11 [13440/72641]\tLoss: 0.6189\tLR: 0.000010\n",
      "Training Epoch: 11 [13760/72641]\tLoss: 0.6829\tLR: 0.000010\n",
      "Training Epoch: 11 [14080/72641]\tLoss: 0.6607\tLR: 0.000010\n",
      "Training Epoch: 11 [14400/72641]\tLoss: 0.6761\tLR: 0.000010\n",
      "Training Epoch: 11 [14720/72641]\tLoss: 0.6412\tLR: 0.000010\n",
      "Training Epoch: 11 [15040/72641]\tLoss: 0.6424\tLR: 0.000010\n",
      "Training Epoch: 11 [15360/72641]\tLoss: 0.6966\tLR: 0.000010\n",
      "Training Epoch: 11 [15680/72641]\tLoss: 0.6587\tLR: 0.000010\n",
      "Training Epoch: 11 [16000/72641]\tLoss: 0.6515\tLR: 0.000010\n",
      "Training Epoch: 11 [16320/72641]\tLoss: 0.6885\tLR: 0.000010\n",
      "Training Epoch: 11 [16640/72641]\tLoss: 0.6999\tLR: 0.000010\n",
      "Training Epoch: 11 [16960/72641]\tLoss: 0.6144\tLR: 0.000010\n",
      "Training Epoch: 11 [17280/72641]\tLoss: 0.6355\tLR: 0.000010\n",
      "Training Epoch: 11 [17600/72641]\tLoss: 0.6577\tLR: 0.000010\n",
      "Training Epoch: 11 [17920/72641]\tLoss: 0.6435\tLR: 0.000010\n",
      "Training Epoch: 11 [18240/72641]\tLoss: 0.6611\tLR: 0.000010\n",
      "Training Epoch: 11 [18560/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 11 [18880/72641]\tLoss: 0.6249\tLR: 0.000010\n",
      "Training Epoch: 11 [19200/72641]\tLoss: 0.7090\tLR: 0.000010\n",
      "Training Epoch: 11 [19520/72641]\tLoss: 0.6831\tLR: 0.000010\n",
      "Training Epoch: 11 [19840/72641]\tLoss: 0.6631\tLR: 0.000010\n",
      "Training Epoch: 11 [20160/72641]\tLoss: 0.6162\tLR: 0.000010\n",
      "Training Epoch: 11 [20480/72641]\tLoss: 0.6346\tLR: 0.000010\n",
      "Training Epoch: 11 [20800/72641]\tLoss: 0.6321\tLR: 0.000010\n",
      "Training Epoch: 11 [21120/72641]\tLoss: 0.6706\tLR: 0.000010\n",
      "Training Epoch: 11 [21440/72641]\tLoss: 0.6763\tLR: 0.000010\n",
      "Training Epoch: 11 [21760/72641]\tLoss: 0.6552\tLR: 0.000010\n",
      "Training Epoch: 11 [22080/72641]\tLoss: 0.6810\tLR: 0.000010\n",
      "Training Epoch: 11 [22400/72641]\tLoss: 0.6508\tLR: 0.000010\n",
      "Training Epoch: 11 [22720/72641]\tLoss: 0.5845\tLR: 0.000010\n",
      "Training Epoch: 11 [23040/72641]\tLoss: 0.6376\tLR: 0.000010\n",
      "Training Epoch: 11 [23360/72641]\tLoss: 0.6165\tLR: 0.000010\n",
      "Training Epoch: 11 [23680/72641]\tLoss: 0.6322\tLR: 0.000010\n",
      "Training Epoch: 11 [24000/72641]\tLoss: 0.6326\tLR: 0.000010\n",
      "Training Epoch: 11 [24320/72641]\tLoss: 0.6496\tLR: 0.000010\n",
      "Training Epoch: 11 [24640/72641]\tLoss: 0.6832\tLR: 0.000010\n",
      "Training Epoch: 11 [24960/72641]\tLoss: 0.6202\tLR: 0.000010\n",
      "Training Epoch: 11 [25280/72641]\tLoss: 0.6818\tLR: 0.000010\n",
      "Training Epoch: 11 [25600/72641]\tLoss: 0.6463\tLR: 0.000010\n",
      "Training Epoch: 11 [25920/72641]\tLoss: 0.6390\tLR: 0.000010\n",
      "Training Epoch: 11 [26240/72641]\tLoss: 0.6485\tLR: 0.000010\n",
      "Training Epoch: 11 [26560/72641]\tLoss: 0.6720\tLR: 0.000010\n",
      "Training Epoch: 11 [26880/72641]\tLoss: 0.6773\tLR: 0.000010\n",
      "Training Epoch: 11 [27200/72641]\tLoss: 0.6426\tLR: 0.000010\n",
      "Training Epoch: 11 [27520/72641]\tLoss: 0.6758\tLR: 0.000010\n",
      "Training Epoch: 11 [27840/72641]\tLoss: 0.6710\tLR: 0.000010\n",
      "Training Epoch: 11 [28160/72641]\tLoss: 0.6524\tLR: 0.000010\n",
      "Training Epoch: 11 [28480/72641]\tLoss: 0.6192\tLR: 0.000010\n",
      "Training Epoch: 11 [28800/72641]\tLoss: 0.6035\tLR: 0.000010\n",
      "Training Epoch: 11 [29120/72641]\tLoss: 0.6478\tLR: 0.000010\n",
      "Training Epoch: 11 [29440/72641]\tLoss: 0.6402\tLR: 0.000010\n",
      "Training Epoch: 11 [29760/72641]\tLoss: 0.6188\tLR: 0.000010\n",
      "Training Epoch: 11 [30080/72641]\tLoss: 0.6741\tLR: 0.000010\n",
      "Training Epoch: 11 [30400/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 11 [30720/72641]\tLoss: 0.6459\tLR: 0.000010\n",
      "Training Epoch: 11 [31040/72641]\tLoss: 0.6407\tLR: 0.000010\n",
      "Training Epoch: 11 [31360/72641]\tLoss: 0.6352\tLR: 0.000010\n",
      "Training Epoch: 11 [31680/72641]\tLoss: 0.6769\tLR: 0.000010\n",
      "Training Epoch: 11 [32000/72641]\tLoss: 0.6657\tLR: 0.000010\n",
      "Training Epoch: 11 [32320/72641]\tLoss: 0.6762\tLR: 0.000010\n",
      "Training Epoch: 11 [32640/72641]\tLoss: 0.6284\tLR: 0.000010\n",
      "Training Epoch: 11 [32960/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 11 [33280/72641]\tLoss: 0.6700\tLR: 0.000010\n",
      "Training Epoch: 11 [33600/72641]\tLoss: 0.6286\tLR: 0.000010\n",
      "Training Epoch: 11 [33920/72641]\tLoss: 0.6530\tLR: 0.000010\n",
      "Training Epoch: 11 [34240/72641]\tLoss: 0.6582\tLR: 0.000010\n",
      "Training Epoch: 11 [34560/72641]\tLoss: 0.6684\tLR: 0.000010\n",
      "Training Epoch: 11 [34880/72641]\tLoss: 0.6423\tLR: 0.000010\n",
      "Training Epoch: 11 [35200/72641]\tLoss: 0.6446\tLR: 0.000010\n",
      "Training Epoch: 11 [35520/72641]\tLoss: 0.6628\tLR: 0.000010\n",
      "Training Epoch: 11 [35840/72641]\tLoss: 0.7008\tLR: 0.000010\n",
      "Training Epoch: 11 [36160/72641]\tLoss: 0.6260\tLR: 0.000010\n",
      "Training Epoch: 11 [36480/72641]\tLoss: 0.6106\tLR: 0.000010\n",
      "Training Epoch: 11 [36800/72641]\tLoss: 0.6369\tLR: 0.000010\n",
      "Training Epoch: 11 [37120/72641]\tLoss: 0.6962\tLR: 0.000010\n",
      "Training Epoch: 11 [37440/72641]\tLoss: 0.6614\tLR: 0.000010\n",
      "Training Epoch: 11 [37760/72641]\tLoss: 0.6742\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [38080/72641]\tLoss: 0.6456\tLR: 0.000010\n",
      "Training Epoch: 11 [38400/72641]\tLoss: 0.6543\tLR: 0.000010\n",
      "Training Epoch: 11 [38720/72641]\tLoss: 0.6959\tLR: 0.000010\n",
      "Training Epoch: 11 [39040/72641]\tLoss: 0.6119\tLR: 0.000010\n",
      "Training Epoch: 11 [39360/72641]\tLoss: 0.6337\tLR: 0.000010\n",
      "Training Epoch: 11 [39680/72641]\tLoss: 0.6521\tLR: 0.000010\n",
      "Training Epoch: 11 [40000/72641]\tLoss: 0.6764\tLR: 0.000010\n",
      "Training Epoch: 11 [40320/72641]\tLoss: 0.6072\tLR: 0.000010\n",
      "Training Epoch: 11 [40640/72641]\tLoss: 0.6896\tLR: 0.000010\n",
      "Training Epoch: 11 [40960/72641]\tLoss: 0.6531\tLR: 0.000010\n",
      "Training Epoch: 11 [41280/72641]\tLoss: 0.6413\tLR: 0.000010\n",
      "Training Epoch: 11 [41600/72641]\tLoss: 0.6327\tLR: 0.000010\n",
      "Training Epoch: 11 [41920/72641]\tLoss: 0.6573\tLR: 0.000010\n",
      "Training Epoch: 11 [42240/72641]\tLoss: 0.6218\tLR: 0.000010\n",
      "Training Epoch: 11 [42560/72641]\tLoss: 0.6198\tLR: 0.000010\n",
      "Training Epoch: 11 [42880/72641]\tLoss: 0.6918\tLR: 0.000010\n",
      "Training Epoch: 11 [43200/72641]\tLoss: 0.6477\tLR: 0.000010\n",
      "Training Epoch: 11 [43520/72641]\tLoss: 0.6452\tLR: 0.000010\n",
      "Training Epoch: 11 [43840/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 11 [44160/72641]\tLoss: 0.6514\tLR: 0.000010\n",
      "Training Epoch: 11 [44480/72641]\tLoss: 0.6655\tLR: 0.000010\n",
      "Training Epoch: 11 [44800/72641]\tLoss: 0.6343\tLR: 0.000010\n",
      "Training Epoch: 11 [45120/72641]\tLoss: 0.6690\tLR: 0.000010\n",
      "Training Epoch: 11 [45440/72641]\tLoss: 0.6594\tLR: 0.000010\n",
      "Training Epoch: 11 [45760/72641]\tLoss: 0.6883\tLR: 0.000010\n",
      "Training Epoch: 11 [46080/72641]\tLoss: 0.6684\tLR: 0.000010\n",
      "Training Epoch: 11 [46400/72641]\tLoss: 0.6156\tLR: 0.000010\n",
      "Training Epoch: 11 [46720/72641]\tLoss: 0.6586\tLR: 0.000010\n",
      "Training Epoch: 11 [47040/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 11 [47360/72641]\tLoss: 0.7055\tLR: 0.000010\n",
      "Training Epoch: 11 [47680/72641]\tLoss: 0.6573\tLR: 0.000010\n",
      "Training Epoch: 11 [48000/72641]\tLoss: 0.6752\tLR: 0.000010\n",
      "Training Epoch: 11 [48320/72641]\tLoss: 0.6436\tLR: 0.000010\n",
      "Training Epoch: 11 [48640/72641]\tLoss: 0.6638\tLR: 0.000010\n",
      "Training Epoch: 11 [48960/72641]\tLoss: 0.6349\tLR: 0.000010\n",
      "Training Epoch: 11 [49280/72641]\tLoss: 0.6500\tLR: 0.000010\n",
      "Training Epoch: 11 [49600/72641]\tLoss: 0.6547\tLR: 0.000010\n",
      "Training Epoch: 11 [49920/72641]\tLoss: 0.6679\tLR: 0.000010\n",
      "Training Epoch: 11 [50240/72641]\tLoss: 0.6425\tLR: 0.000010\n",
      "Training Epoch: 11 [50560/72641]\tLoss: 0.6489\tLR: 0.000010\n",
      "Training Epoch: 11 [50880/72641]\tLoss: 0.6347\tLR: 0.000010\n",
      "Training Epoch: 11 [51200/72641]\tLoss: 0.6769\tLR: 0.000010\n",
      "Training Epoch: 11 [51520/72641]\tLoss: 0.6727\tLR: 0.000010\n",
      "Training Epoch: 11 [51840/72641]\tLoss: 0.6766\tLR: 0.000010\n",
      "Training Epoch: 11 [52160/72641]\tLoss: 0.6500\tLR: 0.000010\n",
      "Training Epoch: 11 [52480/72641]\tLoss: 0.6314\tLR: 0.000010\n",
      "Training Epoch: 11 [52800/72641]\tLoss: 0.6373\tLR: 0.000010\n",
      "Training Epoch: 11 [53120/72641]\tLoss: 0.6188\tLR: 0.000010\n",
      "Training Epoch: 11 [53440/72641]\tLoss: 0.6583\tLR: 0.000010\n",
      "Training Epoch: 11 [53760/72641]\tLoss: 0.6837\tLR: 0.000010\n",
      "Training Epoch: 11 [54080/72641]\tLoss: 0.6506\tLR: 0.000010\n",
      "Training Epoch: 11 [54400/72641]\tLoss: 0.6429\tLR: 0.000010\n",
      "Training Epoch: 11 [54720/72641]\tLoss: 0.6378\tLR: 0.000010\n",
      "Training Epoch: 11 [55040/72641]\tLoss: 0.6478\tLR: 0.000010\n",
      "Training Epoch: 11 [55360/72641]\tLoss: 0.6400\tLR: 0.000010\n",
      "Training Epoch: 11 [55680/72641]\tLoss: 0.6379\tLR: 0.000010\n",
      "Training Epoch: 11 [56000/72641]\tLoss: 0.5944\tLR: 0.000010\n",
      "Training Epoch: 11 [56320/72641]\tLoss: 0.6633\tLR: 0.000010\n",
      "Training Epoch: 11 [56640/72641]\tLoss: 0.6260\tLR: 0.000010\n",
      "Training Epoch: 11 [56960/72641]\tLoss: 0.6729\tLR: 0.000010\n",
      "Training Epoch: 11 [57280/72641]\tLoss: 0.6254\tLR: 0.000010\n",
      "Training Epoch: 11 [57600/72641]\tLoss: 0.6671\tLR: 0.000010\n",
      "Training Epoch: 11 [57920/72641]\tLoss: 0.6873\tLR: 0.000010\n",
      "Training Epoch: 11 [58240/72641]\tLoss: 0.6410\tLR: 0.000010\n",
      "Training Epoch: 11 [58560/72641]\tLoss: 0.6514\tLR: 0.000010\n",
      "Training Epoch: 11 [58880/72641]\tLoss: 0.6436\tLR: 0.000010\n",
      "Training Epoch: 11 [59200/72641]\tLoss: 0.6822\tLR: 0.000010\n",
      "Training Epoch: 11 [59520/72641]\tLoss: 0.6254\tLR: 0.000010\n",
      "Training Epoch: 11 [59840/72641]\tLoss: 0.6985\tLR: 0.000010\n",
      "Training Epoch: 11 [60160/72641]\tLoss: 0.6395\tLR: 0.000010\n",
      "Training Epoch: 11 [60480/72641]\tLoss: 0.6713\tLR: 0.000010\n",
      "Training Epoch: 11 [60800/72641]\tLoss: 0.6324\tLR: 0.000010\n",
      "Training Epoch: 11 [61120/72641]\tLoss: 0.6437\tLR: 0.000010\n",
      "Training Epoch: 11 [61440/72641]\tLoss: 0.6230\tLR: 0.000010\n",
      "Training Epoch: 11 [61760/72641]\tLoss: 0.6116\tLR: 0.000010\n",
      "Training Epoch: 11 [62080/72641]\tLoss: 0.7329\tLR: 0.000010\n",
      "Training Epoch: 11 [62400/72641]\tLoss: 0.6298\tLR: 0.000010\n",
      "Training Epoch: 11 [62720/72641]\tLoss: 0.6524\tLR: 0.000010\n",
      "Training Epoch: 11 [63040/72641]\tLoss: 0.6335\tLR: 0.000010\n",
      "Training Epoch: 11 [63360/72641]\tLoss: 0.7051\tLR: 0.000010\n",
      "Training Epoch: 11 [63680/72641]\tLoss: 0.6687\tLR: 0.000010\n",
      "Training Epoch: 11 [64000/72641]\tLoss: 0.6369\tLR: 0.000010\n",
      "Training Epoch: 11 [64320/72641]\tLoss: 0.6922\tLR: 0.000010\n",
      "Training Epoch: 11 [64640/72641]\tLoss: 0.6388\tLR: 0.000010\n",
      "Training Epoch: 11 [64960/72641]\tLoss: 0.6594\tLR: 0.000010\n",
      "Training Epoch: 11 [65280/72641]\tLoss: 0.6242\tLR: 0.000010\n",
      "Training Epoch: 11 [65600/72641]\tLoss: 0.6435\tLR: 0.000010\n",
      "Training Epoch: 11 [65920/72641]\tLoss: 0.6811\tLR: 0.000010\n",
      "Training Epoch: 11 [66240/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 11 [66560/72641]\tLoss: 0.6400\tLR: 0.000010\n",
      "Training Epoch: 11 [66880/72641]\tLoss: 0.6406\tLR: 0.000010\n",
      "Training Epoch: 11 [67200/72641]\tLoss: 0.6551\tLR: 0.000010\n",
      "Training Epoch: 11 [67520/72641]\tLoss: 0.6553\tLR: 0.000010\n",
      "Training Epoch: 11 [67840/72641]\tLoss: 0.6541\tLR: 0.000010\n",
      "Training Epoch: 11 [68160/72641]\tLoss: 0.6542\tLR: 0.000010\n",
      "Training Epoch: 11 [68480/72641]\tLoss: 0.6296\tLR: 0.000010\n",
      "Training Epoch: 11 [68800/72641]\tLoss: 0.6604\tLR: 0.000010\n",
      "Training Epoch: 11 [69120/72641]\tLoss: 0.6522\tLR: 0.000010\n",
      "Training Epoch: 11 [69440/72641]\tLoss: 0.6183\tLR: 0.000010\n",
      "Training Epoch: 11 [69760/72641]\tLoss: 0.6665\tLR: 0.000010\n",
      "Training Epoch: 11 [70080/72641]\tLoss: 0.6220\tLR: 0.000010\n",
      "Training Epoch: 11 [70400/72641]\tLoss: 0.6579\tLR: 0.000010\n",
      "Training Epoch: 11 [70720/72641]\tLoss: 0.6900\tLR: 0.000010\n",
      "Training Epoch: 11 [71040/72641]\tLoss: 0.6384\tLR: 0.000010\n",
      "Training Epoch: 11 [71360/72641]\tLoss: 0.6775\tLR: 0.000010\n",
      "Training Epoch: 11 [71680/72641]\tLoss: 0.6660\tLR: 0.000010\n",
      "Training Epoch: 11 [72000/72641]\tLoss: 0.6769\tLR: 0.000010\n",
      "Training Epoch: 11 [72320/72641]\tLoss: 0.6626\tLR: 0.000010\n",
      "Training Epoch: 11 [72640/72641]\tLoss: 0.6093\tLR: 0.000010\n",
      "Val Result: Acc: 0.1495, C_ACC: 0.6985, DOA: 87.8040, ACC_k: 0.1075\n",
      "ext:0.0, cls:0.577913, coar:0.0, fine:0.0,\n",
      "Training Epoch: 12 [320/72641]\tLoss: 0.6655\tLR: 0.000010\n",
      "Training Epoch: 12 [640/72641]\tLoss: 0.6102\tLR: 0.000010\n",
      "Training Epoch: 12 [960/72641]\tLoss: 0.6247\tLR: 0.000010\n",
      "Training Epoch: 12 [1280/72641]\tLoss: 0.6542\tLR: 0.000010\n",
      "Training Epoch: 12 [1600/72641]\tLoss: 0.6521\tLR: 0.000010\n",
      "Training Epoch: 12 [1920/72641]\tLoss: 0.6535\tLR: 0.000010\n",
      "Training Epoch: 12 [2240/72641]\tLoss: 0.6213\tLR: 0.000010\n",
      "Training Epoch: 12 [2560/72641]\tLoss: 0.6408\tLR: 0.000010\n",
      "Training Epoch: 12 [2880/72641]\tLoss: 0.6710\tLR: 0.000010\n",
      "Training Epoch: 12 [3200/72641]\tLoss: 0.6891\tLR: 0.000010\n",
      "Training Epoch: 12 [3520/72641]\tLoss: 0.6558\tLR: 0.000010\n",
      "Training Epoch: 12 [3840/72641]\tLoss: 0.6877\tLR: 0.000010\n",
      "Training Epoch: 12 [4160/72641]\tLoss: 0.6360\tLR: 0.000010\n",
      "Training Epoch: 12 [4480/72641]\tLoss: 0.6088\tLR: 0.000010\n",
      "Training Epoch: 12 [4800/72641]\tLoss: 0.6553\tLR: 0.000010\n",
      "Training Epoch: 12 [5120/72641]\tLoss: 0.6116\tLR: 0.000010\n",
      "Training Epoch: 12 [5440/72641]\tLoss: 0.6608\tLR: 0.000010\n",
      "Training Epoch: 12 [5760/72641]\tLoss: 0.6565\tLR: 0.000010\n",
      "Training Epoch: 12 [6080/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 12 [6400/72641]\tLoss: 0.6428\tLR: 0.000010\n",
      "Training Epoch: 12 [6720/72641]\tLoss: 0.6033\tLR: 0.000010\n",
      "Training Epoch: 12 [7040/72641]\tLoss: 0.6515\tLR: 0.000010\n",
      "Training Epoch: 12 [7360/72641]\tLoss: 0.6412\tLR: 0.000010\n",
      "Training Epoch: 12 [7680/72641]\tLoss: 0.6758\tLR: 0.000010\n",
      "Training Epoch: 12 [8000/72641]\tLoss: 0.6701\tLR: 0.000010\n",
      "Training Epoch: 12 [8320/72641]\tLoss: 0.6645\tLR: 0.000010\n",
      "Training Epoch: 12 [8640/72641]\tLoss: 0.6753\tLR: 0.000010\n",
      "Training Epoch: 12 [8960/72641]\tLoss: 0.5812\tLR: 0.000010\n",
      "Training Epoch: 12 [9280/72641]\tLoss: 0.6288\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [9600/72641]\tLoss: 0.6613\tLR: 0.000010\n",
      "Training Epoch: 12 [9920/72641]\tLoss: 0.6788\tLR: 0.000010\n",
      "Training Epoch: 12 [10240/72641]\tLoss: 0.6256\tLR: 0.000010\n",
      "Training Epoch: 12 [10560/72641]\tLoss: 0.6255\tLR: 0.000010\n",
      "Training Epoch: 12 [10880/72641]\tLoss: 0.6511\tLR: 0.000010\n",
      "Training Epoch: 12 [11200/72641]\tLoss: 0.6711\tLR: 0.000010\n",
      "Training Epoch: 12 [11520/72641]\tLoss: 0.6351\tLR: 0.000010\n",
      "Training Epoch: 12 [11840/72641]\tLoss: 0.6187\tLR: 0.000010\n",
      "Training Epoch: 12 [12160/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 12 [12480/72641]\tLoss: 0.6303\tLR: 0.000010\n",
      "Training Epoch: 12 [12800/72641]\tLoss: 0.6552\tLR: 0.000010\n",
      "Training Epoch: 12 [13120/72641]\tLoss: 0.6938\tLR: 0.000010\n",
      "Training Epoch: 12 [13440/72641]\tLoss: 0.6517\tLR: 0.000010\n",
      "Training Epoch: 12 [13760/72641]\tLoss: 0.6785\tLR: 0.000010\n",
      "Training Epoch: 12 [14080/72641]\tLoss: 0.6907\tLR: 0.000010\n",
      "Training Epoch: 12 [14400/72641]\tLoss: 0.6277\tLR: 0.000010\n",
      "Training Epoch: 12 [14720/72641]\tLoss: 0.6500\tLR: 0.000010\n",
      "Training Epoch: 12 [15040/72641]\tLoss: 0.6540\tLR: 0.000010\n",
      "Training Epoch: 12 [15360/72641]\tLoss: 0.6538\tLR: 0.000010\n",
      "Training Epoch: 12 [15680/72641]\tLoss: 0.6813\tLR: 0.000010\n",
      "Training Epoch: 12 [16000/72641]\tLoss: 0.6420\tLR: 0.000010\n",
      "Training Epoch: 12 [16320/72641]\tLoss: 0.6768\tLR: 0.000010\n",
      "Training Epoch: 12 [16640/72641]\tLoss: 0.6470\tLR: 0.000010\n",
      "Training Epoch: 12 [16960/72641]\tLoss: 0.6071\tLR: 0.000010\n",
      "Training Epoch: 12 [17280/72641]\tLoss: 0.6505\tLR: 0.000010\n",
      "Training Epoch: 12 [17600/72641]\tLoss: 0.6125\tLR: 0.000010\n",
      "Training Epoch: 12 [17920/72641]\tLoss: 0.6924\tLR: 0.000010\n",
      "Training Epoch: 12 [18240/72641]\tLoss: 0.6481\tLR: 0.000010\n",
      "Training Epoch: 12 [18560/72641]\tLoss: 0.6543\tLR: 0.000010\n",
      "Training Epoch: 12 [18880/72641]\tLoss: 0.6298\tLR: 0.000010\n",
      "Training Epoch: 12 [19200/72641]\tLoss: 0.6598\tLR: 0.000010\n",
      "Training Epoch: 12 [19520/72641]\tLoss: 0.6563\tLR: 0.000010\n",
      "Training Epoch: 12 [19840/72641]\tLoss: 0.6308\tLR: 0.000010\n",
      "Training Epoch: 12 [20160/72641]\tLoss: 0.6329\tLR: 0.000010\n",
      "Training Epoch: 12 [20480/72641]\tLoss: 0.6262\tLR: 0.000010\n",
      "Training Epoch: 12 [20800/72641]\tLoss: 0.6797\tLR: 0.000010\n",
      "Training Epoch: 12 [21120/72641]\tLoss: 0.6451\tLR: 0.000010\n",
      "Training Epoch: 12 [21440/72641]\tLoss: 0.6647\tLR: 0.000010\n",
      "Training Epoch: 12 [21760/72641]\tLoss: 0.6567\tLR: 0.000010\n",
      "Training Epoch: 12 [22080/72641]\tLoss: 0.6656\tLR: 0.000010\n",
      "Training Epoch: 12 [22400/72641]\tLoss: 0.6304\tLR: 0.000010\n",
      "Training Epoch: 12 [22720/72641]\tLoss: 0.6168\tLR: 0.000010\n",
      "Training Epoch: 12 [23040/72641]\tLoss: 0.6462\tLR: 0.000010\n",
      "Training Epoch: 12 [23360/72641]\tLoss: 0.6332\tLR: 0.000010\n",
      "Training Epoch: 12 [23680/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 12 [24000/72641]\tLoss: 0.6093\tLR: 0.000010\n",
      "Training Epoch: 12 [24320/72641]\tLoss: 0.6720\tLR: 0.000010\n",
      "Training Epoch: 12 [24640/72641]\tLoss: 0.6540\tLR: 0.000010\n",
      "Training Epoch: 12 [24960/72641]\tLoss: 0.6324\tLR: 0.000010\n",
      "Training Epoch: 12 [25280/72641]\tLoss: 0.6416\tLR: 0.000010\n",
      "Training Epoch: 12 [25600/72641]\tLoss: 0.6613\tLR: 0.000010\n",
      "Training Epoch: 12 [25920/72641]\tLoss: 0.6149\tLR: 0.000010\n",
      "Training Epoch: 12 [26240/72641]\tLoss: 0.6267\tLR: 0.000010\n",
      "Training Epoch: 12 [26560/72641]\tLoss: 0.6869\tLR: 0.000010\n",
      "Training Epoch: 12 [26880/72641]\tLoss: 0.6439\tLR: 0.000010\n",
      "Training Epoch: 12 [27200/72641]\tLoss: 0.6508\tLR: 0.000010\n",
      "Training Epoch: 12 [27520/72641]\tLoss: 0.6849\tLR: 0.000010\n",
      "Training Epoch: 12 [27840/72641]\tLoss: 0.6282\tLR: 0.000010\n",
      "Training Epoch: 12 [28160/72641]\tLoss: 0.6665\tLR: 0.000010\n",
      "Training Epoch: 12 [28480/72641]\tLoss: 0.6044\tLR: 0.000010\n",
      "Training Epoch: 12 [28800/72641]\tLoss: 0.6351\tLR: 0.000010\n",
      "Training Epoch: 12 [29120/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 12 [29440/72641]\tLoss: 0.6558\tLR: 0.000010\n",
      "Training Epoch: 12 [29760/72641]\tLoss: 0.6837\tLR: 0.000010\n",
      "Training Epoch: 12 [30080/72641]\tLoss: 0.7052\tLR: 0.000010\n",
      "Training Epoch: 12 [30400/72641]\tLoss: 0.6595\tLR: 0.000010\n",
      "Training Epoch: 12 [30720/72641]\tLoss: 0.6915\tLR: 0.000010\n",
      "Training Epoch: 12 [31040/72641]\tLoss: 0.6619\tLR: 0.000010\n",
      "Training Epoch: 12 [31360/72641]\tLoss: 0.6082\tLR: 0.000010\n",
      "Training Epoch: 12 [31680/72641]\tLoss: 0.6308\tLR: 0.000010\n",
      "Training Epoch: 12 [32000/72641]\tLoss: 0.6345\tLR: 0.000010\n",
      "Training Epoch: 12 [32320/72641]\tLoss: 0.6841\tLR: 0.000010\n",
      "Training Epoch: 12 [32640/72641]\tLoss: 0.6111\tLR: 0.000010\n",
      "Training Epoch: 12 [32960/72641]\tLoss: 0.6952\tLR: 0.000010\n",
      "Training Epoch: 12 [33280/72641]\tLoss: 0.6470\tLR: 0.000010\n",
      "Training Epoch: 12 [33600/72641]\tLoss: 0.6511\tLR: 0.000010\n",
      "Training Epoch: 12 [33920/72641]\tLoss: 0.6436\tLR: 0.000010\n",
      "Training Epoch: 12 [34240/72641]\tLoss: 0.6266\tLR: 0.000010\n",
      "Training Epoch: 12 [34560/72641]\tLoss: 0.7104\tLR: 0.000010\n",
      "Training Epoch: 12 [34880/72641]\tLoss: 0.6745\tLR: 0.000010\n",
      "Training Epoch: 12 [35200/72641]\tLoss: 0.6435\tLR: 0.000010\n",
      "Training Epoch: 12 [35520/72641]\tLoss: 0.6269\tLR: 0.000010\n",
      "Training Epoch: 12 [35840/72641]\tLoss: 0.6668\tLR: 0.000010\n",
      "Training Epoch: 12 [36160/72641]\tLoss: 0.6360\tLR: 0.000010\n",
      "Training Epoch: 12 [36480/72641]\tLoss: 0.5987\tLR: 0.000010\n",
      "Training Epoch: 12 [36800/72641]\tLoss: 0.6671\tLR: 0.000010\n",
      "Training Epoch: 12 [37120/72641]\tLoss: 0.6560\tLR: 0.000010\n",
      "Training Epoch: 12 [37440/72641]\tLoss: 0.6800\tLR: 0.000010\n",
      "Training Epoch: 12 [37760/72641]\tLoss: 0.6654\tLR: 0.000010\n",
      "Training Epoch: 12 [38080/72641]\tLoss: 0.6544\tLR: 0.000010\n",
      "Training Epoch: 12 [38400/72641]\tLoss: 0.6709\tLR: 0.000010\n",
      "Training Epoch: 12 [38720/72641]\tLoss: 0.6553\tLR: 0.000010\n",
      "Training Epoch: 12 [39040/72641]\tLoss: 0.6525\tLR: 0.000010\n",
      "Training Epoch: 12 [39360/72641]\tLoss: 0.6458\tLR: 0.000010\n",
      "Training Epoch: 12 [39680/72641]\tLoss: 0.6375\tLR: 0.000010\n",
      "Training Epoch: 12 [40000/72641]\tLoss: 0.6829\tLR: 0.000010\n",
      "Training Epoch: 12 [40320/72641]\tLoss: 0.6655\tLR: 0.000010\n",
      "Training Epoch: 12 [40640/72641]\tLoss: 0.6532\tLR: 0.000010\n",
      "Training Epoch: 12 [40960/72641]\tLoss: 0.6341\tLR: 0.000010\n",
      "Training Epoch: 12 [41280/72641]\tLoss: 0.6561\tLR: 0.000010\n",
      "Training Epoch: 12 [41600/72641]\tLoss: 0.6418\tLR: 0.000010\n",
      "Training Epoch: 12 [41920/72641]\tLoss: 0.6517\tLR: 0.000010\n",
      "Training Epoch: 12 [42240/72641]\tLoss: 0.6391\tLR: 0.000010\n",
      "Training Epoch: 12 [42560/72641]\tLoss: 0.6593\tLR: 0.000010\n",
      "Training Epoch: 12 [42880/72641]\tLoss: 0.6737\tLR: 0.000010\n",
      "Training Epoch: 12 [43200/72641]\tLoss: 0.6445\tLR: 0.000010\n",
      "Training Epoch: 12 [43520/72641]\tLoss: 0.6482\tLR: 0.000010\n",
      "Training Epoch: 12 [43840/72641]\tLoss: 0.6165\tLR: 0.000010\n",
      "Training Epoch: 12 [44160/72641]\tLoss: 0.6749\tLR: 0.000010\n",
      "Training Epoch: 12 [44480/72641]\tLoss: 0.6782\tLR: 0.000010\n",
      "Training Epoch: 12 [44800/72641]\tLoss: 0.6491\tLR: 0.000010\n",
      "Training Epoch: 12 [45120/72641]\tLoss: 0.6634\tLR: 0.000010\n",
      "Training Epoch: 12 [45440/72641]\tLoss: 0.6384\tLR: 0.000010\n",
      "Training Epoch: 12 [45760/72641]\tLoss: 0.6189\tLR: 0.000010\n",
      "Training Epoch: 12 [46080/72641]\tLoss: 0.6651\tLR: 0.000010\n",
      "Training Epoch: 12 [46400/72641]\tLoss: 0.6737\tLR: 0.000010\n",
      "Training Epoch: 12 [46720/72641]\tLoss: 0.6679\tLR: 0.000010\n",
      "Training Epoch: 12 [47040/72641]\tLoss: 0.6149\tLR: 0.000010\n",
      "Training Epoch: 12 [47360/72641]\tLoss: 0.6849\tLR: 0.000010\n",
      "Training Epoch: 12 [47680/72641]\tLoss: 0.6687\tLR: 0.000010\n",
      "Training Epoch: 12 [48000/72641]\tLoss: 0.6241\tLR: 0.000010\n",
      "Training Epoch: 12 [48320/72641]\tLoss: 0.6807\tLR: 0.000010\n",
      "Training Epoch: 12 [48640/72641]\tLoss: 0.6716\tLR: 0.000010\n",
      "Training Epoch: 12 [48960/72641]\tLoss: 0.6462\tLR: 0.000010\n",
      "Training Epoch: 12 [49280/72641]\tLoss: 0.6888\tLR: 0.000010\n",
      "Training Epoch: 12 [49600/72641]\tLoss: 0.6817\tLR: 0.000010\n",
      "Training Epoch: 12 [49920/72641]\tLoss: 0.6059\tLR: 0.000010\n",
      "Training Epoch: 12 [50240/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 12 [50560/72641]\tLoss: 0.6492\tLR: 0.000010\n",
      "Training Epoch: 12 [50880/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 12 [51200/72641]\tLoss: 0.6726\tLR: 0.000010\n",
      "Training Epoch: 12 [51520/72641]\tLoss: 0.6493\tLR: 0.000010\n",
      "Training Epoch: 12 [51840/72641]\tLoss: 0.6326\tLR: 0.000010\n",
      "Training Epoch: 12 [52160/72641]\tLoss: 0.6632\tLR: 0.000010\n",
      "Training Epoch: 12 [52480/72641]\tLoss: 0.6638\tLR: 0.000010\n",
      "Training Epoch: 12 [52800/72641]\tLoss: 0.6709\tLR: 0.000010\n",
      "Training Epoch: 12 [53120/72641]\tLoss: 0.6331\tLR: 0.000010\n",
      "Training Epoch: 12 [53440/72641]\tLoss: 0.6437\tLR: 0.000010\n",
      "Training Epoch: 12 [53760/72641]\tLoss: 0.6382\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [54080/72641]\tLoss: 0.6568\tLR: 0.000010\n",
      "Training Epoch: 12 [54400/72641]\tLoss: 0.6374\tLR: 0.000010\n",
      "Training Epoch: 12 [54720/72641]\tLoss: 0.6638\tLR: 0.000010\n",
      "Training Epoch: 12 [55040/72641]\tLoss: 0.6729\tLR: 0.000010\n",
      "Training Epoch: 12 [55360/72641]\tLoss: 0.6230\tLR: 0.000010\n",
      "Training Epoch: 12 [55680/72641]\tLoss: 0.6158\tLR: 0.000010\n",
      "Training Epoch: 12 [56000/72641]\tLoss: 0.6604\tLR: 0.000010\n",
      "Training Epoch: 12 [56320/72641]\tLoss: 0.6379\tLR: 0.000010\n",
      "Training Epoch: 12 [56640/72641]\tLoss: 0.6913\tLR: 0.000010\n",
      "Training Epoch: 12 [56960/72641]\tLoss: 0.6848\tLR: 0.000010\n",
      "Training Epoch: 12 [57280/72641]\tLoss: 0.6295\tLR: 0.000010\n",
      "Training Epoch: 12 [57600/72641]\tLoss: 0.6850\tLR: 0.000010\n",
      "Training Epoch: 12 [57920/72641]\tLoss: 0.6227\tLR: 0.000010\n",
      "Training Epoch: 12 [58240/72641]\tLoss: 0.6619\tLR: 0.000010\n",
      "Training Epoch: 12 [58560/72641]\tLoss: 0.6541\tLR: 0.000010\n",
      "Training Epoch: 12 [58880/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 12 [59200/72641]\tLoss: 0.6780\tLR: 0.000010\n",
      "Training Epoch: 12 [59520/72641]\tLoss: 0.6618\tLR: 0.000010\n",
      "Training Epoch: 12 [59840/72641]\tLoss: 0.6608\tLR: 0.000010\n",
      "Training Epoch: 12 [60160/72641]\tLoss: 0.6501\tLR: 0.000010\n",
      "Training Epoch: 12 [60480/72641]\tLoss: 0.6845\tLR: 0.000010\n",
      "Training Epoch: 12 [60800/72641]\tLoss: 0.6460\tLR: 0.000010\n",
      "Training Epoch: 12 [61120/72641]\tLoss: 0.6275\tLR: 0.000010\n",
      "Training Epoch: 12 [61440/72641]\tLoss: 0.6118\tLR: 0.000010\n",
      "Training Epoch: 12 [61760/72641]\tLoss: 0.6486\tLR: 0.000010\n",
      "Training Epoch: 12 [62080/72641]\tLoss: 0.6547\tLR: 0.000010\n",
      "Training Epoch: 12 [62400/72641]\tLoss: 0.6434\tLR: 0.000010\n",
      "Training Epoch: 12 [62720/72641]\tLoss: 0.6547\tLR: 0.000010\n",
      "Training Epoch: 12 [63040/72641]\tLoss: 0.6311\tLR: 0.000010\n",
      "Training Epoch: 12 [63360/72641]\tLoss: 0.6886\tLR: 0.000010\n",
      "Training Epoch: 12 [63680/72641]\tLoss: 0.6617\tLR: 0.000010\n",
      "Training Epoch: 12 [64000/72641]\tLoss: 0.6234\tLR: 0.000010\n",
      "Training Epoch: 12 [64320/72641]\tLoss: 0.6227\tLR: 0.000010\n",
      "Training Epoch: 12 [64640/72641]\tLoss: 0.6386\tLR: 0.000010\n",
      "Training Epoch: 12 [64960/72641]\tLoss: 0.6629\tLR: 0.000010\n",
      "Training Epoch: 12 [65280/72641]\tLoss: 0.6324\tLR: 0.000010\n",
      "Training Epoch: 12 [65600/72641]\tLoss: 0.6479\tLR: 0.000010\n",
      "Training Epoch: 12 [65920/72641]\tLoss: 0.6658\tLR: 0.000010\n",
      "Training Epoch: 12 [66240/72641]\tLoss: 0.7089\tLR: 0.000010\n",
      "Training Epoch: 12 [66560/72641]\tLoss: 0.6480\tLR: 0.000010\n",
      "Training Epoch: 12 [66880/72641]\tLoss: 0.6485\tLR: 0.000010\n",
      "Training Epoch: 12 [67200/72641]\tLoss: 0.6120\tLR: 0.000010\n",
      "Training Epoch: 12 [67520/72641]\tLoss: 0.6423\tLR: 0.000010\n",
      "Training Epoch: 12 [67840/72641]\tLoss: 0.6581\tLR: 0.000010\n",
      "Training Epoch: 12 [68160/72641]\tLoss: 0.6244\tLR: 0.000010\n",
      "Training Epoch: 12 [68480/72641]\tLoss: 0.6458\tLR: 0.000010\n",
      "Training Epoch: 12 [68800/72641]\tLoss: 0.6415\tLR: 0.000010\n",
      "Training Epoch: 12 [69120/72641]\tLoss: 0.6639\tLR: 0.000010\n",
      "Training Epoch: 12 [69440/72641]\tLoss: 0.6444\tLR: 0.000010\n",
      "Training Epoch: 12 [69760/72641]\tLoss: 0.6705\tLR: 0.000010\n",
      "Training Epoch: 12 [70080/72641]\tLoss: 0.6449\tLR: 0.000010\n",
      "Training Epoch: 12 [70400/72641]\tLoss: 0.6908\tLR: 0.000010\n",
      "Training Epoch: 12 [70720/72641]\tLoss: 0.6956\tLR: 0.000010\n",
      "Training Epoch: 12 [71040/72641]\tLoss: 0.6457\tLR: 0.000010\n",
      "Training Epoch: 12 [71360/72641]\tLoss: 0.6175\tLR: 0.000010\n",
      "Training Epoch: 12 [71680/72641]\tLoss: 0.6527\tLR: 0.000010\n",
      "Training Epoch: 12 [72000/72641]\tLoss: 0.6490\tLR: 0.000010\n",
      "Training Epoch: 12 [72320/72641]\tLoss: 0.6170\tLR: 0.000010\n",
      "Training Epoch: 12 [72640/72641]\tLoss: 0.6396\tLR: 0.000010\n",
      "Val Result: Acc: 0.1465, C_ACC: 0.6860, DOA: 88.7327, ACC_k: 0.1039\n",
      "ext:0.0, cls:0.587463, coar:0.0, fine:0.0,\n",
      "Training Epoch: 13 [320/72641]\tLoss: 0.6609\tLR: 0.000010\n",
      "Training Epoch: 13 [640/72641]\tLoss: 0.6332\tLR: 0.000010\n",
      "Training Epoch: 13 [960/72641]\tLoss: 0.6251\tLR: 0.000010\n",
      "Training Epoch: 13 [1280/72641]\tLoss: 0.6333\tLR: 0.000010\n",
      "Training Epoch: 13 [1600/72641]\tLoss: 0.6821\tLR: 0.000010\n",
      "Training Epoch: 13 [1920/72641]\tLoss: 0.6386\tLR: 0.000010\n",
      "Training Epoch: 13 [2240/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 13 [2560/72641]\tLoss: 0.6145\tLR: 0.000010\n",
      "Training Epoch: 13 [2880/72641]\tLoss: 0.6499\tLR: 0.000010\n",
      "Training Epoch: 13 [3200/72641]\tLoss: 0.6345\tLR: 0.000010\n",
      "Training Epoch: 13 [3520/72641]\tLoss: 0.6302\tLR: 0.000010\n",
      "Training Epoch: 13 [3840/72641]\tLoss: 0.6777\tLR: 0.000010\n",
      "Training Epoch: 13 [4160/72641]\tLoss: 0.6647\tLR: 0.000010\n",
      "Training Epoch: 13 [4480/72641]\tLoss: 0.6625\tLR: 0.000010\n",
      "Training Epoch: 13 [4800/72641]\tLoss: 0.6736\tLR: 0.000010\n",
      "Training Epoch: 13 [5120/72641]\tLoss: 0.6271\tLR: 0.000010\n",
      "Training Epoch: 13 [5440/72641]\tLoss: 0.6534\tLR: 0.000010\n",
      "Training Epoch: 13 [5760/72641]\tLoss: 0.6598\tLR: 0.000010\n",
      "Training Epoch: 13 [6080/72641]\tLoss: 0.6464\tLR: 0.000010\n",
      "Training Epoch: 13 [6400/72641]\tLoss: 0.6148\tLR: 0.000010\n",
      "Training Epoch: 13 [6720/72641]\tLoss: 0.6291\tLR: 0.000010\n",
      "Training Epoch: 13 [7040/72641]\tLoss: 0.6526\tLR: 0.000010\n",
      "Training Epoch: 13 [7360/72641]\tLoss: 0.6770\tLR: 0.000010\n",
      "Training Epoch: 13 [7680/72641]\tLoss: 0.6783\tLR: 0.000010\n",
      "Training Epoch: 13 [8000/72641]\tLoss: 0.6649\tLR: 0.000010\n",
      "Training Epoch: 13 [8320/72641]\tLoss: 0.6544\tLR: 0.000010\n",
      "Training Epoch: 13 [8640/72641]\tLoss: 0.6456\tLR: 0.000010\n",
      "Training Epoch: 13 [8960/72641]\tLoss: 0.6141\tLR: 0.000010\n",
      "Training Epoch: 13 [9280/72641]\tLoss: 0.6140\tLR: 0.000010\n",
      "Training Epoch: 13 [9600/72641]\tLoss: 0.6397\tLR: 0.000010\n",
      "Training Epoch: 13 [9920/72641]\tLoss: 0.6817\tLR: 0.000010\n",
      "Training Epoch: 13 [10240/72641]\tLoss: 0.6657\tLR: 0.000010\n",
      "Training Epoch: 13 [10560/72641]\tLoss: 0.6409\tLR: 0.000010\n",
      "Training Epoch: 13 [10880/72641]\tLoss: 0.6620\tLR: 0.000010\n",
      "Training Epoch: 13 [11200/72641]\tLoss: 0.6810\tLR: 0.000010\n",
      "Training Epoch: 13 [11520/72641]\tLoss: 0.6540\tLR: 0.000010\n",
      "Training Epoch: 13 [11840/72641]\tLoss: 0.6052\tLR: 0.000010\n",
      "Training Epoch: 13 [12160/72641]\tLoss: 0.6011\tLR: 0.000010\n",
      "Training Epoch: 13 [12480/72641]\tLoss: 0.6553\tLR: 0.000010\n",
      "Training Epoch: 13 [12800/72641]\tLoss: 0.6848\tLR: 0.000010\n",
      "Training Epoch: 13 [13120/72641]\tLoss: 0.6351\tLR: 0.000010\n",
      "Training Epoch: 13 [13440/72641]\tLoss: 0.6360\tLR: 0.000010\n",
      "Training Epoch: 13 [13760/72641]\tLoss: 0.6592\tLR: 0.000010\n",
      "Training Epoch: 13 [14080/72641]\tLoss: 0.6831\tLR: 0.000010\n",
      "Training Epoch: 13 [14400/72641]\tLoss: 0.5989\tLR: 0.000010\n",
      "Training Epoch: 13 [14720/72641]\tLoss: 0.6295\tLR: 0.000010\n",
      "Training Epoch: 13 [15040/72641]\tLoss: 0.6492\tLR: 0.000010\n",
      "Training Epoch: 13 [15360/72641]\tLoss: 0.6880\tLR: 0.000010\n",
      "Training Epoch: 13 [15680/72641]\tLoss: 0.6212\tLR: 0.000010\n",
      "Training Epoch: 13 [16000/72641]\tLoss: 0.6627\tLR: 0.000010\n",
      "Training Epoch: 13 [16320/72641]\tLoss: 0.6716\tLR: 0.000010\n",
      "Training Epoch: 13 [16640/72641]\tLoss: 0.6504\tLR: 0.000010\n",
      "Training Epoch: 13 [16960/72641]\tLoss: 0.6415\tLR: 0.000010\n",
      "Training Epoch: 13 [17280/72641]\tLoss: 0.6423\tLR: 0.000010\n",
      "Training Epoch: 13 [17600/72641]\tLoss: 0.6336\tLR: 0.000010\n",
      "Training Epoch: 13 [17920/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 13 [18240/72641]\tLoss: 0.6597\tLR: 0.000010\n",
      "Training Epoch: 13 [18560/72641]\tLoss: 0.6638\tLR: 0.000010\n",
      "Training Epoch: 13 [18880/72641]\tLoss: 0.6087\tLR: 0.000010\n",
      "Training Epoch: 13 [19200/72641]\tLoss: 0.6506\tLR: 0.000010\n",
      "Training Epoch: 13 [19520/72641]\tLoss: 0.6426\tLR: 0.000010\n",
      "Training Epoch: 13 [19840/72641]\tLoss: 0.6205\tLR: 0.000010\n",
      "Training Epoch: 13 [20160/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 13 [20480/72641]\tLoss: 0.6186\tLR: 0.000010\n",
      "Training Epoch: 13 [20800/72641]\tLoss: 0.6595\tLR: 0.000010\n",
      "Training Epoch: 13 [21120/72641]\tLoss: 0.7290\tLR: 0.000010\n",
      "Training Epoch: 13 [21440/72641]\tLoss: 0.6497\tLR: 0.000010\n",
      "Training Epoch: 13 [21760/72641]\tLoss: 0.6675\tLR: 0.000010\n",
      "Training Epoch: 13 [22080/72641]\tLoss: 0.6688\tLR: 0.000010\n",
      "Training Epoch: 13 [22400/72641]\tLoss: 0.6127\tLR: 0.000010\n",
      "Training Epoch: 13 [22720/72641]\tLoss: 0.6436\tLR: 0.000010\n",
      "Training Epoch: 13 [23040/72641]\tLoss: 0.6408\tLR: 0.000010\n",
      "Training Epoch: 13 [23360/72641]\tLoss: 0.6610\tLR: 0.000010\n",
      "Training Epoch: 13 [23680/72641]\tLoss: 0.6484\tLR: 0.000010\n",
      "Training Epoch: 13 [24000/72641]\tLoss: 0.6563\tLR: 0.000010\n",
      "Training Epoch: 13 [24320/72641]\tLoss: 0.6247\tLR: 0.000010\n",
      "Training Epoch: 13 [24640/72641]\tLoss: 0.6625\tLR: 0.000010\n",
      "Training Epoch: 13 [24960/72641]\tLoss: 0.6489\tLR: 0.000010\n",
      "Training Epoch: 13 [25280/72641]\tLoss: 0.6409\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [25600/72641]\tLoss: 0.6234\tLR: 0.000010\n",
      "Training Epoch: 13 [25920/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 13 [26240/72641]\tLoss: 0.6363\tLR: 0.000010\n",
      "Training Epoch: 13 [26560/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 13 [26880/72641]\tLoss: 0.6034\tLR: 0.000010\n",
      "Training Epoch: 13 [27200/72641]\tLoss: 0.6117\tLR: 0.000010\n",
      "Training Epoch: 13 [27520/72641]\tLoss: 0.6773\tLR: 0.000010\n",
      "Training Epoch: 13 [27840/72641]\tLoss: 0.6567\tLR: 0.000010\n",
      "Training Epoch: 13 [28160/72641]\tLoss: 0.6376\tLR: 0.000010\n",
      "Training Epoch: 13 [28480/72641]\tLoss: 0.5758\tLR: 0.000010\n",
      "Training Epoch: 13 [28800/72641]\tLoss: 0.6338\tLR: 0.000010\n",
      "Training Epoch: 13 [29120/72641]\tLoss: 0.7197\tLR: 0.000010\n",
      "Training Epoch: 13 [29440/72641]\tLoss: 0.6282\tLR: 0.000010\n",
      "Training Epoch: 13 [29760/72641]\tLoss: 0.6240\tLR: 0.000010\n",
      "Training Epoch: 13 [30080/72641]\tLoss: 0.6587\tLR: 0.000010\n",
      "Training Epoch: 13 [30400/72641]\tLoss: 0.6599\tLR: 0.000010\n",
      "Training Epoch: 13 [30720/72641]\tLoss: 0.6369\tLR: 0.000010\n",
      "Training Epoch: 13 [31040/72641]\tLoss: 0.6754\tLR: 0.000010\n",
      "Training Epoch: 13 [31360/72641]\tLoss: 0.6345\tLR: 0.000010\n",
      "Training Epoch: 13 [31680/72641]\tLoss: 0.6608\tLR: 0.000010\n",
      "Training Epoch: 13 [32000/72641]\tLoss: 0.6608\tLR: 0.000010\n",
      "Training Epoch: 13 [32320/72641]\tLoss: 0.6708\tLR: 0.000010\n",
      "Training Epoch: 13 [32640/72641]\tLoss: 0.6163\tLR: 0.000010\n",
      "Training Epoch: 13 [32960/72641]\tLoss: 0.6910\tLR: 0.000010\n",
      "Training Epoch: 13 [33280/72641]\tLoss: 0.6578\tLR: 0.000010\n",
      "Training Epoch: 13 [33600/72641]\tLoss: 0.5892\tLR: 0.000010\n",
      "Training Epoch: 13 [33920/72641]\tLoss: 0.6261\tLR: 0.000010\n",
      "Training Epoch: 13 [34240/72641]\tLoss: 0.6235\tLR: 0.000010\n",
      "Training Epoch: 13 [34560/72641]\tLoss: 0.6696\tLR: 0.000010\n",
      "Training Epoch: 13 [34880/72641]\tLoss: 0.6870\tLR: 0.000010\n",
      "Training Epoch: 13 [35200/72641]\tLoss: 0.6706\tLR: 0.000010\n",
      "Training Epoch: 13 [35520/72641]\tLoss: 0.6369\tLR: 0.000010\n",
      "Training Epoch: 13 [35840/72641]\tLoss: 0.6710\tLR: 0.000010\n",
      "Training Epoch: 13 [36160/72641]\tLoss: 0.6367\tLR: 0.000010\n",
      "Training Epoch: 13 [36480/72641]\tLoss: 0.6430\tLR: 0.000010\n",
      "Training Epoch: 13 [36800/72641]\tLoss: 0.6439\tLR: 0.000010\n",
      "Training Epoch: 13 [37120/72641]\tLoss: 0.6498\tLR: 0.000010\n",
      "Training Epoch: 13 [37440/72641]\tLoss: 0.6309\tLR: 0.000010\n",
      "Training Epoch: 13 [37760/72641]\tLoss: 0.6323\tLR: 0.000010\n",
      "Training Epoch: 13 [38080/72641]\tLoss: 0.6413\tLR: 0.000010\n",
      "Training Epoch: 13 [38400/72641]\tLoss: 0.7010\tLR: 0.000010\n",
      "Training Epoch: 13 [38720/72641]\tLoss: 0.6465\tLR: 0.000010\n",
      "Training Epoch: 13 [39040/72641]\tLoss: 0.6268\tLR: 0.000010\n",
      "Training Epoch: 13 [39360/72641]\tLoss: 0.6256\tLR: 0.000010\n",
      "Training Epoch: 13 [39680/72641]\tLoss: 0.6177\tLR: 0.000010\n",
      "Training Epoch: 13 [40000/72641]\tLoss: 0.6720\tLR: 0.000010\n",
      "Training Epoch: 13 [40320/72641]\tLoss: 0.6696\tLR: 0.000010\n",
      "Training Epoch: 13 [40640/72641]\tLoss: 0.6493\tLR: 0.000010\n",
      "Training Epoch: 13 [40960/72641]\tLoss: 0.6861\tLR: 0.000010\n",
      "Training Epoch: 13 [41280/72641]\tLoss: 0.6674\tLR: 0.000010\n",
      "Training Epoch: 13 [41600/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 13 [41920/72641]\tLoss: 0.6366\tLR: 0.000010\n",
      "Training Epoch: 13 [42240/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 13 [42560/72641]\tLoss: 0.6136\tLR: 0.000010\n",
      "Training Epoch: 13 [42880/72641]\tLoss: 0.6981\tLR: 0.000010\n",
      "Training Epoch: 13 [43200/72641]\tLoss: 0.6510\tLR: 0.000010\n",
      "Training Epoch: 13 [43520/72641]\tLoss: 0.6628\tLR: 0.000010\n",
      "Training Epoch: 13 [43840/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 13 [44160/72641]\tLoss: 0.7281\tLR: 0.000010\n",
      "Training Epoch: 13 [44480/72641]\tLoss: 0.6124\tLR: 0.000010\n",
      "Training Epoch: 13 [44800/72641]\tLoss: 0.6322\tLR: 0.000010\n",
      "Training Epoch: 13 [45120/72641]\tLoss: 0.6140\tLR: 0.000010\n",
      "Training Epoch: 13 [45440/72641]\tLoss: 0.6769\tLR: 0.000010\n",
      "Training Epoch: 13 [45760/72641]\tLoss: 0.6387\tLR: 0.000010\n",
      "Training Epoch: 13 [46080/72641]\tLoss: 0.6388\tLR: 0.000010\n",
      "Training Epoch: 13 [46400/72641]\tLoss: 0.6355\tLR: 0.000010\n",
      "Training Epoch: 13 [46720/72641]\tLoss: 0.6235\tLR: 0.000010\n",
      "Training Epoch: 13 [47040/72641]\tLoss: 0.6439\tLR: 0.000010\n",
      "Training Epoch: 13 [47360/72641]\tLoss: 0.6390\tLR: 0.000010\n",
      "Training Epoch: 13 [47680/72641]\tLoss: 0.6458\tLR: 0.000010\n",
      "Training Epoch: 13 [48000/72641]\tLoss: 0.6852\tLR: 0.000010\n",
      "Training Epoch: 13 [48320/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 13 [48640/72641]\tLoss: 0.6297\tLR: 0.000010\n",
      "Training Epoch: 13 [48960/72641]\tLoss: 0.6634\tLR: 0.000010\n",
      "Training Epoch: 13 [49280/72641]\tLoss: 0.6602\tLR: 0.000010\n",
      "Training Epoch: 13 [49600/72641]\tLoss: 0.6994\tLR: 0.000010\n",
      "Training Epoch: 13 [49920/72641]\tLoss: 0.6261\tLR: 0.000010\n",
      "Training Epoch: 13 [50240/72641]\tLoss: 0.6408\tLR: 0.000010\n",
      "Training Epoch: 13 [50560/72641]\tLoss: 0.6609\tLR: 0.000010\n",
      "Training Epoch: 13 [50880/72641]\tLoss: 0.6205\tLR: 0.000010\n",
      "Training Epoch: 13 [51200/72641]\tLoss: 0.6498\tLR: 0.000010\n",
      "Training Epoch: 13 [51520/72641]\tLoss: 0.6369\tLR: 0.000010\n",
      "Training Epoch: 13 [51840/72641]\tLoss: 0.6213\tLR: 0.000010\n",
      "Training Epoch: 13 [52160/72641]\tLoss: 0.6921\tLR: 0.000010\n",
      "Training Epoch: 13 [52480/72641]\tLoss: 0.6192\tLR: 0.000010\n",
      "Training Epoch: 13 [52800/72641]\tLoss: 0.6326\tLR: 0.000010\n",
      "Training Epoch: 13 [53120/72641]\tLoss: 0.6182\tLR: 0.000010\n",
      "Training Epoch: 13 [53440/72641]\tLoss: 0.6671\tLR: 0.000010\n",
      "Training Epoch: 13 [53760/72641]\tLoss: 0.6815\tLR: 0.000010\n",
      "Training Epoch: 13 [54080/72641]\tLoss: 0.6341\tLR: 0.000010\n",
      "Training Epoch: 13 [54400/72641]\tLoss: 0.6614\tLR: 0.000010\n",
      "Training Epoch: 13 [54720/72641]\tLoss: 0.6610\tLR: 0.000010\n",
      "Training Epoch: 13 [55040/72641]\tLoss: 0.6648\tLR: 0.000010\n",
      "Training Epoch: 13 [55360/72641]\tLoss: 0.6467\tLR: 0.000010\n",
      "Training Epoch: 13 [55680/72641]\tLoss: 0.6402\tLR: 0.000010\n",
      "Training Epoch: 13 [56000/72641]\tLoss: 0.6305\tLR: 0.000010\n",
      "Training Epoch: 13 [56320/72641]\tLoss: 0.5950\tLR: 0.000010\n",
      "Training Epoch: 13 [56640/72641]\tLoss: 0.6719\tLR: 0.000010\n",
      "Training Epoch: 13 [56960/72641]\tLoss: 0.6309\tLR: 0.000010\n",
      "Training Epoch: 13 [57280/72641]\tLoss: 0.6249\tLR: 0.000010\n",
      "Training Epoch: 13 [57600/72641]\tLoss: 0.6718\tLR: 0.000010\n",
      "Training Epoch: 13 [57920/72641]\tLoss: 0.6442\tLR: 0.000010\n",
      "Training Epoch: 13 [58240/72641]\tLoss: 0.6353\tLR: 0.000010\n",
      "Training Epoch: 13 [58560/72641]\tLoss: 0.6745\tLR: 0.000010\n",
      "Training Epoch: 13 [58880/72641]\tLoss: 0.6439\tLR: 0.000010\n",
      "Training Epoch: 13 [59200/72641]\tLoss: 0.6521\tLR: 0.000010\n",
      "Training Epoch: 13 [59520/72641]\tLoss: 0.6509\tLR: 0.000010\n",
      "Training Epoch: 13 [59840/72641]\tLoss: 0.6284\tLR: 0.000010\n",
      "Training Epoch: 13 [60160/72641]\tLoss: 0.6359\tLR: 0.000010\n",
      "Training Epoch: 13 [60480/72641]\tLoss: 0.6624\tLR: 0.000010\n",
      "Training Epoch: 13 [60800/72641]\tLoss: 0.6580\tLR: 0.000010\n",
      "Training Epoch: 13 [61120/72641]\tLoss: 0.6277\tLR: 0.000010\n",
      "Training Epoch: 13 [61440/72641]\tLoss: 0.6222\tLR: 0.000010\n",
      "Training Epoch: 13 [61760/72641]\tLoss: 0.6641\tLR: 0.000010\n",
      "Training Epoch: 13 [62080/72641]\tLoss: 0.6624\tLR: 0.000010\n",
      "Training Epoch: 13 [62400/72641]\tLoss: 0.6598\tLR: 0.000010\n",
      "Training Epoch: 13 [62720/72641]\tLoss: 0.6356\tLR: 0.000010\n",
      "Training Epoch: 13 [63040/72641]\tLoss: 0.6217\tLR: 0.000010\n",
      "Training Epoch: 13 [63360/72641]\tLoss: 0.6968\tLR: 0.000010\n",
      "Training Epoch: 13 [63680/72641]\tLoss: 0.6053\tLR: 0.000010\n",
      "Training Epoch: 13 [64000/72641]\tLoss: 0.6284\tLR: 0.000010\n",
      "Training Epoch: 13 [64320/72641]\tLoss: 0.6097\tLR: 0.000010\n",
      "Training Epoch: 13 [64640/72641]\tLoss: 0.6702\tLR: 0.000010\n",
      "Training Epoch: 13 [64960/72641]\tLoss: 0.6073\tLR: 0.000010\n",
      "Training Epoch: 13 [65280/72641]\tLoss: 0.6719\tLR: 0.000010\n",
      "Training Epoch: 13 [65600/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 13 [65920/72641]\tLoss: 0.6858\tLR: 0.000010\n",
      "Training Epoch: 13 [66240/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 13 [66560/72641]\tLoss: 0.6450\tLR: 0.000010\n",
      "Training Epoch: 13 [66880/72641]\tLoss: 0.6278\tLR: 0.000010\n",
      "Training Epoch: 13 [67200/72641]\tLoss: 0.6227\tLR: 0.000010\n",
      "Training Epoch: 13 [67520/72641]\tLoss: 0.6866\tLR: 0.000010\n",
      "Training Epoch: 13 [67840/72641]\tLoss: 0.6660\tLR: 0.000010\n",
      "Training Epoch: 13 [68160/72641]\tLoss: 0.6179\tLR: 0.000010\n",
      "Training Epoch: 13 [68480/72641]\tLoss: 0.6563\tLR: 0.000010\n",
      "Training Epoch: 13 [68800/72641]\tLoss: 0.6070\tLR: 0.000010\n",
      "Training Epoch: 13 [69120/72641]\tLoss: 0.6494\tLR: 0.000010\n",
      "Training Epoch: 13 [69440/72641]\tLoss: 0.6202\tLR: 0.000010\n",
      "Training Epoch: 13 [69760/72641]\tLoss: 0.6421\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [70080/72641]\tLoss: 0.6292\tLR: 0.000010\n",
      "Training Epoch: 13 [70400/72641]\tLoss: 0.6178\tLR: 0.000010\n",
      "Training Epoch: 13 [70720/72641]\tLoss: 0.6234\tLR: 0.000010\n",
      "Training Epoch: 13 [71040/72641]\tLoss: 0.6787\tLR: 0.000010\n",
      "Training Epoch: 13 [71360/72641]\tLoss: 0.6976\tLR: 0.000010\n",
      "Training Epoch: 13 [71680/72641]\tLoss: 0.6890\tLR: 0.000010\n",
      "Training Epoch: 13 [72000/72641]\tLoss: 0.6548\tLR: 0.000010\n",
      "Training Epoch: 13 [72320/72641]\tLoss: 0.6868\tLR: 0.000010\n",
      "Training Epoch: 13 [72640/72641]\tLoss: 0.6009\tLR: 0.000010\n",
      "Val Result: Acc: 0.1484, C_ACC: 0.6975, DOA: 88.1099, ACC_k: 0.1053\n",
      "ext:0.0, cls:0.580121, coar:0.0, fine:0.0,\n",
      "Training Epoch: 14 [320/72641]\tLoss: 0.6528\tLR: 0.000010\n",
      "Training Epoch: 14 [640/72641]\tLoss: 0.6142\tLR: 0.000010\n",
      "Training Epoch: 14 [960/72641]\tLoss: 0.6907\tLR: 0.000010\n",
      "Training Epoch: 14 [1280/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 14 [1600/72641]\tLoss: 0.6827\tLR: 0.000010\n",
      "Training Epoch: 14 [1920/72641]\tLoss: 0.6495\tLR: 0.000010\n",
      "Training Epoch: 14 [2240/72641]\tLoss: 0.6371\tLR: 0.000010\n",
      "Training Epoch: 14 [2560/72641]\tLoss: 0.6780\tLR: 0.000010\n",
      "Training Epoch: 14 [2880/72641]\tLoss: 0.6033\tLR: 0.000010\n",
      "Training Epoch: 14 [3200/72641]\tLoss: 0.6383\tLR: 0.000010\n",
      "Training Epoch: 14 [3520/72641]\tLoss: 0.6364\tLR: 0.000010\n",
      "Training Epoch: 14 [3840/72641]\tLoss: 0.6117\tLR: 0.000010\n",
      "Training Epoch: 14 [4160/72641]\tLoss: 0.6526\tLR: 0.000010\n",
      "Training Epoch: 14 [4480/72641]\tLoss: 0.6437\tLR: 0.000010\n",
      "Training Epoch: 14 [4800/72641]\tLoss: 0.6866\tLR: 0.000010\n",
      "Training Epoch: 14 [5120/72641]\tLoss: 0.6671\tLR: 0.000010\n",
      "Training Epoch: 14 [5440/72641]\tLoss: 0.6649\tLR: 0.000010\n",
      "Training Epoch: 14 [5760/72641]\tLoss: 0.6386\tLR: 0.000010\n",
      "Training Epoch: 14 [6080/72641]\tLoss: 0.6075\tLR: 0.000010\n",
      "Training Epoch: 14 [6400/72641]\tLoss: 0.6309\tLR: 0.000010\n",
      "Training Epoch: 14 [6720/72641]\tLoss: 0.6208\tLR: 0.000010\n",
      "Training Epoch: 14 [7040/72641]\tLoss: 0.6261\tLR: 0.000010\n",
      "Training Epoch: 14 [7360/72641]\tLoss: 0.6028\tLR: 0.000010\n",
      "Training Epoch: 14 [7680/72641]\tLoss: 0.6336\tLR: 0.000010\n",
      "Training Epoch: 14 [8000/72641]\tLoss: 0.6868\tLR: 0.000010\n",
      "Training Epoch: 14 [8320/72641]\tLoss: 0.6720\tLR: 0.000010\n",
      "Training Epoch: 14 [8640/72641]\tLoss: 0.6799\tLR: 0.000010\n",
      "Training Epoch: 14 [8960/72641]\tLoss: 0.6422\tLR: 0.000010\n",
      "Training Epoch: 14 [9280/72641]\tLoss: 0.6792\tLR: 0.000010\n",
      "Training Epoch: 14 [9600/72641]\tLoss: 0.6907\tLR: 0.000010\n",
      "Training Epoch: 14 [9920/72641]\tLoss: 0.6385\tLR: 0.000010\n",
      "Training Epoch: 14 [10240/72641]\tLoss: 0.7179\tLR: 0.000010\n",
      "Training Epoch: 14 [10560/72641]\tLoss: 0.6580\tLR: 0.000010\n",
      "Training Epoch: 14 [10880/72641]\tLoss: 0.6516\tLR: 0.000010\n",
      "Training Epoch: 14 [11200/72641]\tLoss: 0.6551\tLR: 0.000010\n",
      "Training Epoch: 14 [11520/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 14 [11840/72641]\tLoss: 0.6255\tLR: 0.000010\n",
      "Training Epoch: 14 [12160/72641]\tLoss: 0.6553\tLR: 0.000010\n",
      "Training Epoch: 14 [12480/72641]\tLoss: 0.6364\tLR: 0.000010\n",
      "Training Epoch: 14 [12800/72641]\tLoss: 0.6310\tLR: 0.000010\n",
      "Training Epoch: 14 [13120/72641]\tLoss: 0.6678\tLR: 0.000010\n",
      "Training Epoch: 14 [13440/72641]\tLoss: 0.6965\tLR: 0.000010\n",
      "Training Epoch: 14 [13760/72641]\tLoss: 0.7057\tLR: 0.000010\n",
      "Training Epoch: 14 [14080/72641]\tLoss: 0.6862\tLR: 0.000010\n",
      "Training Epoch: 14 [14400/72641]\tLoss: 0.6412\tLR: 0.000010\n",
      "Training Epoch: 14 [14720/72641]\tLoss: 0.6700\tLR: 0.000010\n",
      "Training Epoch: 14 [15040/72641]\tLoss: 0.6359\tLR: 0.000010\n",
      "Training Epoch: 14 [15360/72641]\tLoss: 0.6504\tLR: 0.000010\n",
      "Training Epoch: 14 [15680/72641]\tLoss: 0.6322\tLR: 0.000010\n",
      "Training Epoch: 14 [16000/72641]\tLoss: 0.6357\tLR: 0.000010\n",
      "Training Epoch: 14 [16320/72641]\tLoss: 0.6720\tLR: 0.000010\n",
      "Training Epoch: 14 [16640/72641]\tLoss: 0.6424\tLR: 0.000010\n",
      "Training Epoch: 14 [16960/72641]\tLoss: 0.6688\tLR: 0.000010\n",
      "Training Epoch: 14 [17280/72641]\tLoss: 0.5978\tLR: 0.000010\n",
      "Training Epoch: 14 [17600/72641]\tLoss: 0.6330\tLR: 0.000010\n",
      "Training Epoch: 14 [17920/72641]\tLoss: 0.6531\tLR: 0.000010\n",
      "Training Epoch: 14 [18240/72641]\tLoss: 0.6272\tLR: 0.000010\n",
      "Training Epoch: 14 [18560/72641]\tLoss: 0.6692\tLR: 0.000010\n",
      "Training Epoch: 14 [18880/72641]\tLoss: 0.6844\tLR: 0.000010\n",
      "Training Epoch: 14 [19200/72641]\tLoss: 0.7019\tLR: 0.000010\n",
      "Training Epoch: 14 [19520/72641]\tLoss: 0.5830\tLR: 0.000010\n",
      "Training Epoch: 14 [19840/72641]\tLoss: 0.6219\tLR: 0.000010\n",
      "Training Epoch: 14 [20160/72641]\tLoss: 0.6165\tLR: 0.000010\n",
      "Training Epoch: 14 [20480/72641]\tLoss: 0.6603\tLR: 0.000010\n",
      "Training Epoch: 14 [20800/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 14 [21120/72641]\tLoss: 0.6566\tLR: 0.000010\n",
      "Training Epoch: 14 [21440/72641]\tLoss: 0.6348\tLR: 0.000010\n",
      "Training Epoch: 14 [21760/72641]\tLoss: 0.6589\tLR: 0.000010\n",
      "Training Epoch: 14 [22080/72641]\tLoss: 0.6466\tLR: 0.000010\n",
      "Training Epoch: 14 [22400/72641]\tLoss: 0.6193\tLR: 0.000010\n",
      "Training Epoch: 14 [22720/72641]\tLoss: 0.6027\tLR: 0.000010\n",
      "Training Epoch: 14 [23040/72641]\tLoss: 0.6478\tLR: 0.000010\n",
      "Training Epoch: 14 [23360/72641]\tLoss: 0.6446\tLR: 0.000010\n",
      "Training Epoch: 14 [23680/72641]\tLoss: 0.6718\tLR: 0.000010\n",
      "Training Epoch: 14 [24000/72641]\tLoss: 0.6585\tLR: 0.000010\n",
      "Training Epoch: 14 [24320/72641]\tLoss: 0.6499\tLR: 0.000010\n",
      "Training Epoch: 14 [24640/72641]\tLoss: 0.6724\tLR: 0.000010\n",
      "Training Epoch: 14 [24960/72641]\tLoss: 0.6737\tLR: 0.000010\n",
      "Training Epoch: 14 [25280/72641]\tLoss: 0.6327\tLR: 0.000010\n",
      "Training Epoch: 14 [25600/72641]\tLoss: 0.6550\tLR: 0.000010\n",
      "Training Epoch: 14 [25920/72641]\tLoss: 0.6344\tLR: 0.000010\n",
      "Training Epoch: 14 [26240/72641]\tLoss: 0.6326\tLR: 0.000010\n",
      "Training Epoch: 14 [26560/72641]\tLoss: 0.6226\tLR: 0.000010\n",
      "Training Epoch: 14 [26880/72641]\tLoss: 0.6143\tLR: 0.000010\n",
      "Training Epoch: 14 [27200/72641]\tLoss: 0.6301\tLR: 0.000010\n",
      "Training Epoch: 14 [27520/72641]\tLoss: 0.6858\tLR: 0.000010\n",
      "Training Epoch: 14 [27840/72641]\tLoss: 0.6698\tLR: 0.000010\n",
      "Training Epoch: 14 [28160/72641]\tLoss: 0.6979\tLR: 0.000010\n",
      "Training Epoch: 14 [28480/72641]\tLoss: 0.5973\tLR: 0.000010\n",
      "Training Epoch: 14 [28800/72641]\tLoss: 0.6212\tLR: 0.000010\n",
      "Training Epoch: 14 [29120/72641]\tLoss: 0.6925\tLR: 0.000010\n",
      "Training Epoch: 14 [29440/72641]\tLoss: 0.6002\tLR: 0.000010\n",
      "Training Epoch: 14 [29760/72641]\tLoss: 0.6477\tLR: 0.000010\n",
      "Training Epoch: 14 [30080/72641]\tLoss: 0.6490\tLR: 0.000010\n",
      "Training Epoch: 14 [30400/72641]\tLoss: 0.6347\tLR: 0.000010\n",
      "Training Epoch: 14 [30720/72641]\tLoss: 0.6433\tLR: 0.000010\n",
      "Training Epoch: 14 [31040/72641]\tLoss: 0.6435\tLR: 0.000010\n",
      "Training Epoch: 14 [31360/72641]\tLoss: 0.6180\tLR: 0.000010\n",
      "Training Epoch: 14 [31680/72641]\tLoss: 0.6479\tLR: 0.000010\n",
      "Training Epoch: 14 [32000/72641]\tLoss: 0.6480\tLR: 0.000010\n",
      "Training Epoch: 14 [32320/72641]\tLoss: 0.6475\tLR: 0.000010\n",
      "Training Epoch: 14 [32640/72641]\tLoss: 0.6111\tLR: 0.000010\n",
      "Training Epoch: 14 [32960/72641]\tLoss: 0.6596\tLR: 0.000010\n",
      "Training Epoch: 14 [33280/72641]\tLoss: 0.6423\tLR: 0.000010\n",
      "Training Epoch: 14 [33600/72641]\tLoss: 0.5889\tLR: 0.000010\n",
      "Training Epoch: 14 [33920/72641]\tLoss: 0.6738\tLR: 0.000010\n",
      "Training Epoch: 14 [34240/72641]\tLoss: 0.6126\tLR: 0.000010\n",
      "Training Epoch: 14 [34560/72641]\tLoss: 0.6761\tLR: 0.000010\n",
      "Training Epoch: 14 [34880/72641]\tLoss: 0.6493\tLR: 0.000010\n",
      "Training Epoch: 14 [35200/72641]\tLoss: 0.5978\tLR: 0.000010\n",
      "Training Epoch: 14 [35520/72641]\tLoss: 0.6391\tLR: 0.000010\n",
      "Training Epoch: 14 [35840/72641]\tLoss: 0.6930\tLR: 0.000010\n",
      "Training Epoch: 14 [36160/72641]\tLoss: 0.6701\tLR: 0.000010\n",
      "Training Epoch: 14 [36480/72641]\tLoss: 0.5989\tLR: 0.000010\n",
      "Training Epoch: 14 [36800/72641]\tLoss: 0.6593\tLR: 0.000010\n",
      "Training Epoch: 14 [37120/72641]\tLoss: 0.6654\tLR: 0.000010\n",
      "Training Epoch: 14 [37440/72641]\tLoss: 0.6285\tLR: 0.000010\n",
      "Training Epoch: 14 [37760/72641]\tLoss: 0.6465\tLR: 0.000010\n",
      "Training Epoch: 14 [38080/72641]\tLoss: 0.6427\tLR: 0.000010\n",
      "Training Epoch: 14 [38400/72641]\tLoss: 0.6499\tLR: 0.000010\n",
      "Training Epoch: 14 [38720/72641]\tLoss: 0.6157\tLR: 0.000010\n",
      "Training Epoch: 14 [39040/72641]\tLoss: 0.6570\tLR: 0.000010\n",
      "Training Epoch: 14 [39360/72641]\tLoss: 0.6307\tLR: 0.000010\n",
      "Training Epoch: 14 [39680/72641]\tLoss: 0.6340\tLR: 0.000010\n",
      "Training Epoch: 14 [40000/72641]\tLoss: 0.6632\tLR: 0.000010\n",
      "Training Epoch: 14 [40320/72641]\tLoss: 0.6324\tLR: 0.000010\n",
      "Training Epoch: 14 [40640/72641]\tLoss: 0.6264\tLR: 0.000010\n",
      "Training Epoch: 14 [40960/72641]\tLoss: 0.6127\tLR: 0.000010\n",
      "Training Epoch: 14 [41280/72641]\tLoss: 0.6549\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [41600/72641]\tLoss: 0.6405\tLR: 0.000010\n",
      "Training Epoch: 14 [41920/72641]\tLoss: 0.6594\tLR: 0.000010\n",
      "Training Epoch: 14 [42240/72641]\tLoss: 0.6263\tLR: 0.000010\n",
      "Training Epoch: 14 [42560/72641]\tLoss: 0.6533\tLR: 0.000010\n",
      "Training Epoch: 14 [42880/72641]\tLoss: 0.6844\tLR: 0.000010\n",
      "Training Epoch: 14 [43200/72641]\tLoss: 0.6265\tLR: 0.000010\n",
      "Training Epoch: 14 [43520/72641]\tLoss: 0.6699\tLR: 0.000010\n",
      "Training Epoch: 14 [43840/72641]\tLoss: 0.6339\tLR: 0.000010\n",
      "Training Epoch: 14 [44160/72641]\tLoss: 0.6734\tLR: 0.000010\n",
      "Training Epoch: 14 [44480/72641]\tLoss: 0.6410\tLR: 0.000010\n",
      "Training Epoch: 14 [44800/72641]\tLoss: 0.7034\tLR: 0.000010\n",
      "Training Epoch: 14 [45120/72641]\tLoss: 0.6454\tLR: 0.000010\n",
      "Training Epoch: 14 [45440/72641]\tLoss: 0.6734\tLR: 0.000010\n",
      "Training Epoch: 14 [45760/72641]\tLoss: 0.6584\tLR: 0.000010\n",
      "Training Epoch: 14 [46080/72641]\tLoss: 0.6515\tLR: 0.000010\n",
      "Training Epoch: 14 [46400/72641]\tLoss: 0.6316\tLR: 0.000010\n",
      "Training Epoch: 14 [46720/72641]\tLoss: 0.6808\tLR: 0.000010\n",
      "Training Epoch: 14 [47040/72641]\tLoss: 0.6420\tLR: 0.000010\n",
      "Training Epoch: 14 [47360/72641]\tLoss: 0.6407\tLR: 0.000010\n",
      "Training Epoch: 14 [47680/72641]\tLoss: 0.6285\tLR: 0.000010\n",
      "Training Epoch: 14 [48000/72641]\tLoss: 0.6346\tLR: 0.000010\n",
      "Training Epoch: 14 [48320/72641]\tLoss: 0.6454\tLR: 0.000010\n",
      "Training Epoch: 14 [48640/72641]\tLoss: 0.6866\tLR: 0.000010\n",
      "Training Epoch: 14 [48960/72641]\tLoss: 0.6597\tLR: 0.000010\n",
      "Training Epoch: 14 [49280/72641]\tLoss: 0.6838\tLR: 0.000010\n",
      "Training Epoch: 14 [49600/72641]\tLoss: 0.6713\tLR: 0.000010\n",
      "Training Epoch: 14 [49920/72641]\tLoss: 0.6139\tLR: 0.000010\n",
      "Training Epoch: 14 [50240/72641]\tLoss: 0.6307\tLR: 0.000010\n",
      "Training Epoch: 14 [50560/72641]\tLoss: 0.6018\tLR: 0.000010\n",
      "Training Epoch: 14 [50880/72641]\tLoss: 0.6509\tLR: 0.000010\n",
      "Training Epoch: 14 [51200/72641]\tLoss: 0.6629\tLR: 0.000010\n",
      "Training Epoch: 14 [51520/72641]\tLoss: 0.6340\tLR: 0.000010\n",
      "Training Epoch: 14 [51840/72641]\tLoss: 0.6423\tLR: 0.000010\n",
      "Training Epoch: 14 [52160/72641]\tLoss: 0.6713\tLR: 0.000010\n",
      "Training Epoch: 14 [52480/72641]\tLoss: 0.6188\tLR: 0.000010\n",
      "Training Epoch: 14 [52800/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 14 [53120/72641]\tLoss: 0.6224\tLR: 0.000010\n",
      "Training Epoch: 14 [53440/72641]\tLoss: 0.6275\tLR: 0.000010\n",
      "Training Epoch: 14 [53760/72641]\tLoss: 0.6680\tLR: 0.000010\n",
      "Training Epoch: 14 [54080/72641]\tLoss: 0.6612\tLR: 0.000010\n",
      "Training Epoch: 14 [54400/72641]\tLoss: 0.6570\tLR: 0.000010\n",
      "Training Epoch: 14 [54720/72641]\tLoss: 0.6604\tLR: 0.000010\n",
      "Training Epoch: 14 [55040/72641]\tLoss: 0.6833\tLR: 0.000010\n",
      "Training Epoch: 14 [55360/72641]\tLoss: 0.6511\tLR: 0.000010\n",
      "Training Epoch: 14 [55680/72641]\tLoss: 0.6011\tLR: 0.000010\n",
      "Training Epoch: 14 [56000/72641]\tLoss: 0.6251\tLR: 0.000010\n",
      "Training Epoch: 14 [56320/72641]\tLoss: 0.6655\tLR: 0.000010\n",
      "Training Epoch: 14 [56640/72641]\tLoss: 0.6321\tLR: 0.000010\n",
      "Training Epoch: 14 [56960/72641]\tLoss: 0.6301\tLR: 0.000010\n",
      "Training Epoch: 14 [57280/72641]\tLoss: 0.6585\tLR: 0.000010\n",
      "Training Epoch: 14 [57600/72641]\tLoss: 0.6879\tLR: 0.000010\n",
      "Training Epoch: 14 [57920/72641]\tLoss: 0.6268\tLR: 0.000010\n",
      "Training Epoch: 14 [58240/72641]\tLoss: 0.6082\tLR: 0.000010\n",
      "Training Epoch: 14 [58560/72641]\tLoss: 0.6022\tLR: 0.000010\n",
      "Training Epoch: 14 [58880/72641]\tLoss: 0.6098\tLR: 0.000010\n",
      "Training Epoch: 14 [59200/72641]\tLoss: 0.6421\tLR: 0.000010\n",
      "Training Epoch: 14 [59520/72641]\tLoss: 0.6724\tLR: 0.000010\n",
      "Training Epoch: 14 [59840/72641]\tLoss: 0.6815\tLR: 0.000010\n",
      "Training Epoch: 14 [60160/72641]\tLoss: 0.6218\tLR: 0.000010\n",
      "Training Epoch: 14 [60480/72641]\tLoss: 0.6220\tLR: 0.000010\n",
      "Training Epoch: 14 [60800/72641]\tLoss: 0.6053\tLR: 0.000010\n",
      "Training Epoch: 14 [61120/72641]\tLoss: 0.5936\tLR: 0.000010\n",
      "Training Epoch: 14 [61440/72641]\tLoss: 0.6139\tLR: 0.000010\n",
      "Training Epoch: 14 [61760/72641]\tLoss: 0.6288\tLR: 0.000010\n",
      "Training Epoch: 14 [62080/72641]\tLoss: 0.6420\tLR: 0.000010\n",
      "Training Epoch: 14 [62400/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 14 [62720/72641]\tLoss: 0.6609\tLR: 0.000010\n",
      "Training Epoch: 14 [63040/72641]\tLoss: 0.6753\tLR: 0.000010\n",
      "Training Epoch: 14 [63360/72641]\tLoss: 0.6866\tLR: 0.000010\n",
      "Training Epoch: 14 [63680/72641]\tLoss: 0.6253\tLR: 0.000010\n",
      "Training Epoch: 14 [64000/72641]\tLoss: 0.6420\tLR: 0.000010\n",
      "Training Epoch: 14 [64320/72641]\tLoss: 0.6270\tLR: 0.000010\n",
      "Training Epoch: 14 [64640/72641]\tLoss: 0.6518\tLR: 0.000010\n",
      "Training Epoch: 14 [64960/72641]\tLoss: 0.6568\tLR: 0.000010\n",
      "Training Epoch: 14 [65280/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 14 [65600/72641]\tLoss: 0.6480\tLR: 0.000010\n",
      "Training Epoch: 14 [65920/72641]\tLoss: 0.6836\tLR: 0.000010\n",
      "Training Epoch: 14 [66240/72641]\tLoss: 0.6514\tLR: 0.000010\n",
      "Training Epoch: 14 [66560/72641]\tLoss: 0.6501\tLR: 0.000010\n",
      "Training Epoch: 14 [66880/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 14 [67200/72641]\tLoss: 0.5948\tLR: 0.000010\n",
      "Training Epoch: 14 [67520/72641]\tLoss: 0.6842\tLR: 0.000010\n",
      "Training Epoch: 14 [67840/72641]\tLoss: 0.6526\tLR: 0.000010\n",
      "Training Epoch: 14 [68160/72641]\tLoss: 0.6405\tLR: 0.000010\n",
      "Training Epoch: 14 [68480/72641]\tLoss: 0.6288\tLR: 0.000010\n",
      "Training Epoch: 14 [68800/72641]\tLoss: 0.6562\tLR: 0.000010\n",
      "Training Epoch: 14 [69120/72641]\tLoss: 0.6555\tLR: 0.000010\n",
      "Training Epoch: 14 [69440/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 14 [69760/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 14 [70080/72641]\tLoss: 0.6113\tLR: 0.000010\n",
      "Training Epoch: 14 [70400/72641]\tLoss: 0.6658\tLR: 0.000010\n",
      "Training Epoch: 14 [70720/72641]\tLoss: 0.6607\tLR: 0.000010\n",
      "Training Epoch: 14 [71040/72641]\tLoss: 0.6707\tLR: 0.000010\n",
      "Training Epoch: 14 [71360/72641]\tLoss: 0.6077\tLR: 0.000010\n",
      "Training Epoch: 14 [71680/72641]\tLoss: 0.6604\tLR: 0.000010\n",
      "Training Epoch: 14 [72000/72641]\tLoss: 0.6415\tLR: 0.000010\n",
      "Training Epoch: 14 [72320/72641]\tLoss: 0.6308\tLR: 0.000010\n",
      "Training Epoch: 14 [72640/72641]\tLoss: 0.6258\tLR: 0.000010\n",
      "Val Result: Acc: 0.1426, C_ACC: 0.7043, DOA: 88.2802, ACC_k: 0.1032\n",
      "ext:0.0, cls:0.565598, coar:0.0, fine:0.0,\n",
      "Training Epoch: 15 [320/72641]\tLoss: 0.6565\tLR: 0.000010\n",
      "Training Epoch: 15 [640/72641]\tLoss: 0.6630\tLR: 0.000010\n",
      "Training Epoch: 15 [960/72641]\tLoss: 0.6534\tLR: 0.000010\n",
      "Training Epoch: 15 [1280/72641]\tLoss: 0.6429\tLR: 0.000010\n",
      "Training Epoch: 15 [1600/72641]\tLoss: 0.7096\tLR: 0.000010\n",
      "Training Epoch: 15 [1920/72641]\tLoss: 0.6394\tLR: 0.000010\n",
      "Training Epoch: 15 [2240/72641]\tLoss: 0.6379\tLR: 0.000010\n",
      "Training Epoch: 15 [2560/72641]\tLoss: 0.6369\tLR: 0.000010\n",
      "Training Epoch: 15 [2880/72641]\tLoss: 0.6618\tLR: 0.000010\n",
      "Training Epoch: 15 [3200/72641]\tLoss: 0.6809\tLR: 0.000010\n",
      "Training Epoch: 15 [3520/72641]\tLoss: 0.6704\tLR: 0.000010\n",
      "Training Epoch: 15 [3840/72641]\tLoss: 0.6294\tLR: 0.000010\n",
      "Training Epoch: 15 [4160/72641]\tLoss: 0.6745\tLR: 0.000010\n",
      "Training Epoch: 15 [4480/72641]\tLoss: 0.6416\tLR: 0.000010\n",
      "Training Epoch: 15 [4800/72641]\tLoss: 0.6119\tLR: 0.000010\n",
      "Training Epoch: 15 [5120/72641]\tLoss: 0.6270\tLR: 0.000010\n",
      "Training Epoch: 15 [5440/72641]\tLoss: 0.6034\tLR: 0.000010\n",
      "Training Epoch: 15 [5760/72641]\tLoss: 0.6314\tLR: 0.000010\n",
      "Training Epoch: 15 [6080/72641]\tLoss: 0.6219\tLR: 0.000010\n",
      "Training Epoch: 15 [6400/72641]\tLoss: 0.6344\tLR: 0.000010\n",
      "Training Epoch: 15 [6720/72641]\tLoss: 0.6067\tLR: 0.000010\n",
      "Training Epoch: 15 [7040/72641]\tLoss: 0.6489\tLR: 0.000010\n",
      "Training Epoch: 15 [7360/72641]\tLoss: 0.6548\tLR: 0.000010\n",
      "Training Epoch: 15 [7680/72641]\tLoss: 0.6072\tLR: 0.000010\n",
      "Training Epoch: 15 [8000/72641]\tLoss: 0.6352\tLR: 0.000010\n",
      "Training Epoch: 15 [8320/72641]\tLoss: 0.6961\tLR: 0.000010\n",
      "Training Epoch: 15 [8640/72641]\tLoss: 0.6089\tLR: 0.000010\n",
      "Training Epoch: 15 [8960/72641]\tLoss: 0.6310\tLR: 0.000010\n",
      "Training Epoch: 15 [9280/72641]\tLoss: 0.6457\tLR: 0.000010\n",
      "Training Epoch: 15 [9600/72641]\tLoss: 0.6901\tLR: 0.000010\n",
      "Training Epoch: 15 [9920/72641]\tLoss: 0.6286\tLR: 0.000010\n",
      "Training Epoch: 15 [10240/72641]\tLoss: 0.6565\tLR: 0.000010\n",
      "Training Epoch: 15 [10560/72641]\tLoss: 0.6528\tLR: 0.000010\n",
      "Training Epoch: 15 [10880/72641]\tLoss: 0.6091\tLR: 0.000010\n",
      "Training Epoch: 15 [11200/72641]\tLoss: 0.6358\tLR: 0.000010\n",
      "Training Epoch: 15 [11520/72641]\tLoss: 0.6677\tLR: 0.000010\n",
      "Training Epoch: 15 [11840/72641]\tLoss: 0.5906\tLR: 0.000010\n",
      "Training Epoch: 15 [12160/72641]\tLoss: 0.6374\tLR: 0.000010\n",
      "Training Epoch: 15 [12480/72641]\tLoss: 0.6697\tLR: 0.000010\n",
      "Training Epoch: 15 [12800/72641]\tLoss: 0.6357\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [13120/72641]\tLoss: 0.6001\tLR: 0.000010\n",
      "Training Epoch: 15 [13440/72641]\tLoss: 0.6525\tLR: 0.000010\n",
      "Training Epoch: 15 [13760/72641]\tLoss: 0.6381\tLR: 0.000010\n",
      "Training Epoch: 15 [14080/72641]\tLoss: 0.6070\tLR: 0.000010\n",
      "Training Epoch: 15 [14400/72641]\tLoss: 0.6433\tLR: 0.000010\n",
      "Training Epoch: 15 [14720/72641]\tLoss: 0.6103\tLR: 0.000010\n",
      "Training Epoch: 15 [15040/72641]\tLoss: 0.5960\tLR: 0.000010\n",
      "Training Epoch: 15 [15360/72641]\tLoss: 0.6363\tLR: 0.000010\n",
      "Training Epoch: 15 [15680/72641]\tLoss: 0.6482\tLR: 0.000010\n",
      "Training Epoch: 15 [16000/72641]\tLoss: 0.6952\tLR: 0.000010\n",
      "Training Epoch: 15 [16320/72641]\tLoss: 0.7295\tLR: 0.000010\n",
      "Training Epoch: 15 [16640/72641]\tLoss: 0.6810\tLR: 0.000010\n",
      "Training Epoch: 15 [16960/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 15 [17280/72641]\tLoss: 0.6526\tLR: 0.000010\n",
      "Training Epoch: 15 [17600/72641]\tLoss: 0.5973\tLR: 0.000010\n",
      "Training Epoch: 15 [17920/72641]\tLoss: 0.6183\tLR: 0.000010\n",
      "Training Epoch: 15 [18240/72641]\tLoss: 0.6674\tLR: 0.000010\n",
      "Training Epoch: 15 [18560/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 15 [18880/72641]\tLoss: 0.6526\tLR: 0.000010\n",
      "Training Epoch: 15 [19200/72641]\tLoss: 0.6832\tLR: 0.000010\n",
      "Training Epoch: 15 [19520/72641]\tLoss: 0.6117\tLR: 0.000010\n",
      "Training Epoch: 15 [19840/72641]\tLoss: 0.6142\tLR: 0.000010\n",
      "Training Epoch: 15 [20160/72641]\tLoss: 0.6383\tLR: 0.000010\n",
      "Training Epoch: 15 [20480/72641]\tLoss: 0.6581\tLR: 0.000010\n",
      "Training Epoch: 15 [20800/72641]\tLoss: 0.7125\tLR: 0.000010\n",
      "Training Epoch: 15 [21120/72641]\tLoss: 0.6150\tLR: 0.000010\n",
      "Training Epoch: 15 [21440/72641]\tLoss: 0.6709\tLR: 0.000010\n",
      "Training Epoch: 15 [21760/72641]\tLoss: 0.6706\tLR: 0.000010\n",
      "Training Epoch: 15 [22080/72641]\tLoss: 0.6929\tLR: 0.000010\n",
      "Training Epoch: 15 [22400/72641]\tLoss: 0.6018\tLR: 0.000010\n",
      "Training Epoch: 15 [22720/72641]\tLoss: 0.6296\tLR: 0.000010\n",
      "Training Epoch: 15 [23040/72641]\tLoss: 0.6609\tLR: 0.000010\n",
      "Training Epoch: 15 [23360/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 15 [23680/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 15 [24000/72641]\tLoss: 0.6055\tLR: 0.000010\n",
      "Training Epoch: 15 [24320/72641]\tLoss: 0.6177\tLR: 0.000010\n",
      "Training Epoch: 15 [24640/72641]\tLoss: 0.6668\tLR: 0.000010\n",
      "Training Epoch: 15 [24960/72641]\tLoss: 0.6768\tLR: 0.000010\n",
      "Training Epoch: 15 [25280/72641]\tLoss: 0.6666\tLR: 0.000010\n",
      "Training Epoch: 15 [25600/72641]\tLoss: 0.6258\tLR: 0.000010\n",
      "Training Epoch: 15 [25920/72641]\tLoss: 0.6397\tLR: 0.000010\n",
      "Training Epoch: 15 [26240/72641]\tLoss: 0.6317\tLR: 0.000010\n",
      "Training Epoch: 15 [26560/72641]\tLoss: 0.6502\tLR: 0.000010\n",
      "Training Epoch: 15 [26880/72641]\tLoss: 0.6388\tLR: 0.000010\n",
      "Training Epoch: 15 [27200/72641]\tLoss: 0.6399\tLR: 0.000010\n",
      "Training Epoch: 15 [27520/72641]\tLoss: 0.6632\tLR: 0.000010\n",
      "Training Epoch: 15 [27840/72641]\tLoss: 0.6451\tLR: 0.000010\n",
      "Training Epoch: 15 [28160/72641]\tLoss: 0.6550\tLR: 0.000010\n",
      "Training Epoch: 15 [28480/72641]\tLoss: 0.6420\tLR: 0.000010\n",
      "Training Epoch: 15 [28800/72641]\tLoss: 0.6392\tLR: 0.000010\n",
      "Training Epoch: 15 [29120/72641]\tLoss: 0.6494\tLR: 0.000010\n",
      "Training Epoch: 15 [29440/72641]\tLoss: 0.6215\tLR: 0.000010\n",
      "Training Epoch: 15 [29760/72641]\tLoss: 0.6229\tLR: 0.000010\n",
      "Training Epoch: 15 [30080/72641]\tLoss: 0.6735\tLR: 0.000010\n",
      "Training Epoch: 15 [30400/72641]\tLoss: 0.6607\tLR: 0.000010\n",
      "Training Epoch: 15 [30720/72641]\tLoss: 0.6224\tLR: 0.000010\n",
      "Training Epoch: 15 [31040/72641]\tLoss: 0.6547\tLR: 0.000010\n",
      "Training Epoch: 15 [31360/72641]\tLoss: 0.6386\tLR: 0.000010\n",
      "Training Epoch: 15 [31680/72641]\tLoss: 0.6878\tLR: 0.000010\n",
      "Training Epoch: 15 [32000/72641]\tLoss: 0.6347\tLR: 0.000010\n",
      "Training Epoch: 15 [32320/72641]\tLoss: 0.6701\tLR: 0.000010\n",
      "Training Epoch: 15 [32640/72641]\tLoss: 0.6325\tLR: 0.000010\n",
      "Training Epoch: 15 [32960/72641]\tLoss: 0.7083\tLR: 0.000010\n",
      "Training Epoch: 15 [33280/72641]\tLoss: 0.6531\tLR: 0.000010\n",
      "Training Epoch: 15 [33600/72641]\tLoss: 0.6155\tLR: 0.000010\n",
      "Training Epoch: 15 [33920/72641]\tLoss: 0.6151\tLR: 0.000010\n",
      "Training Epoch: 15 [34240/72641]\tLoss: 0.5965\tLR: 0.000010\n",
      "Training Epoch: 15 [34560/72641]\tLoss: 0.6846\tLR: 0.000010\n",
      "Training Epoch: 15 [34880/72641]\tLoss: 0.6163\tLR: 0.000010\n",
      "Training Epoch: 15 [35200/72641]\tLoss: 0.6082\tLR: 0.000010\n",
      "Training Epoch: 15 [35520/72641]\tLoss: 0.6124\tLR: 0.000010\n",
      "Training Epoch: 15 [35840/72641]\tLoss: 0.6542\tLR: 0.000010\n",
      "Training Epoch: 15 [36160/72641]\tLoss: 0.6701\tLR: 0.000010\n",
      "Training Epoch: 15 [36480/72641]\tLoss: 0.6534\tLR: 0.000010\n",
      "Training Epoch: 15 [36800/72641]\tLoss: 0.6375\tLR: 0.000010\n",
      "Training Epoch: 15 [37120/72641]\tLoss: 0.6709\tLR: 0.000010\n",
      "Training Epoch: 15 [37440/72641]\tLoss: 0.6781\tLR: 0.000010\n",
      "Training Epoch: 15 [37760/72641]\tLoss: 0.6350\tLR: 0.000010\n",
      "Training Epoch: 15 [38080/72641]\tLoss: 0.6360\tLR: 0.000010\n",
      "Training Epoch: 15 [38400/72641]\tLoss: 0.7052\tLR: 0.000010\n",
      "Training Epoch: 15 [38720/72641]\tLoss: 0.6618\tLR: 0.000010\n",
      "Training Epoch: 15 [39040/72641]\tLoss: 0.6983\tLR: 0.000010\n",
      "Training Epoch: 15 [39360/72641]\tLoss: 0.6235\tLR: 0.000010\n",
      "Training Epoch: 15 [39680/72641]\tLoss: 0.6583\tLR: 0.000010\n",
      "Training Epoch: 15 [40000/72641]\tLoss: 0.6375\tLR: 0.000010\n",
      "Training Epoch: 15 [40320/72641]\tLoss: 0.6036\tLR: 0.000010\n",
      "Training Epoch: 15 [40640/72641]\tLoss: 0.6391\tLR: 0.000010\n",
      "Training Epoch: 15 [40960/72641]\tLoss: 0.6176\tLR: 0.000010\n",
      "Training Epoch: 15 [41280/72641]\tLoss: 0.6827\tLR: 0.000010\n",
      "Training Epoch: 15 [41600/72641]\tLoss: 0.6259\tLR: 0.000010\n",
      "Training Epoch: 15 [41920/72641]\tLoss: 0.6724\tLR: 0.000010\n",
      "Training Epoch: 15 [42240/72641]\tLoss: 0.5787\tLR: 0.000010\n",
      "Training Epoch: 15 [42560/72641]\tLoss: 0.6508\tLR: 0.000010\n",
      "Training Epoch: 15 [42880/72641]\tLoss: 0.6331\tLR: 0.000010\n",
      "Training Epoch: 15 [43200/72641]\tLoss: 0.6419\tLR: 0.000010\n",
      "Training Epoch: 15 [43520/72641]\tLoss: 0.6378\tLR: 0.000010\n",
      "Training Epoch: 15 [43840/72641]\tLoss: 0.6813\tLR: 0.000010\n",
      "Training Epoch: 15 [44160/72641]\tLoss: 0.6407\tLR: 0.000010\n",
      "Training Epoch: 15 [44480/72641]\tLoss: 0.6283\tLR: 0.000010\n",
      "Training Epoch: 15 [44800/72641]\tLoss: 0.6263\tLR: 0.000010\n",
      "Training Epoch: 15 [45120/72641]\tLoss: 0.6135\tLR: 0.000010\n",
      "Training Epoch: 15 [45440/72641]\tLoss: 0.6563\tLR: 0.000010\n",
      "Training Epoch: 15 [45760/72641]\tLoss: 0.7020\tLR: 0.000010\n",
      "Training Epoch: 15 [46080/72641]\tLoss: 0.6582\tLR: 0.000010\n",
      "Training Epoch: 15 [46400/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 15 [46720/72641]\tLoss: 0.6461\tLR: 0.000010\n",
      "Training Epoch: 15 [47040/72641]\tLoss: 0.6247\tLR: 0.000010\n",
      "Training Epoch: 15 [47360/72641]\tLoss: 0.6674\tLR: 0.000010\n",
      "Training Epoch: 15 [47680/72641]\tLoss: 0.6269\tLR: 0.000010\n",
      "Training Epoch: 15 [48000/72641]\tLoss: 0.6287\tLR: 0.000010\n",
      "Training Epoch: 15 [48320/72641]\tLoss: 0.6824\tLR: 0.000010\n",
      "Training Epoch: 15 [48640/72641]\tLoss: 0.6337\tLR: 0.000010\n",
      "Training Epoch: 15 [48960/72641]\tLoss: 0.6328\tLR: 0.000010\n",
      "Training Epoch: 15 [49280/72641]\tLoss: 0.6152\tLR: 0.000010\n",
      "Training Epoch: 15 [49600/72641]\tLoss: 0.6879\tLR: 0.000010\n",
      "Training Epoch: 15 [49920/72641]\tLoss: 0.6264\tLR: 0.000010\n",
      "Training Epoch: 15 [50240/72641]\tLoss: 0.6070\tLR: 0.000010\n",
      "Training Epoch: 15 [50560/72641]\tLoss: 0.6314\tLR: 0.000010\n",
      "Training Epoch: 15 [50880/72641]\tLoss: 0.6364\tLR: 0.000010\n",
      "Training Epoch: 15 [51200/72641]\tLoss: 0.6821\tLR: 0.000010\n",
      "Training Epoch: 15 [51520/72641]\tLoss: 0.6360\tLR: 0.000010\n",
      "Training Epoch: 15 [51840/72641]\tLoss: 0.6030\tLR: 0.000010\n",
      "Training Epoch: 15 [52160/72641]\tLoss: 0.6211\tLR: 0.000010\n",
      "Training Epoch: 15 [52480/72641]\tLoss: 0.6367\tLR: 0.000010\n",
      "Training Epoch: 15 [52800/72641]\tLoss: 0.6810\tLR: 0.000010\n",
      "Training Epoch: 15 [53120/72641]\tLoss: 0.6050\tLR: 0.000010\n",
      "Training Epoch: 15 [53440/72641]\tLoss: 0.6539\tLR: 0.000010\n",
      "Training Epoch: 15 [53760/72641]\tLoss: 0.6929\tLR: 0.000010\n",
      "Training Epoch: 15 [54080/72641]\tLoss: 0.6494\tLR: 0.000010\n",
      "Training Epoch: 15 [54400/72641]\tLoss: 0.6450\tLR: 0.000010\n",
      "Training Epoch: 15 [54720/72641]\tLoss: 0.6279\tLR: 0.000010\n",
      "Training Epoch: 15 [55040/72641]\tLoss: 0.6747\tLR: 0.000010\n",
      "Training Epoch: 15 [55360/72641]\tLoss: 0.6419\tLR: 0.000010\n",
      "Training Epoch: 15 [55680/72641]\tLoss: 0.6027\tLR: 0.000010\n",
      "Training Epoch: 15 [56000/72641]\tLoss: 0.6247\tLR: 0.000010\n",
      "Training Epoch: 15 [56320/72641]\tLoss: 0.6845\tLR: 0.000010\n",
      "Training Epoch: 15 [56640/72641]\tLoss: 0.6478\tLR: 0.000010\n",
      "Training Epoch: 15 [56960/72641]\tLoss: 0.6580\tLR: 0.000010\n",
      "Training Epoch: 15 [57280/72641]\tLoss: 0.6003\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [57600/72641]\tLoss: 0.6702\tLR: 0.000010\n",
      "Training Epoch: 15 [57920/72641]\tLoss: 0.7053\tLR: 0.000010\n",
      "Training Epoch: 15 [58240/72641]\tLoss: 0.6537\tLR: 0.000010\n",
      "Training Epoch: 15 [58560/72641]\tLoss: 0.6086\tLR: 0.000010\n",
      "Training Epoch: 15 [58880/72641]\tLoss: 0.6187\tLR: 0.000010\n",
      "Training Epoch: 15 [59200/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 15 [59520/72641]\tLoss: 0.6442\tLR: 0.000010\n",
      "Training Epoch: 15 [59840/72641]\tLoss: 0.6794\tLR: 0.000010\n",
      "Training Epoch: 15 [60160/72641]\tLoss: 0.6481\tLR: 0.000010\n",
      "Training Epoch: 15 [60480/72641]\tLoss: 0.6620\tLR: 0.000010\n",
      "Training Epoch: 15 [60800/72641]\tLoss: 0.6458\tLR: 0.000010\n",
      "Training Epoch: 15 [61120/72641]\tLoss: 0.6344\tLR: 0.000010\n",
      "Training Epoch: 15 [61440/72641]\tLoss: 0.6042\tLR: 0.000010\n",
      "Training Epoch: 15 [61760/72641]\tLoss: 0.5991\tLR: 0.000010\n",
      "Training Epoch: 15 [62080/72641]\tLoss: 0.6870\tLR: 0.000010\n",
      "Training Epoch: 15 [62400/72641]\tLoss: 0.6360\tLR: 0.000010\n",
      "Training Epoch: 15 [62720/72641]\tLoss: 0.6433\tLR: 0.000010\n",
      "Training Epoch: 15 [63040/72641]\tLoss: 0.6373\tLR: 0.000010\n",
      "Training Epoch: 15 [63360/72641]\tLoss: 0.7125\tLR: 0.000010\n",
      "Training Epoch: 15 [63680/72641]\tLoss: 0.6696\tLR: 0.000010\n",
      "Training Epoch: 15 [64000/72641]\tLoss: 0.6183\tLR: 0.000010\n",
      "Training Epoch: 15 [64320/72641]\tLoss: 0.5908\tLR: 0.000010\n",
      "Training Epoch: 15 [64640/72641]\tLoss: 0.6220\tLR: 0.000010\n",
      "Training Epoch: 15 [64960/72641]\tLoss: 0.6392\tLR: 0.000010\n",
      "Training Epoch: 15 [65280/72641]\tLoss: 0.6286\tLR: 0.000010\n",
      "Training Epoch: 15 [65600/72641]\tLoss: 0.6415\tLR: 0.000010\n",
      "Training Epoch: 15 [65920/72641]\tLoss: 0.6756\tLR: 0.000010\n",
      "Training Epoch: 15 [66240/72641]\tLoss: 0.6408\tLR: 0.000010\n",
      "Training Epoch: 15 [66560/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 15 [66880/72641]\tLoss: 0.6033\tLR: 0.000010\n",
      "Training Epoch: 15 [67200/72641]\tLoss: 0.6188\tLR: 0.000010\n",
      "Training Epoch: 15 [67520/72641]\tLoss: 0.6138\tLR: 0.000010\n",
      "Training Epoch: 15 [67840/72641]\tLoss: 0.6497\tLR: 0.000010\n",
      "Training Epoch: 15 [68160/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 15 [68480/72641]\tLoss: 0.6357\tLR: 0.000010\n",
      "Training Epoch: 15 [68800/72641]\tLoss: 0.6586\tLR: 0.000010\n",
      "Training Epoch: 15 [69120/72641]\tLoss: 0.6862\tLR: 0.000010\n",
      "Training Epoch: 15 [69440/72641]\tLoss: 0.6241\tLR: 0.000010\n",
      "Training Epoch: 15 [69760/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 15 [70080/72641]\tLoss: 0.6159\tLR: 0.000010\n",
      "Training Epoch: 15 [70400/72641]\tLoss: 0.6739\tLR: 0.000010\n",
      "Training Epoch: 15 [70720/72641]\tLoss: 0.6984\tLR: 0.000010\n",
      "Training Epoch: 15 [71040/72641]\tLoss: 0.6091\tLR: 0.000010\n",
      "Training Epoch: 15 [71360/72641]\tLoss: 0.6856\tLR: 0.000010\n",
      "Training Epoch: 15 [71680/72641]\tLoss: 0.6812\tLR: 0.000010\n",
      "Training Epoch: 15 [72000/72641]\tLoss: 0.6379\tLR: 0.000010\n",
      "Training Epoch: 15 [72320/72641]\tLoss: 0.6498\tLR: 0.000010\n",
      "Training Epoch: 15 [72640/72641]\tLoss: 0.5971\tLR: 0.000010\n",
      "Val Result: Acc: 0.1437, C_ACC: 0.6922, DOA: 87.9820, ACC_k: 0.1008\n",
      "ext:0.0, cls:0.585429, coar:0.0, fine:0.0,\n",
      "Training Epoch: 16 [320/72641]\tLoss: 0.6447\tLR: 0.000010\n",
      "Training Epoch: 16 [640/72641]\tLoss: 0.6532\tLR: 0.000010\n",
      "Training Epoch: 16 [960/72641]\tLoss: 0.6217\tLR: 0.000010\n",
      "Training Epoch: 16 [1280/72641]\tLoss: 0.6095\tLR: 0.000010\n",
      "Training Epoch: 16 [1600/72641]\tLoss: 0.6654\tLR: 0.000010\n",
      "Training Epoch: 16 [1920/72641]\tLoss: 0.6407\tLR: 0.000010\n",
      "Training Epoch: 16 [2240/72641]\tLoss: 0.6789\tLR: 0.000010\n",
      "Training Epoch: 16 [2560/72641]\tLoss: 0.6266\tLR: 0.000010\n",
      "Training Epoch: 16 [2880/72641]\tLoss: 0.6388\tLR: 0.000010\n",
      "Training Epoch: 16 [3200/72641]\tLoss: 0.6255\tLR: 0.000010\n",
      "Training Epoch: 16 [3520/72641]\tLoss: 0.6457\tLR: 0.000010\n",
      "Training Epoch: 16 [3840/72641]\tLoss: 0.6434\tLR: 0.000010\n",
      "Training Epoch: 16 [4160/72641]\tLoss: 0.6253\tLR: 0.000010\n",
      "Training Epoch: 16 [4480/72641]\tLoss: 0.6514\tLR: 0.000010\n",
      "Training Epoch: 16 [4800/72641]\tLoss: 0.6534\tLR: 0.000010\n",
      "Training Epoch: 16 [5120/72641]\tLoss: 0.6528\tLR: 0.000010\n",
      "Training Epoch: 16 [5440/72641]\tLoss: 0.6294\tLR: 0.000010\n",
      "Training Epoch: 16 [5760/72641]\tLoss: 0.6334\tLR: 0.000010\n",
      "Training Epoch: 16 [6080/72641]\tLoss: 0.6202\tLR: 0.000010\n",
      "Training Epoch: 16 [6400/72641]\tLoss: 0.5915\tLR: 0.000010\n",
      "Training Epoch: 16 [6720/72641]\tLoss: 0.6100\tLR: 0.000010\n",
      "Training Epoch: 16 [7040/72641]\tLoss: 0.6622\tLR: 0.000010\n",
      "Training Epoch: 16 [7360/72641]\tLoss: 0.6715\tLR: 0.000010\n",
      "Training Epoch: 16 [7680/72641]\tLoss: 0.6721\tLR: 0.000010\n",
      "Training Epoch: 16 [8000/72641]\tLoss: 0.6412\tLR: 0.000010\n",
      "Training Epoch: 16 [8320/72641]\tLoss: 0.5932\tLR: 0.000010\n",
      "Training Epoch: 16 [8640/72641]\tLoss: 0.6729\tLR: 0.000010\n",
      "Training Epoch: 16 [8960/72641]\tLoss: 0.5759\tLR: 0.000010\n",
      "Training Epoch: 16 [9280/72641]\tLoss: 0.6051\tLR: 0.000010\n",
      "Training Epoch: 16 [9600/72641]\tLoss: 0.6828\tLR: 0.000010\n",
      "Training Epoch: 16 [9920/72641]\tLoss: 0.6462\tLR: 0.000010\n",
      "Training Epoch: 16 [10240/72641]\tLoss: 0.6714\tLR: 0.000010\n",
      "Training Epoch: 16 [10560/72641]\tLoss: 0.6571\tLR: 0.000010\n",
      "Training Epoch: 16 [10880/72641]\tLoss: 0.6507\tLR: 0.000010\n",
      "Training Epoch: 16 [11200/72641]\tLoss: 0.6039\tLR: 0.000010\n",
      "Training Epoch: 16 [11520/72641]\tLoss: 0.6709\tLR: 0.000010\n",
      "Training Epoch: 16 [11840/72641]\tLoss: 0.6218\tLR: 0.000010\n",
      "Training Epoch: 16 [12160/72641]\tLoss: 0.6305\tLR: 0.000010\n",
      "Training Epoch: 16 [12480/72641]\tLoss: 0.6408\tLR: 0.000010\n",
      "Training Epoch: 16 [12800/72641]\tLoss: 0.6551\tLR: 0.000010\n",
      "Training Epoch: 16 [13120/72641]\tLoss: 0.6335\tLR: 0.000010\n",
      "Training Epoch: 16 [13440/72641]\tLoss: 0.6374\tLR: 0.000010\n",
      "Training Epoch: 16 [13760/72641]\tLoss: 0.6571\tLR: 0.000010\n",
      "Training Epoch: 16 [14080/72641]\tLoss: 0.6530\tLR: 0.000010\n",
      "Training Epoch: 16 [14400/72641]\tLoss: 0.6145\tLR: 0.000010\n",
      "Training Epoch: 16 [14720/72641]\tLoss: 0.6382\tLR: 0.000010\n",
      "Training Epoch: 16 [15040/72641]\tLoss: 0.6537\tLR: 0.000010\n",
      "Training Epoch: 16 [15360/72641]\tLoss: 0.6729\tLR: 0.000010\n",
      "Training Epoch: 16 [15680/72641]\tLoss: 0.6365\tLR: 0.000010\n",
      "Training Epoch: 16 [16000/72641]\tLoss: 0.6358\tLR: 0.000010\n",
      "Training Epoch: 16 [16320/72641]\tLoss: 0.7182\tLR: 0.000010\n",
      "Training Epoch: 16 [16640/72641]\tLoss: 0.6567\tLR: 0.000010\n",
      "Training Epoch: 16 [16960/72641]\tLoss: 0.6275\tLR: 0.000010\n",
      "Training Epoch: 16 [17280/72641]\tLoss: 0.6292\tLR: 0.000010\n",
      "Training Epoch: 16 [17600/72641]\tLoss: 0.6286\tLR: 0.000010\n",
      "Training Epoch: 16 [17920/72641]\tLoss: 0.6522\tLR: 0.000010\n",
      "Training Epoch: 16 [18240/72641]\tLoss: 0.6181\tLR: 0.000010\n",
      "Training Epoch: 16 [18560/72641]\tLoss: 0.6109\tLR: 0.000010\n",
      "Training Epoch: 16 [18880/72641]\tLoss: 0.6218\tLR: 0.000010\n",
      "Training Epoch: 16 [19200/72641]\tLoss: 0.7146\tLR: 0.000010\n",
      "Training Epoch: 16 [19520/72641]\tLoss: 0.5911\tLR: 0.000010\n",
      "Training Epoch: 16 [19840/72641]\tLoss: 0.6723\tLR: 0.000010\n",
      "Training Epoch: 16 [20160/72641]\tLoss: 0.6433\tLR: 0.000010\n",
      "Training Epoch: 16 [20480/72641]\tLoss: 0.6437\tLR: 0.000010\n",
      "Training Epoch: 16 [20800/72641]\tLoss: 0.6503\tLR: 0.000010\n",
      "Training Epoch: 16 [21120/72641]\tLoss: 0.6616\tLR: 0.000010\n",
      "Training Epoch: 16 [21440/72641]\tLoss: 0.6178\tLR: 0.000010\n",
      "Training Epoch: 16 [21760/72641]\tLoss: 0.6605\tLR: 0.000010\n",
      "Training Epoch: 16 [22080/72641]\tLoss: 0.6622\tLR: 0.000010\n",
      "Training Epoch: 16 [22400/72641]\tLoss: 0.6358\tLR: 0.000010\n",
      "Training Epoch: 16 [22720/72641]\tLoss: 0.6043\tLR: 0.000010\n",
      "Training Epoch: 16 [23040/72641]\tLoss: 0.6398\tLR: 0.000010\n",
      "Training Epoch: 16 [23360/72641]\tLoss: 0.6749\tLR: 0.000010\n",
      "Training Epoch: 16 [23680/72641]\tLoss: 0.6741\tLR: 0.000010\n",
      "Training Epoch: 16 [24000/72641]\tLoss: 0.6367\tLR: 0.000010\n",
      "Training Epoch: 16 [24320/72641]\tLoss: 0.6551\tLR: 0.000010\n",
      "Training Epoch: 16 [24640/72641]\tLoss: 0.6197\tLR: 0.000010\n",
      "Training Epoch: 16 [24960/72641]\tLoss: 0.6562\tLR: 0.000010\n",
      "Training Epoch: 16 [25280/72641]\tLoss: 0.6680\tLR: 0.000010\n",
      "Training Epoch: 16 [25600/72641]\tLoss: 0.6278\tLR: 0.000010\n",
      "Training Epoch: 16 [25920/72641]\tLoss: 0.6097\tLR: 0.000010\n",
      "Training Epoch: 16 [26240/72641]\tLoss: 0.6377\tLR: 0.000010\n",
      "Training Epoch: 16 [26560/72641]\tLoss: 0.6610\tLR: 0.000010\n",
      "Training Epoch: 16 [26880/72641]\tLoss: 0.6451\tLR: 0.000010\n",
      "Training Epoch: 16 [27200/72641]\tLoss: 0.6025\tLR: 0.000010\n",
      "Training Epoch: 16 [27520/72641]\tLoss: 0.6372\tLR: 0.000010\n",
      "Training Epoch: 16 [27840/72641]\tLoss: 0.6356\tLR: 0.000010\n",
      "Training Epoch: 16 [28160/72641]\tLoss: 0.6345\tLR: 0.000010\n",
      "Training Epoch: 16 [28480/72641]\tLoss: 0.6388\tLR: 0.000010\n",
      "Training Epoch: 16 [28800/72641]\tLoss: 0.6241\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [29120/72641]\tLoss: 0.6301\tLR: 0.000010\n",
      "Training Epoch: 16 [29440/72641]\tLoss: 0.6645\tLR: 0.000010\n",
      "Training Epoch: 16 [29760/72641]\tLoss: 0.6524\tLR: 0.000010\n",
      "Training Epoch: 16 [30080/72641]\tLoss: 0.6554\tLR: 0.000010\n",
      "Training Epoch: 16 [30400/72641]\tLoss: 0.6988\tLR: 0.000010\n",
      "Training Epoch: 16 [30720/72641]\tLoss: 0.6723\tLR: 0.000010\n",
      "Training Epoch: 16 [31040/72641]\tLoss: 0.6507\tLR: 0.000010\n",
      "Training Epoch: 16 [31360/72641]\tLoss: 0.6322\tLR: 0.000010\n",
      "Training Epoch: 16 [31680/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 16 [32000/72641]\tLoss: 0.6317\tLR: 0.000010\n",
      "Training Epoch: 16 [32320/72641]\tLoss: 0.6643\tLR: 0.000010\n",
      "Training Epoch: 16 [32640/72641]\tLoss: 0.6260\tLR: 0.000010\n",
      "Training Epoch: 16 [32960/72641]\tLoss: 0.7162\tLR: 0.000010\n",
      "Training Epoch: 16 [33280/72641]\tLoss: 0.6083\tLR: 0.000010\n",
      "Training Epoch: 16 [33600/72641]\tLoss: 0.6310\tLR: 0.000010\n",
      "Training Epoch: 16 [33920/72641]\tLoss: 0.6359\tLR: 0.000010\n",
      "Training Epoch: 16 [34240/72641]\tLoss: 0.6052\tLR: 0.000010\n",
      "Training Epoch: 16 [34560/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 16 [34880/72641]\tLoss: 0.7040\tLR: 0.000010\n",
      "Training Epoch: 16 [35200/72641]\tLoss: 0.6227\tLR: 0.000010\n",
      "Training Epoch: 16 [35520/72641]\tLoss: 0.6016\tLR: 0.000010\n",
      "Training Epoch: 16 [35840/72641]\tLoss: 0.6319\tLR: 0.000010\n",
      "Training Epoch: 16 [36160/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 16 [36480/72641]\tLoss: 0.6438\tLR: 0.000010\n",
      "Training Epoch: 16 [36800/72641]\tLoss: 0.6034\tLR: 0.000010\n",
      "Training Epoch: 16 [37120/72641]\tLoss: 0.6310\tLR: 0.000010\n",
      "Training Epoch: 16 [37440/72641]\tLoss: 0.6664\tLR: 0.000010\n",
      "Training Epoch: 16 [37760/72641]\tLoss: 0.6548\tLR: 0.000010\n",
      "Training Epoch: 16 [38080/72641]\tLoss: 0.6635\tLR: 0.000010\n",
      "Training Epoch: 16 [38400/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 16 [38720/72641]\tLoss: 0.6676\tLR: 0.000010\n",
      "Training Epoch: 16 [39040/72641]\tLoss: 0.6539\tLR: 0.000010\n",
      "Training Epoch: 16 [39360/72641]\tLoss: 0.6095\tLR: 0.000010\n",
      "Training Epoch: 16 [39680/72641]\tLoss: 0.6033\tLR: 0.000010\n",
      "Training Epoch: 16 [40000/72641]\tLoss: 0.6426\tLR: 0.000010\n",
      "Training Epoch: 16 [40320/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 16 [40640/72641]\tLoss: 0.6443\tLR: 0.000010\n",
      "Training Epoch: 16 [40960/72641]\tLoss: 0.6572\tLR: 0.000010\n",
      "Training Epoch: 16 [41280/72641]\tLoss: 0.6536\tLR: 0.000010\n",
      "Training Epoch: 16 [41600/72641]\tLoss: 0.6516\tLR: 0.000010\n",
      "Training Epoch: 16 [41920/72641]\tLoss: 0.5897\tLR: 0.000010\n",
      "Training Epoch: 16 [42240/72641]\tLoss: 0.6065\tLR: 0.000010\n",
      "Training Epoch: 16 [42560/72641]\tLoss: 0.6339\tLR: 0.000010\n",
      "Training Epoch: 16 [42880/72641]\tLoss: 0.6679\tLR: 0.000010\n",
      "Training Epoch: 16 [43200/72641]\tLoss: 0.6581\tLR: 0.000010\n",
      "Training Epoch: 16 [43520/72641]\tLoss: 0.6289\tLR: 0.000010\n",
      "Training Epoch: 16 [43840/72641]\tLoss: 0.6181\tLR: 0.000010\n",
      "Training Epoch: 16 [44160/72641]\tLoss: 0.6732\tLR: 0.000010\n",
      "Training Epoch: 16 [44480/72641]\tLoss: 0.6719\tLR: 0.000010\n",
      "Training Epoch: 16 [44800/72641]\tLoss: 0.5711\tLR: 0.000010\n",
      "Training Epoch: 16 [45120/72641]\tLoss: 0.6123\tLR: 0.000010\n",
      "Training Epoch: 16 [45440/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 16 [45760/72641]\tLoss: 0.6223\tLR: 0.000010\n",
      "Training Epoch: 16 [46080/72641]\tLoss: 0.7001\tLR: 0.000010\n",
      "Training Epoch: 16 [46400/72641]\tLoss: 0.6619\tLR: 0.000010\n",
      "Training Epoch: 16 [46720/72641]\tLoss: 0.6541\tLR: 0.000010\n",
      "Training Epoch: 16 [47040/72641]\tLoss: 0.6466\tLR: 0.000010\n",
      "Training Epoch: 16 [47360/72641]\tLoss: 0.6318\tLR: 0.000010\n",
      "Training Epoch: 16 [47680/72641]\tLoss: 0.6298\tLR: 0.000010\n",
      "Training Epoch: 16 [48000/72641]\tLoss: 0.6387\tLR: 0.000010\n",
      "Training Epoch: 16 [48320/72641]\tLoss: 0.6514\tLR: 0.000010\n",
      "Training Epoch: 16 [48640/72641]\tLoss: 0.5679\tLR: 0.000010\n",
      "Training Epoch: 16 [48960/72641]\tLoss: 0.6702\tLR: 0.000010\n",
      "Training Epoch: 16 [49280/72641]\tLoss: 0.6233\tLR: 0.000010\n",
      "Training Epoch: 16 [49600/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 16 [49920/72641]\tLoss: 0.5808\tLR: 0.000010\n",
      "Training Epoch: 16 [50240/72641]\tLoss: 0.6494\tLR: 0.000010\n",
      "Training Epoch: 16 [50560/72641]\tLoss: 0.6301\tLR: 0.000010\n",
      "Training Epoch: 16 [50880/72641]\tLoss: 0.6162\tLR: 0.000010\n",
      "Training Epoch: 16 [51200/72641]\tLoss: 0.6410\tLR: 0.000010\n",
      "Training Epoch: 16 [51520/72641]\tLoss: 0.6548\tLR: 0.000010\n",
      "Training Epoch: 16 [51840/72641]\tLoss: 0.6087\tLR: 0.000010\n",
      "Training Epoch: 16 [52160/72641]\tLoss: 0.6413\tLR: 0.000010\n",
      "Training Epoch: 16 [52480/72641]\tLoss: 0.6409\tLR: 0.000010\n",
      "Training Epoch: 16 [52800/72641]\tLoss: 0.6609\tLR: 0.000010\n",
      "Training Epoch: 16 [53120/72641]\tLoss: 0.6029\tLR: 0.000010\n",
      "Training Epoch: 16 [53440/72641]\tLoss: 0.5995\tLR: 0.000010\n",
      "Training Epoch: 16 [53760/72641]\tLoss: 0.6633\tLR: 0.000010\n",
      "Training Epoch: 16 [54080/72641]\tLoss: 0.6659\tLR: 0.000010\n",
      "Training Epoch: 16 [54400/72641]\tLoss: 0.6811\tLR: 0.000010\n",
      "Training Epoch: 16 [54720/72641]\tLoss: 0.6499\tLR: 0.000010\n",
      "Training Epoch: 16 [55040/72641]\tLoss: 0.6327\tLR: 0.000010\n",
      "Training Epoch: 16 [55360/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 16 [55680/72641]\tLoss: 0.6285\tLR: 0.000010\n",
      "Training Epoch: 16 [56000/72641]\tLoss: 0.6037\tLR: 0.000010\n",
      "Training Epoch: 16 [56320/72641]\tLoss: 0.6314\tLR: 0.000010\n",
      "Training Epoch: 16 [56640/72641]\tLoss: 0.6512\tLR: 0.000010\n",
      "Training Epoch: 16 [56960/72641]\tLoss: 0.6109\tLR: 0.000010\n",
      "Training Epoch: 16 [57280/72641]\tLoss: 0.5914\tLR: 0.000010\n",
      "Training Epoch: 16 [57600/72641]\tLoss: 0.6645\tLR: 0.000010\n",
      "Training Epoch: 16 [57920/72641]\tLoss: 0.6207\tLR: 0.000010\n",
      "Training Epoch: 16 [58240/72641]\tLoss: 0.6071\tLR: 0.000010\n",
      "Training Epoch: 16 [58560/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 16 [58880/72641]\tLoss: 0.6575\tLR: 0.000010\n",
      "Training Epoch: 16 [59200/72641]\tLoss: 0.6642\tLR: 0.000010\n",
      "Training Epoch: 16 [59520/72641]\tLoss: 0.6458\tLR: 0.000010\n",
      "Training Epoch: 16 [59840/72641]\tLoss: 0.6278\tLR: 0.000010\n",
      "Training Epoch: 16 [60160/72641]\tLoss: 0.6222\tLR: 0.000010\n",
      "Training Epoch: 16 [60480/72641]\tLoss: 0.6506\tLR: 0.000010\n",
      "Training Epoch: 16 [60800/72641]\tLoss: 0.6599\tLR: 0.000010\n",
      "Training Epoch: 16 [61120/72641]\tLoss: 0.5962\tLR: 0.000010\n",
      "Training Epoch: 16 [61440/72641]\tLoss: 0.6665\tLR: 0.000010\n",
      "Training Epoch: 16 [61760/72641]\tLoss: 0.6092\tLR: 0.000010\n",
      "Training Epoch: 16 [62080/72641]\tLoss: 0.6562\tLR: 0.000010\n",
      "Training Epoch: 16 [62400/72641]\tLoss: 0.6491\tLR: 0.000010\n",
      "Training Epoch: 16 [62720/72641]\tLoss: 0.6750\tLR: 0.000010\n",
      "Training Epoch: 16 [63040/72641]\tLoss: 0.6336\tLR: 0.000010\n",
      "Training Epoch: 16 [63360/72641]\tLoss: 0.6571\tLR: 0.000010\n",
      "Training Epoch: 16 [63680/72641]\tLoss: 0.6101\tLR: 0.000010\n",
      "Training Epoch: 16 [64000/72641]\tLoss: 0.6313\tLR: 0.000010\n",
      "Training Epoch: 16 [64320/72641]\tLoss: 0.6577\tLR: 0.000010\n",
      "Training Epoch: 16 [64640/72641]\tLoss: 0.6345\tLR: 0.000010\n",
      "Training Epoch: 16 [64960/72641]\tLoss: 0.6491\tLR: 0.000010\n",
      "Training Epoch: 16 [65280/72641]\tLoss: 0.5884\tLR: 0.000010\n",
      "Training Epoch: 16 [65600/72641]\tLoss: 0.6695\tLR: 0.000010\n",
      "Training Epoch: 16 [65920/72641]\tLoss: 0.6600\tLR: 0.000010\n",
      "Training Epoch: 16 [66240/72641]\tLoss: 0.6523\tLR: 0.000010\n",
      "Training Epoch: 16 [66560/72641]\tLoss: 0.6233\tLR: 0.000010\n",
      "Training Epoch: 16 [66880/72641]\tLoss: 0.6391\tLR: 0.000010\n",
      "Training Epoch: 16 [67200/72641]\tLoss: 0.5898\tLR: 0.000010\n",
      "Training Epoch: 16 [67520/72641]\tLoss: 0.6724\tLR: 0.000010\n",
      "Training Epoch: 16 [67840/72641]\tLoss: 0.6429\tLR: 0.000010\n",
      "Training Epoch: 16 [68160/72641]\tLoss: 0.6443\tLR: 0.000010\n",
      "Training Epoch: 16 [68480/72641]\tLoss: 0.6351\tLR: 0.000010\n",
      "Training Epoch: 16 [68800/72641]\tLoss: 0.6468\tLR: 0.000010\n",
      "Training Epoch: 16 [69120/72641]\tLoss: 0.6412\tLR: 0.000010\n",
      "Training Epoch: 16 [69440/72641]\tLoss: 0.6253\tLR: 0.000010\n",
      "Training Epoch: 16 [69760/72641]\tLoss: 0.6540\tLR: 0.000010\n",
      "Training Epoch: 16 [70080/72641]\tLoss: 0.6184\tLR: 0.000010\n",
      "Training Epoch: 16 [70400/72641]\tLoss: 0.6454\tLR: 0.000010\n",
      "Training Epoch: 16 [70720/72641]\tLoss: 0.6381\tLR: 0.000010\n",
      "Training Epoch: 16 [71040/72641]\tLoss: 0.6412\tLR: 0.000010\n",
      "Training Epoch: 16 [71360/72641]\tLoss: 0.6436\tLR: 0.000010\n",
      "Training Epoch: 16 [71680/72641]\tLoss: 0.6946\tLR: 0.000010\n",
      "Training Epoch: 16 [72000/72641]\tLoss: 0.6635\tLR: 0.000010\n",
      "Training Epoch: 16 [72320/72641]\tLoss: 0.5805\tLR: 0.000010\n",
      "Training Epoch: 16 [72640/72641]\tLoss: 0.6359\tLR: 0.000010\n",
      "Val Result: Acc: 0.1484, C_ACC: 0.6935, DOA: 88.0147, ACC_k: 0.1040\n",
      "ext:0.0, cls:0.573738, coar:0.0, fine:0.0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [320/72641]\tLoss: 0.6264\tLR: 0.000010\n",
      "Training Epoch: 17 [640/72641]\tLoss: 0.6016\tLR: 0.000010\n",
      "Training Epoch: 17 [960/72641]\tLoss: 0.6319\tLR: 0.000010\n",
      "Training Epoch: 17 [1280/72641]\tLoss: 0.6133\tLR: 0.000010\n",
      "Training Epoch: 17 [1600/72641]\tLoss: 0.6612\tLR: 0.000010\n",
      "Training Epoch: 17 [1920/72641]\tLoss: 0.6373\tLR: 0.000010\n",
      "Training Epoch: 17 [2240/72641]\tLoss: 0.6456\tLR: 0.000010\n",
      "Training Epoch: 17 [2560/72641]\tLoss: 0.6524\tLR: 0.000010\n",
      "Training Epoch: 17 [2880/72641]\tLoss: 0.6585\tLR: 0.000010\n",
      "Training Epoch: 17 [3200/72641]\tLoss: 0.6330\tLR: 0.000010\n",
      "Training Epoch: 17 [3520/72641]\tLoss: 0.6525\tLR: 0.000010\n",
      "Training Epoch: 17 [3840/72641]\tLoss: 0.6083\tLR: 0.000010\n",
      "Training Epoch: 17 [4160/72641]\tLoss: 0.6680\tLR: 0.000010\n",
      "Training Epoch: 17 [4480/72641]\tLoss: 0.6687\tLR: 0.000010\n",
      "Training Epoch: 17 [4800/72641]\tLoss: 0.6645\tLR: 0.000010\n",
      "Training Epoch: 17 [5120/72641]\tLoss: 0.6443\tLR: 0.000010\n",
      "Training Epoch: 17 [5440/72641]\tLoss: 0.6465\tLR: 0.000010\n",
      "Training Epoch: 17 [5760/72641]\tLoss: 0.6624\tLR: 0.000010\n",
      "Training Epoch: 17 [6080/72641]\tLoss: 0.6282\tLR: 0.000010\n",
      "Training Epoch: 17 [6400/72641]\tLoss: 0.5889\tLR: 0.000010\n",
      "Training Epoch: 17 [6720/72641]\tLoss: 0.6225\tLR: 0.000010\n",
      "Training Epoch: 17 [7040/72641]\tLoss: 0.6543\tLR: 0.000010\n",
      "Training Epoch: 17 [7360/72641]\tLoss: 0.6593\tLR: 0.000010\n",
      "Training Epoch: 17 [7680/72641]\tLoss: 0.6385\tLR: 0.000010\n",
      "Training Epoch: 17 [8000/72641]\tLoss: 0.6716\tLR: 0.000010\n",
      "Training Epoch: 17 [8320/72641]\tLoss: 0.6521\tLR: 0.000010\n",
      "Training Epoch: 17 [8640/72641]\tLoss: 0.6179\tLR: 0.000010\n",
      "Training Epoch: 17 [8960/72641]\tLoss: 0.5989\tLR: 0.000010\n",
      "Training Epoch: 17 [9280/72641]\tLoss: 0.6105\tLR: 0.000010\n",
      "Training Epoch: 17 [9600/72641]\tLoss: 0.6828\tLR: 0.000010\n",
      "Training Epoch: 17 [9920/72641]\tLoss: 0.6476\tLR: 0.000010\n",
      "Training Epoch: 17 [10240/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 17 [10560/72641]\tLoss: 0.6317\tLR: 0.000010\n",
      "Training Epoch: 17 [10880/72641]\tLoss: 0.6629\tLR: 0.000010\n",
      "Training Epoch: 17 [11200/72641]\tLoss: 0.6354\tLR: 0.000010\n",
      "Training Epoch: 17 [11520/72641]\tLoss: 0.5914\tLR: 0.000010\n",
      "Training Epoch: 17 [11840/72641]\tLoss: 0.5820\tLR: 0.000010\n",
      "Training Epoch: 17 [12160/72641]\tLoss: 0.6245\tLR: 0.000010\n",
      "Training Epoch: 17 [12480/72641]\tLoss: 0.6580\tLR: 0.000010\n",
      "Training Epoch: 17 [12800/72641]\tLoss: 0.6564\tLR: 0.000010\n",
      "Training Epoch: 17 [13120/72641]\tLoss: 0.5778\tLR: 0.000010\n",
      "Training Epoch: 17 [13440/72641]\tLoss: 0.6518\tLR: 0.000010\n",
      "Training Epoch: 17 [13760/72641]\tLoss: 0.6549\tLR: 0.000010\n",
      "Training Epoch: 17 [14080/72641]\tLoss: 0.5876\tLR: 0.000010\n",
      "Training Epoch: 17 [14400/72641]\tLoss: 0.6344\tLR: 0.000010\n",
      "Training Epoch: 17 [14720/72641]\tLoss: 0.6630\tLR: 0.000010\n",
      "Training Epoch: 17 [15040/72641]\tLoss: 0.6282\tLR: 0.000010\n",
      "Training Epoch: 17 [15360/72641]\tLoss: 0.6829\tLR: 0.000010\n",
      "Training Epoch: 17 [15680/72641]\tLoss: 0.6584\tLR: 0.000010\n",
      "Training Epoch: 17 [16000/72641]\tLoss: 0.6186\tLR: 0.000010\n",
      "Training Epoch: 17 [16320/72641]\tLoss: 0.6578\tLR: 0.000010\n",
      "Training Epoch: 17 [16640/72641]\tLoss: 0.6284\tLR: 0.000010\n",
      "Training Epoch: 17 [16960/72641]\tLoss: 0.6264\tLR: 0.000010\n",
      "Training Epoch: 17 [17280/72641]\tLoss: 0.6433\tLR: 0.000010\n",
      "Training Epoch: 17 [17600/72641]\tLoss: 0.6396\tLR: 0.000010\n",
      "Training Epoch: 17 [17920/72641]\tLoss: 0.6175\tLR: 0.000010\n",
      "Training Epoch: 17 [18240/72641]\tLoss: 0.6597\tLR: 0.000010\n",
      "Training Epoch: 17 [18560/72641]\tLoss: 0.6252\tLR: 0.000010\n",
      "Training Epoch: 17 [18880/72641]\tLoss: 0.6529\tLR: 0.000010\n",
      "Training Epoch: 17 [19200/72641]\tLoss: 0.6338\tLR: 0.000010\n",
      "Training Epoch: 17 [19520/72641]\tLoss: 0.6355\tLR: 0.000010\n",
      "Training Epoch: 17 [19840/72641]\tLoss: 0.6280\tLR: 0.000010\n",
      "Training Epoch: 17 [20160/72641]\tLoss: 0.6403\tLR: 0.000010\n",
      "Training Epoch: 17 [20480/72641]\tLoss: 0.6234\tLR: 0.000010\n",
      "Training Epoch: 17 [20800/72641]\tLoss: 0.6315\tLR: 0.000010\n",
      "Training Epoch: 17 [21120/72641]\tLoss: 0.6378\tLR: 0.000010\n",
      "Training Epoch: 17 [21440/72641]\tLoss: 0.6260\tLR: 0.000010\n",
      "Training Epoch: 17 [21760/72641]\tLoss: 0.6402\tLR: 0.000010\n",
      "Training Epoch: 17 [22080/72641]\tLoss: 0.6230\tLR: 0.000010\n",
      "Training Epoch: 17 [22400/72641]\tLoss: 0.6382\tLR: 0.000010\n",
      "Training Epoch: 17 [22720/72641]\tLoss: 0.6472\tLR: 0.000010\n",
      "Training Epoch: 17 [23040/72641]\tLoss: 0.6607\tLR: 0.000010\n",
      "Training Epoch: 17 [23360/72641]\tLoss: 0.6117\tLR: 0.000010\n",
      "Training Epoch: 17 [23680/72641]\tLoss: 0.6490\tLR: 0.000010\n",
      "Training Epoch: 17 [24000/72641]\tLoss: 0.6282\tLR: 0.000010\n",
      "Training Epoch: 17 [24320/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 17 [24640/72641]\tLoss: 0.7025\tLR: 0.000010\n",
      "Training Epoch: 17 [24960/72641]\tLoss: 0.6544\tLR: 0.000010\n",
      "Training Epoch: 17 [25280/72641]\tLoss: 0.6130\tLR: 0.000010\n",
      "Training Epoch: 17 [25600/72641]\tLoss: 0.6032\tLR: 0.000010\n",
      "Training Epoch: 17 [25920/72641]\tLoss: 0.6423\tLR: 0.000010\n",
      "Training Epoch: 17 [26240/72641]\tLoss: 0.6689\tLR: 0.000010\n",
      "Training Epoch: 17 [26560/72641]\tLoss: 0.6516\tLR: 0.000010\n",
      "Training Epoch: 17 [26880/72641]\tLoss: 0.6171\tLR: 0.000010\n",
      "Training Epoch: 17 [27200/72641]\tLoss: 0.6409\tLR: 0.000010\n",
      "Training Epoch: 17 [27520/72641]\tLoss: 0.6664\tLR: 0.000010\n",
      "Training Epoch: 17 [27840/72641]\tLoss: 0.6239\tLR: 0.000010\n",
      "Training Epoch: 17 [28160/72641]\tLoss: 0.6466\tLR: 0.000010\n",
      "Training Epoch: 17 [28480/72641]\tLoss: 0.6224\tLR: 0.000010\n",
      "Training Epoch: 17 [28800/72641]\tLoss: 0.6373\tLR: 0.000010\n",
      "Training Epoch: 17 [29120/72641]\tLoss: 0.6376\tLR: 0.000010\n",
      "Training Epoch: 17 [29440/72641]\tLoss: 0.6268\tLR: 0.000010\n",
      "Training Epoch: 17 [29760/72641]\tLoss: 0.6114\tLR: 0.000010\n",
      "Training Epoch: 17 [30080/72641]\tLoss: 0.6536\tLR: 0.000010\n",
      "Training Epoch: 17 [30400/72641]\tLoss: 0.5922\tLR: 0.000010\n",
      "Training Epoch: 17 [30720/72641]\tLoss: 0.6731\tLR: 0.000010\n",
      "Training Epoch: 17 [31040/72641]\tLoss: 0.5786\tLR: 0.000010\n",
      "Training Epoch: 17 [31360/72641]\tLoss: 0.6252\tLR: 0.000010\n",
      "Training Epoch: 17 [31680/72641]\tLoss: 0.6623\tLR: 0.000010\n",
      "Training Epoch: 17 [32000/72641]\tLoss: 0.6437\tLR: 0.000010\n",
      "Training Epoch: 17 [32320/72641]\tLoss: 0.6718\tLR: 0.000010\n",
      "Training Epoch: 17 [32640/72641]\tLoss: 0.6534\tLR: 0.000010\n",
      "Training Epoch: 17 [32960/72641]\tLoss: 0.6898\tLR: 0.000010\n",
      "Training Epoch: 17 [33280/72641]\tLoss: 0.6349\tLR: 0.000010\n",
      "Training Epoch: 17 [33600/72641]\tLoss: 0.6201\tLR: 0.000010\n",
      "Training Epoch: 17 [33920/72641]\tLoss: 0.6052\tLR: 0.000010\n",
      "Training Epoch: 17 [34240/72641]\tLoss: 0.6175\tLR: 0.000010\n",
      "Training Epoch: 17 [34560/72641]\tLoss: 0.6683\tLR: 0.000010\n",
      "Training Epoch: 17 [34880/72641]\tLoss: 0.6359\tLR: 0.000010\n",
      "Training Epoch: 17 [35200/72641]\tLoss: 0.6803\tLR: 0.000010\n",
      "Training Epoch: 17 [35520/72641]\tLoss: 0.6120\tLR: 0.000010\n",
      "Training Epoch: 17 [35840/72641]\tLoss: 0.6587\tLR: 0.000010\n",
      "Training Epoch: 17 [36160/72641]\tLoss: 0.6434\tLR: 0.000010\n",
      "Training Epoch: 17 [36480/72641]\tLoss: 0.6172\tLR: 0.000010\n",
      "Training Epoch: 17 [36800/72641]\tLoss: 0.5913\tLR: 0.000010\n",
      "Training Epoch: 17 [37120/72641]\tLoss: 0.6397\tLR: 0.000010\n",
      "Training Epoch: 17 [37440/72641]\tLoss: 0.6347\tLR: 0.000010\n",
      "Training Epoch: 17 [37760/72641]\tLoss: 0.6467\tLR: 0.000010\n",
      "Training Epoch: 17 [38080/72641]\tLoss: 0.6350\tLR: 0.000010\n",
      "Training Epoch: 17 [38400/72641]\tLoss: 0.6516\tLR: 0.000010\n",
      "Training Epoch: 17 [38720/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 17 [39040/72641]\tLoss: 0.6453\tLR: 0.000010\n",
      "Training Epoch: 17 [39360/72641]\tLoss: 0.6409\tLR: 0.000010\n",
      "Training Epoch: 17 [39680/72641]\tLoss: 0.5941\tLR: 0.000010\n",
      "Training Epoch: 17 [40000/72641]\tLoss: 0.6484\tLR: 0.000010\n",
      "Training Epoch: 17 [40320/72641]\tLoss: 0.6284\tLR: 0.000010\n",
      "Training Epoch: 17 [40640/72641]\tLoss: 0.6458\tLR: 0.000010\n",
      "Training Epoch: 17 [40960/72641]\tLoss: 0.6338\tLR: 0.000010\n",
      "Training Epoch: 17 [41280/72641]\tLoss: 0.6414\tLR: 0.000010\n",
      "Training Epoch: 17 [41600/72641]\tLoss: 0.6316\tLR: 0.000010\n",
      "Training Epoch: 17 [41920/72641]\tLoss: 0.6159\tLR: 0.000010\n",
      "Training Epoch: 17 [42240/72641]\tLoss: 0.6053\tLR: 0.000010\n",
      "Training Epoch: 17 [42560/72641]\tLoss: 0.6440\tLR: 0.000010\n",
      "Training Epoch: 17 [42880/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 17 [43200/72641]\tLoss: 0.6403\tLR: 0.000010\n",
      "Training Epoch: 17 [43520/72641]\tLoss: 0.6679\tLR: 0.000010\n",
      "Training Epoch: 17 [43840/72641]\tLoss: 0.6267\tLR: 0.000010\n",
      "Training Epoch: 17 [44160/72641]\tLoss: 0.6569\tLR: 0.000010\n",
      "Training Epoch: 17 [44480/72641]\tLoss: 0.6548\tLR: 0.000010\n",
      "Training Epoch: 17 [44800/72641]\tLoss: 0.6121\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [45120/72641]\tLoss: 0.6220\tLR: 0.000010\n",
      "Training Epoch: 17 [45440/72641]\tLoss: 0.6578\tLR: 0.000010\n",
      "Training Epoch: 17 [45760/72641]\tLoss: 0.6404\tLR: 0.000010\n",
      "Training Epoch: 17 [46080/72641]\tLoss: 0.6378\tLR: 0.000010\n",
      "Training Epoch: 17 [46400/72641]\tLoss: 0.6425\tLR: 0.000010\n",
      "Training Epoch: 17 [46720/72641]\tLoss: 0.6524\tLR: 0.000010\n",
      "Training Epoch: 17 [47040/72641]\tLoss: 0.6337\tLR: 0.000010\n",
      "Training Epoch: 17 [47360/72641]\tLoss: 0.6398\tLR: 0.000010\n",
      "Training Epoch: 17 [47680/72641]\tLoss: 0.6152\tLR: 0.000010\n",
      "Training Epoch: 17 [48000/72641]\tLoss: 0.6174\tLR: 0.000010\n",
      "Training Epoch: 17 [48320/72641]\tLoss: 0.6771\tLR: 0.000010\n",
      "Training Epoch: 17 [48640/72641]\tLoss: 0.6461\tLR: 0.000010\n",
      "Training Epoch: 17 [48960/72641]\tLoss: 0.6605\tLR: 0.000010\n",
      "Training Epoch: 17 [49280/72641]\tLoss: 0.6411\tLR: 0.000010\n",
      "Training Epoch: 17 [49600/72641]\tLoss: 0.6457\tLR: 0.000010\n",
      "Training Epoch: 17 [49920/72641]\tLoss: 0.6573\tLR: 0.000010\n",
      "Training Epoch: 17 [50240/72641]\tLoss: 0.5978\tLR: 0.000010\n",
      "Training Epoch: 17 [50560/72641]\tLoss: 0.6217\tLR: 0.000010\n",
      "Training Epoch: 17 [50880/72641]\tLoss: 0.6055\tLR: 0.000010\n",
      "Training Epoch: 17 [51200/72641]\tLoss: 0.6624\tLR: 0.000010\n",
      "Training Epoch: 17 [51520/72641]\tLoss: 0.6417\tLR: 0.000010\n",
      "Training Epoch: 17 [51840/72641]\tLoss: 0.6079\tLR: 0.000010\n",
      "Training Epoch: 17 [52160/72641]\tLoss: 0.6511\tLR: 0.000010\n",
      "Training Epoch: 17 [52480/72641]\tLoss: 0.6463\tLR: 0.000010\n",
      "Training Epoch: 17 [52800/72641]\tLoss: 0.6318\tLR: 0.000010\n",
      "Training Epoch: 17 [53120/72641]\tLoss: 0.6253\tLR: 0.000010\n",
      "Training Epoch: 17 [53440/72641]\tLoss: 0.6662\tLR: 0.000010\n",
      "Training Epoch: 17 [53760/72641]\tLoss: 0.6685\tLR: 0.000010\n",
      "Training Epoch: 17 [54080/72641]\tLoss: 0.6447\tLR: 0.000010\n",
      "Training Epoch: 17 [54400/72641]\tLoss: 0.6677\tLR: 0.000010\n",
      "Training Epoch: 17 [54720/72641]\tLoss: 0.6521\tLR: 0.000010\n",
      "Training Epoch: 17 [55040/72641]\tLoss: 0.6473\tLR: 0.000010\n",
      "Training Epoch: 17 [55360/72641]\tLoss: 0.6392\tLR: 0.000010\n",
      "Training Epoch: 17 [55680/72641]\tLoss: 0.6551\tLR: 0.000010\n",
      "Training Epoch: 17 [56000/72641]\tLoss: 0.5637\tLR: 0.000010\n",
      "Training Epoch: 17 [56320/72641]\tLoss: 0.5778\tLR: 0.000010\n",
      "Training Epoch: 17 [56640/72641]\tLoss: 0.6367\tLR: 0.000010\n",
      "Training Epoch: 17 [56960/72641]\tLoss: 0.6149\tLR: 0.000010\n",
      "Training Epoch: 17 [57280/72641]\tLoss: 0.6443\tLR: 0.000010\n",
      "Training Epoch: 17 [57600/72641]\tLoss: 0.5912\tLR: 0.000010\n",
      "Training Epoch: 17 [57920/72641]\tLoss: 0.6535\tLR: 0.000010\n",
      "Training Epoch: 17 [58240/72641]\tLoss: 0.5868\tLR: 0.000010\n",
      "Training Epoch: 17 [58560/72641]\tLoss: 0.6499\tLR: 0.000010\n",
      "Training Epoch: 17 [58880/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 17 [59200/72641]\tLoss: 0.6521\tLR: 0.000010\n",
      "Training Epoch: 17 [59520/72641]\tLoss: 0.6414\tLR: 0.000010\n",
      "Training Epoch: 17 [59840/72641]\tLoss: 0.6438\tLR: 0.000010\n",
      "Training Epoch: 17 [60160/72641]\tLoss: 0.6322\tLR: 0.000010\n",
      "Training Epoch: 17 [60480/72641]\tLoss: 0.6772\tLR: 0.000010\n",
      "Training Epoch: 17 [60800/72641]\tLoss: 0.6447\tLR: 0.000010\n",
      "Training Epoch: 17 [61120/72641]\tLoss: 0.5892\tLR: 0.000010\n",
      "Training Epoch: 17 [61440/72641]\tLoss: 0.6098\tLR: 0.000010\n",
      "Training Epoch: 17 [61760/72641]\tLoss: 0.6392\tLR: 0.000010\n",
      "Training Epoch: 17 [62080/72641]\tLoss: 0.6617\tLR: 0.000010\n",
      "Training Epoch: 17 [62400/72641]\tLoss: 0.6570\tLR: 0.000010\n",
      "Training Epoch: 17 [62720/72641]\tLoss: 0.6433\tLR: 0.000010\n",
      "Training Epoch: 17 [63040/72641]\tLoss: 0.6054\tLR: 0.000010\n",
      "Training Epoch: 17 [63360/72641]\tLoss: 0.6535\tLR: 0.000010\n",
      "Training Epoch: 17 [63680/72641]\tLoss: 0.6101\tLR: 0.000010\n",
      "Training Epoch: 17 [64000/72641]\tLoss: 0.6534\tLR: 0.000010\n",
      "Training Epoch: 17 [64320/72641]\tLoss: 0.6375\tLR: 0.000010\n",
      "Training Epoch: 17 [64640/72641]\tLoss: 0.6154\tLR: 0.000010\n",
      "Training Epoch: 17 [64960/72641]\tLoss: 0.6507\tLR: 0.000010\n",
      "Training Epoch: 17 [65280/72641]\tLoss: 0.6216\tLR: 0.000010\n",
      "Training Epoch: 17 [65600/72641]\tLoss: 0.6409\tLR: 0.000010\n",
      "Training Epoch: 17 [65920/72641]\tLoss: 0.6510\tLR: 0.000010\n",
      "Training Epoch: 17 [66240/72641]\tLoss: 0.6532\tLR: 0.000010\n",
      "Training Epoch: 17 [66560/72641]\tLoss: 0.6671\tLR: 0.000010\n",
      "Training Epoch: 17 [66880/72641]\tLoss: 0.6451\tLR: 0.000010\n",
      "Training Epoch: 17 [67200/72641]\tLoss: 0.5981\tLR: 0.000010\n",
      "Training Epoch: 17 [67520/72641]\tLoss: 0.6173\tLR: 0.000010\n",
      "Training Epoch: 17 [67840/72641]\tLoss: 0.6223\tLR: 0.000010\n",
      "Training Epoch: 17 [68160/72641]\tLoss: 0.6358\tLR: 0.000010\n",
      "Training Epoch: 17 [68480/72641]\tLoss: 0.6238\tLR: 0.000010\n",
      "Training Epoch: 17 [68800/72641]\tLoss: 0.6722\tLR: 0.000010\n",
      "Training Epoch: 17 [69120/72641]\tLoss: 0.6953\tLR: 0.000010\n",
      "Training Epoch: 17 [69440/72641]\tLoss: 0.6277\tLR: 0.000010\n",
      "Training Epoch: 17 [69760/72641]\tLoss: 0.6368\tLR: 0.000010\n",
      "Training Epoch: 17 [70080/72641]\tLoss: 0.5558\tLR: 0.000010\n",
      "Training Epoch: 17 [70400/72641]\tLoss: 0.6639\tLR: 0.000010\n",
      "Training Epoch: 17 [70720/72641]\tLoss: 0.6726\tLR: 0.000010\n",
      "Training Epoch: 17 [71040/72641]\tLoss: 0.5824\tLR: 0.000010\n",
      "Training Epoch: 17 [71360/72641]\tLoss: 0.6838\tLR: 0.000010\n",
      "Training Epoch: 17 [71680/72641]\tLoss: 0.6899\tLR: 0.000010\n",
      "Training Epoch: 17 [72000/72641]\tLoss: 0.6554\tLR: 0.000010\n",
      "Training Epoch: 17 [72320/72641]\tLoss: 0.6288\tLR: 0.000010\n",
      "Training Epoch: 17 [72640/72641]\tLoss: 0.6010\tLR: 0.000010\n",
      "Val Result: Acc: 0.1520, C_ACC: 0.7041, DOA: 87.3108, ACC_k: 0.1068\n",
      "ext:0.0, cls:0.568082, coar:0.0, fine:0.0,\n",
      "Training Epoch: 18 [320/72641]\tLoss: 0.6429\tLR: 0.000010\n",
      "Training Epoch: 18 [640/72641]\tLoss: 0.6450\tLR: 0.000010\n",
      "Training Epoch: 18 [960/72641]\tLoss: 0.6478\tLR: 0.000010\n",
      "Training Epoch: 18 [1280/72641]\tLoss: 0.6657\tLR: 0.000010\n",
      "Training Epoch: 18 [1600/72641]\tLoss: 0.6666\tLR: 0.000010\n",
      "Training Epoch: 18 [1920/72641]\tLoss: 0.6268\tLR: 0.000010\n",
      "Training Epoch: 18 [2240/72641]\tLoss: 0.6217\tLR: 0.000010\n",
      "Training Epoch: 18 [2560/72641]\tLoss: 0.6379\tLR: 0.000010\n",
      "Training Epoch: 18 [2880/72641]\tLoss: 0.6472\tLR: 0.000010\n",
      "Training Epoch: 18 [3200/72641]\tLoss: 0.6331\tLR: 0.000010\n",
      "Training Epoch: 18 [3520/72641]\tLoss: 0.6554\tLR: 0.000010\n",
      "Training Epoch: 18 [3840/72641]\tLoss: 0.6509\tLR: 0.000010\n",
      "Training Epoch: 18 [4160/72641]\tLoss: 0.6104\tLR: 0.000010\n",
      "Training Epoch: 18 [4480/72641]\tLoss: 0.6173\tLR: 0.000010\n",
      "Training Epoch: 18 [4800/72641]\tLoss: 0.6291\tLR: 0.000010\n",
      "Training Epoch: 18 [5120/72641]\tLoss: 0.6418\tLR: 0.000010\n",
      "Training Epoch: 18 [5440/72641]\tLoss: 0.6522\tLR: 0.000010\n",
      "Training Epoch: 18 [5760/72641]\tLoss: 0.6607\tLR: 0.000010\n",
      "Training Epoch: 18 [6080/72641]\tLoss: 0.6542\tLR: 0.000010\n",
      "Training Epoch: 18 [6400/72641]\tLoss: 0.6466\tLR: 0.000010\n",
      "Training Epoch: 18 [6720/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 18 [7040/72641]\tLoss: 0.6402\tLR: 0.000010\n",
      "Training Epoch: 18 [7360/72641]\tLoss: 0.6591\tLR: 0.000010\n",
      "Training Epoch: 18 [7680/72641]\tLoss: 0.6454\tLR: 0.000010\n",
      "Training Epoch: 18 [8000/72641]\tLoss: 0.6498\tLR: 0.000010\n",
      "Training Epoch: 18 [8320/72641]\tLoss: 0.6587\tLR: 0.000010\n",
      "Training Epoch: 18 [8640/72641]\tLoss: 0.6409\tLR: 0.000010\n",
      "Training Epoch: 18 [8960/72641]\tLoss: 0.6063\tLR: 0.000010\n",
      "Training Epoch: 18 [9280/72641]\tLoss: 0.6633\tLR: 0.000010\n",
      "Training Epoch: 18 [9600/72641]\tLoss: 0.5921\tLR: 0.000010\n",
      "Training Epoch: 18 [9920/72641]\tLoss: 0.6402\tLR: 0.000010\n",
      "Training Epoch: 18 [10240/72641]\tLoss: 0.6433\tLR: 0.000010\n",
      "Training Epoch: 18 [10560/72641]\tLoss: 0.6459\tLR: 0.000010\n",
      "Training Epoch: 18 [10880/72641]\tLoss: 0.6366\tLR: 0.000010\n",
      "Training Epoch: 18 [11200/72641]\tLoss: 0.6494\tLR: 0.000010\n",
      "Training Epoch: 18 [11520/72641]\tLoss: 0.6238\tLR: 0.000010\n",
      "Training Epoch: 18 [11840/72641]\tLoss: 0.6209\tLR: 0.000010\n",
      "Training Epoch: 18 [12160/72641]\tLoss: 0.6081\tLR: 0.000010\n",
      "Training Epoch: 18 [12480/72641]\tLoss: 0.6584\tLR: 0.000010\n",
      "Training Epoch: 18 [12800/72641]\tLoss: 0.5862\tLR: 0.000010\n",
      "Training Epoch: 18 [13120/72641]\tLoss: 0.6178\tLR: 0.000010\n",
      "Training Epoch: 18 [13440/72641]\tLoss: 0.6200\tLR: 0.000010\n",
      "Training Epoch: 18 [13760/72641]\tLoss: 0.6551\tLR: 0.000010\n",
      "Training Epoch: 18 [14080/72641]\tLoss: 0.6475\tLR: 0.000010\n",
      "Training Epoch: 18 [14400/72641]\tLoss: 0.6610\tLR: 0.000010\n",
      "Training Epoch: 18 [14720/72641]\tLoss: 0.6320\tLR: 0.000010\n",
      "Training Epoch: 18 [15040/72641]\tLoss: 0.6309\tLR: 0.000010\n",
      "Training Epoch: 18 [15360/72641]\tLoss: 0.6387\tLR: 0.000010\n",
      "Training Epoch: 18 [15680/72641]\tLoss: 0.6553\tLR: 0.000010\n",
      "Training Epoch: 18 [16000/72641]\tLoss: 0.5776\tLR: 0.000010\n",
      "Training Epoch: 18 [16320/72641]\tLoss: 0.6570\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [16640/72641]\tLoss: 0.6395\tLR: 0.000010\n",
      "Training Epoch: 18 [16960/72641]\tLoss: 0.6838\tLR: 0.000010\n",
      "Training Epoch: 18 [17280/72641]\tLoss: 0.6229\tLR: 0.000010\n",
      "Training Epoch: 18 [17600/72641]\tLoss: 0.6152\tLR: 0.000010\n",
      "Training Epoch: 18 [17920/72641]\tLoss: 0.6468\tLR: 0.000010\n",
      "Training Epoch: 18 [18240/72641]\tLoss: 0.6445\tLR: 0.000010\n",
      "Training Epoch: 18 [18560/72641]\tLoss: 0.6061\tLR: 0.000010\n",
      "Training Epoch: 18 [18880/72641]\tLoss: 0.6346\tLR: 0.000010\n",
      "Training Epoch: 18 [19200/72641]\tLoss: 0.6765\tLR: 0.000010\n",
      "Training Epoch: 18 [19520/72641]\tLoss: 0.6188\tLR: 0.000010\n",
      "Training Epoch: 18 [19840/72641]\tLoss: 0.6115\tLR: 0.000010\n",
      "Training Epoch: 18 [20160/72641]\tLoss: 0.6372\tLR: 0.000010\n",
      "Training Epoch: 18 [20480/72641]\tLoss: 0.6217\tLR: 0.000010\n",
      "Training Epoch: 18 [20800/72641]\tLoss: 0.6647\tLR: 0.000010\n",
      "Training Epoch: 18 [21120/72641]\tLoss: 0.6593\tLR: 0.000010\n",
      "Training Epoch: 18 [21440/72641]\tLoss: 0.6947\tLR: 0.000010\n",
      "Training Epoch: 18 [21760/72641]\tLoss: 0.6297\tLR: 0.000010\n",
      "Training Epoch: 18 [22080/72641]\tLoss: 0.6198\tLR: 0.000010\n",
      "Training Epoch: 18 [22400/72641]\tLoss: 0.6558\tLR: 0.000010\n",
      "Training Epoch: 18 [22720/72641]\tLoss: 0.6074\tLR: 0.000010\n",
      "Training Epoch: 18 [23040/72641]\tLoss: 0.6195\tLR: 0.000010\n",
      "Training Epoch: 18 [23360/72641]\tLoss: 0.6308\tLR: 0.000010\n",
      "Training Epoch: 18 [23680/72641]\tLoss: 0.6255\tLR: 0.000010\n",
      "Training Epoch: 18 [24000/72641]\tLoss: 0.6067\tLR: 0.000010\n",
      "Training Epoch: 18 [24320/72641]\tLoss: 0.6156\tLR: 0.000010\n",
      "Training Epoch: 18 [24640/72641]\tLoss: 0.6229\tLR: 0.000010\n",
      "Training Epoch: 18 [24960/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 18 [25280/72641]\tLoss: 0.6127\tLR: 0.000010\n",
      "Training Epoch: 18 [25600/72641]\tLoss: 0.6029\tLR: 0.000010\n",
      "Training Epoch: 18 [25920/72641]\tLoss: 0.6502\tLR: 0.000010\n",
      "Training Epoch: 18 [26240/72641]\tLoss: 0.6826\tLR: 0.000010\n",
      "Training Epoch: 18 [26560/72641]\tLoss: 0.6075\tLR: 0.000010\n",
      "Training Epoch: 18 [26880/72641]\tLoss: 0.6828\tLR: 0.000010\n",
      "Training Epoch: 18 [27200/72641]\tLoss: 0.6001\tLR: 0.000010\n",
      "Training Epoch: 18 [27520/72641]\tLoss: 0.6518\tLR: 0.000010\n",
      "Training Epoch: 18 [27840/72641]\tLoss: 0.6690\tLR: 0.000010\n",
      "Training Epoch: 18 [28160/72641]\tLoss: 0.6409\tLR: 0.000010\n",
      "Training Epoch: 18 [28480/72641]\tLoss: 0.6146\tLR: 0.000010\n",
      "Training Epoch: 18 [28800/72641]\tLoss: 0.5713\tLR: 0.000010\n",
      "Training Epoch: 18 [29120/72641]\tLoss: 0.6779\tLR: 0.000010\n",
      "Training Epoch: 18 [29440/72641]\tLoss: 0.6505\tLR: 0.000010\n",
      "Training Epoch: 18 [29760/72641]\tLoss: 0.6336\tLR: 0.000010\n",
      "Training Epoch: 18 [30080/72641]\tLoss: 0.6442\tLR: 0.000010\n",
      "Training Epoch: 18 [30400/72641]\tLoss: 0.6183\tLR: 0.000010\n",
      "Training Epoch: 18 [30720/72641]\tLoss: 0.6084\tLR: 0.000010\n",
      "Training Epoch: 18 [31040/72641]\tLoss: 0.6316\tLR: 0.000010\n",
      "Training Epoch: 18 [31360/72641]\tLoss: 0.5946\tLR: 0.000010\n",
      "Training Epoch: 18 [31680/72641]\tLoss: 0.6812\tLR: 0.000010\n",
      "Training Epoch: 18 [32000/72641]\tLoss: 0.6106\tLR: 0.000010\n",
      "Training Epoch: 18 [32320/72641]\tLoss: 0.6254\tLR: 0.000010\n",
      "Training Epoch: 18 [32640/72641]\tLoss: 0.6629\tLR: 0.000010\n",
      "Training Epoch: 18 [32960/72641]\tLoss: 0.6357\tLR: 0.000010\n",
      "Training Epoch: 18 [33280/72641]\tLoss: 0.6724\tLR: 0.000010\n",
      "Training Epoch: 18 [33600/72641]\tLoss: 0.6376\tLR: 0.000010\n",
      "Training Epoch: 18 [33920/72641]\tLoss: 0.5999\tLR: 0.000010\n",
      "Training Epoch: 18 [34240/72641]\tLoss: 0.6483\tLR: 0.000010\n",
      "Training Epoch: 18 [34560/72641]\tLoss: 0.6691\tLR: 0.000010\n",
      "Training Epoch: 18 [34880/72641]\tLoss: 0.6517\tLR: 0.000010\n",
      "Training Epoch: 18 [35200/72641]\tLoss: 0.5864\tLR: 0.000010\n",
      "Training Epoch: 18 [35520/72641]\tLoss: 0.6391\tLR: 0.000010\n",
      "Training Epoch: 18 [35840/72641]\tLoss: 0.6399\tLR: 0.000010\n",
      "Training Epoch: 18 [36160/72641]\tLoss: 0.6330\tLR: 0.000010\n",
      "Training Epoch: 18 [36480/72641]\tLoss: 0.6454\tLR: 0.000010\n",
      "Training Epoch: 18 [36800/72641]\tLoss: 0.6407\tLR: 0.000010\n",
      "Training Epoch: 18 [37120/72641]\tLoss: 0.6319\tLR: 0.000010\n",
      "Training Epoch: 18 [37440/72641]\tLoss: 0.6621\tLR: 0.000010\n",
      "Training Epoch: 18 [37760/72641]\tLoss: 0.6766\tLR: 0.000010\n",
      "Training Epoch: 18 [38080/72641]\tLoss: 0.6379\tLR: 0.000010\n",
      "Training Epoch: 18 [38400/72641]\tLoss: 0.7075\tLR: 0.000010\n",
      "Training Epoch: 18 [38720/72641]\tLoss: 0.6540\tLR: 0.000010\n",
      "Training Epoch: 18 [39040/72641]\tLoss: 0.6663\tLR: 0.000010\n",
      "Training Epoch: 18 [39360/72641]\tLoss: 0.5744\tLR: 0.000010\n",
      "Training Epoch: 18 [39680/72641]\tLoss: 0.6140\tLR: 0.000010\n",
      "Training Epoch: 18 [40000/72641]\tLoss: 0.6759\tLR: 0.000010\n",
      "Training Epoch: 18 [40320/72641]\tLoss: 0.6600\tLR: 0.000010\n",
      "Training Epoch: 18 [40640/72641]\tLoss: 0.5936\tLR: 0.000010\n",
      "Training Epoch: 18 [40960/72641]\tLoss: 0.6538\tLR: 0.000010\n",
      "Training Epoch: 18 [41280/72641]\tLoss: 0.6517\tLR: 0.000010\n",
      "Training Epoch: 18 [41600/72641]\tLoss: 0.5899\tLR: 0.000010\n",
      "Training Epoch: 18 [41920/72641]\tLoss: 0.6367\tLR: 0.000010\n",
      "Training Epoch: 18 [42240/72641]\tLoss: 0.6399\tLR: 0.000010\n",
      "Training Epoch: 18 [42560/72641]\tLoss: 0.6457\tLR: 0.000010\n",
      "Training Epoch: 18 [42880/72641]\tLoss: 0.6794\tLR: 0.000010\n",
      "Training Epoch: 18 [43200/72641]\tLoss: 0.6269\tLR: 0.000010\n",
      "Training Epoch: 18 [43520/72641]\tLoss: 0.6573\tLR: 0.000010\n",
      "Training Epoch: 18 [43840/72641]\tLoss: 0.6513\tLR: 0.000010\n",
      "Training Epoch: 18 [44160/72641]\tLoss: 0.6997\tLR: 0.000010\n",
      "Training Epoch: 18 [44480/72641]\tLoss: 0.6599\tLR: 0.000010\n",
      "Training Epoch: 18 [44800/72641]\tLoss: 0.6028\tLR: 0.000010\n",
      "Training Epoch: 18 [45120/72641]\tLoss: 0.5957\tLR: 0.000010\n",
      "Training Epoch: 18 [45440/72641]\tLoss: 0.6198\tLR: 0.000010\n",
      "Training Epoch: 18 [45760/72641]\tLoss: 0.6140\tLR: 0.000010\n",
      "Training Epoch: 18 [46080/72641]\tLoss: 0.6192\tLR: 0.000010\n",
      "Training Epoch: 18 [46400/72641]\tLoss: 0.6313\tLR: 0.000010\n",
      "Training Epoch: 18 [46720/72641]\tLoss: 0.6901\tLR: 0.000010\n",
      "Training Epoch: 18 [47040/72641]\tLoss: 0.6530\tLR: 0.000010\n",
      "Training Epoch: 18 [47360/72641]\tLoss: 0.6518\tLR: 0.000010\n",
      "Training Epoch: 18 [47680/72641]\tLoss: 0.6056\tLR: 0.000010\n",
      "Training Epoch: 18 [48000/72641]\tLoss: 0.6452\tLR: 0.000010\n",
      "Training Epoch: 18 [48320/72641]\tLoss: 0.7049\tLR: 0.000010\n",
      "Training Epoch: 18 [48640/72641]\tLoss: 0.6261\tLR: 0.000010\n",
      "Training Epoch: 18 [48960/72641]\tLoss: 0.7008\tLR: 0.000010\n",
      "Training Epoch: 18 [49280/72641]\tLoss: 0.6304\tLR: 0.000010\n",
      "Training Epoch: 18 [49600/72641]\tLoss: 0.6540\tLR: 0.000010\n",
      "Training Epoch: 18 [49920/72641]\tLoss: 0.6203\tLR: 0.000010\n",
      "Training Epoch: 18 [50240/72641]\tLoss: 0.6289\tLR: 0.000010\n",
      "Training Epoch: 18 [50560/72641]\tLoss: 0.6111\tLR: 0.000010\n",
      "Training Epoch: 18 [50880/72641]\tLoss: 0.5693\tLR: 0.000010\n",
      "Training Epoch: 18 [51200/72641]\tLoss: 0.6652\tLR: 0.000010\n",
      "Training Epoch: 18 [51520/72641]\tLoss: 0.6570\tLR: 0.000010\n",
      "Training Epoch: 18 [51840/72641]\tLoss: 0.6105\tLR: 0.000010\n",
      "Training Epoch: 18 [52160/72641]\tLoss: 0.6237\tLR: 0.000010\n",
      "Training Epoch: 18 [52480/72641]\tLoss: 0.6367\tLR: 0.000010\n",
      "Training Epoch: 18 [52800/72641]\tLoss: 0.6488\tLR: 0.000010\n",
      "Training Epoch: 18 [53120/72641]\tLoss: 0.6142\tLR: 0.000010\n",
      "Training Epoch: 18 [53440/72641]\tLoss: 0.6067\tLR: 0.000010\n",
      "Training Epoch: 18 [53760/72641]\tLoss: 0.6274\tLR: 0.000010\n",
      "Training Epoch: 18 [54080/72641]\tLoss: 0.5890\tLR: 0.000010\n",
      "Training Epoch: 18 [54400/72641]\tLoss: 0.6251\tLR: 0.000010\n",
      "Training Epoch: 18 [54720/72641]\tLoss: 0.6085\tLR: 0.000010\n",
      "Training Epoch: 18 [55040/72641]\tLoss: 0.6731\tLR: 0.000010\n",
      "Training Epoch: 18 [55360/72641]\tLoss: 0.6248\tLR: 0.000010\n",
      "Training Epoch: 18 [55680/72641]\tLoss: 0.5981\tLR: 0.000010\n",
      "Training Epoch: 18 [56000/72641]\tLoss: 0.6081\tLR: 0.000010\n",
      "Training Epoch: 18 [56320/72641]\tLoss: 0.6133\tLR: 0.000010\n",
      "Training Epoch: 18 [56640/72641]\tLoss: 0.6310\tLR: 0.000010\n",
      "Training Epoch: 18 [56960/72641]\tLoss: 0.6711\tLR: 0.000010\n",
      "Training Epoch: 18 [57280/72641]\tLoss: 0.5973\tLR: 0.000010\n",
      "Training Epoch: 18 [57600/72641]\tLoss: 0.6356\tLR: 0.000010\n",
      "Training Epoch: 18 [57920/72641]\tLoss: 0.6285\tLR: 0.000010\n",
      "Training Epoch: 18 [58240/72641]\tLoss: 0.6528\tLR: 0.000010\n",
      "Training Epoch: 18 [58560/72641]\tLoss: 0.5957\tLR: 0.000010\n",
      "Training Epoch: 18 [58880/72641]\tLoss: 0.6247\tLR: 0.000010\n",
      "Training Epoch: 18 [59200/72641]\tLoss: 0.6496\tLR: 0.000010\n",
      "Training Epoch: 18 [59520/72641]\tLoss: 0.6032\tLR: 0.000010\n",
      "Training Epoch: 18 [59840/72641]\tLoss: 0.6274\tLR: 0.000010\n",
      "Training Epoch: 18 [60160/72641]\tLoss: 0.6142\tLR: 0.000010\n",
      "Training Epoch: 18 [60480/72641]\tLoss: 0.6310\tLR: 0.000010\n",
      "Training Epoch: 18 [60800/72641]\tLoss: 0.5989\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [61120/72641]\tLoss: 0.5946\tLR: 0.000010\n",
      "Training Epoch: 18 [61440/72641]\tLoss: 0.6526\tLR: 0.000010\n",
      "Training Epoch: 18 [61760/72641]\tLoss: 0.6122\tLR: 0.000010\n",
      "Training Epoch: 18 [62080/72641]\tLoss: 0.6720\tLR: 0.000010\n",
      "Training Epoch: 18 [62400/72641]\tLoss: 0.6226\tLR: 0.000010\n",
      "Training Epoch: 18 [62720/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 18 [63040/72641]\tLoss: 0.6676\tLR: 0.000010\n",
      "Training Epoch: 18 [63360/72641]\tLoss: 0.6707\tLR: 0.000010\n",
      "Training Epoch: 18 [63680/72641]\tLoss: 0.5880\tLR: 0.000010\n",
      "Training Epoch: 18 [64000/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 18 [64320/72641]\tLoss: 0.6402\tLR: 0.000010\n",
      "Training Epoch: 18 [64640/72641]\tLoss: 0.6168\tLR: 0.000010\n",
      "Training Epoch: 18 [64960/72641]\tLoss: 0.6559\tLR: 0.000010\n",
      "Training Epoch: 18 [65280/72641]\tLoss: 0.6016\tLR: 0.000010\n",
      "Training Epoch: 18 [65600/72641]\tLoss: 0.6468\tLR: 0.000010\n",
      "Training Epoch: 18 [65920/72641]\tLoss: 0.6755\tLR: 0.000010\n",
      "Training Epoch: 18 [66240/72641]\tLoss: 0.6937\tLR: 0.000010\n",
      "Training Epoch: 18 [66560/72641]\tLoss: 0.6427\tLR: 0.000010\n",
      "Training Epoch: 18 [66880/72641]\tLoss: 0.6421\tLR: 0.000010\n",
      "Training Epoch: 18 [67200/72641]\tLoss: 0.6016\tLR: 0.000010\n",
      "Training Epoch: 18 [67520/72641]\tLoss: 0.6673\tLR: 0.000010\n",
      "Training Epoch: 18 [67840/72641]\tLoss: 0.6177\tLR: 0.000010\n",
      "Training Epoch: 18 [68160/72641]\tLoss: 0.6552\tLR: 0.000010\n",
      "Training Epoch: 18 [68480/72641]\tLoss: 0.6466\tLR: 0.000010\n",
      "Training Epoch: 18 [68800/72641]\tLoss: 0.6545\tLR: 0.000010\n",
      "Training Epoch: 18 [69120/72641]\tLoss: 0.6508\tLR: 0.000010\n",
      "Training Epoch: 18 [69440/72641]\tLoss: 0.6179\tLR: 0.000010\n",
      "Training Epoch: 18 [69760/72641]\tLoss: 0.6375\tLR: 0.000010\n",
      "Training Epoch: 18 [70080/72641]\tLoss: 0.6641\tLR: 0.000010\n",
      "Training Epoch: 18 [70400/72641]\tLoss: 0.6727\tLR: 0.000010\n",
      "Training Epoch: 18 [70720/72641]\tLoss: 0.6690\tLR: 0.000010\n",
      "Training Epoch: 18 [71040/72641]\tLoss: 0.5981\tLR: 0.000010\n",
      "Training Epoch: 18 [71360/72641]\tLoss: 0.6432\tLR: 0.000010\n",
      "Training Epoch: 18 [71680/72641]\tLoss: 0.7091\tLR: 0.000010\n",
      "Training Epoch: 18 [72000/72641]\tLoss: 0.6412\tLR: 0.000010\n",
      "Training Epoch: 18 [72320/72641]\tLoss: 0.6339\tLR: 0.000010\n",
      "Training Epoch: 18 [72640/72641]\tLoss: 0.6472\tLR: 0.000010\n",
      "Val Result: Acc: 0.1508, C_ACC: 0.7106, DOA: 87.9362, ACC_k: 0.1066\n",
      "ext:0.0, cls:0.566961, coar:0.0, fine:0.0,\n",
      "Training Epoch: 19 [320/72641]\tLoss: 0.6290\tLR: 0.000010\n",
      "Training Epoch: 19 [640/72641]\tLoss: 0.6342\tLR: 0.000010\n",
      "Training Epoch: 19 [960/72641]\tLoss: 0.5767\tLR: 0.000010\n",
      "Training Epoch: 19 [1280/72641]\tLoss: 0.6322\tLR: 0.000010\n",
      "Training Epoch: 19 [1600/72641]\tLoss: 0.6369\tLR: 0.000010\n",
      "Training Epoch: 19 [1920/72641]\tLoss: 0.6597\tLR: 0.000010\n",
      "Training Epoch: 19 [2240/72641]\tLoss: 0.6042\tLR: 0.000010\n",
      "Training Epoch: 19 [2560/72641]\tLoss: 0.6279\tLR: 0.000010\n",
      "Training Epoch: 19 [2880/72641]\tLoss: 0.6543\tLR: 0.000010\n",
      "Training Epoch: 19 [3200/72641]\tLoss: 0.6616\tLR: 0.000010\n",
      "Training Epoch: 19 [3520/72641]\tLoss: 0.6028\tLR: 0.000010\n",
      "Training Epoch: 19 [3840/72641]\tLoss: 0.6337\tLR: 0.000010\n",
      "Training Epoch: 19 [4160/72641]\tLoss: 0.6454\tLR: 0.000010\n",
      "Training Epoch: 19 [4480/72641]\tLoss: 0.5763\tLR: 0.000010\n",
      "Training Epoch: 19 [4800/72641]\tLoss: 0.5559\tLR: 0.000010\n",
      "Training Epoch: 19 [5120/72641]\tLoss: 0.6029\tLR: 0.000010\n",
      "Training Epoch: 19 [5440/72641]\tLoss: 0.6117\tLR: 0.000010\n",
      "Training Epoch: 19 [5760/72641]\tLoss: 0.6609\tLR: 0.000010\n",
      "Training Epoch: 19 [6080/72641]\tLoss: 0.6090\tLR: 0.000010\n",
      "Training Epoch: 19 [6400/72641]\tLoss: 0.6353\tLR: 0.000010\n",
      "Training Epoch: 19 [6720/72641]\tLoss: 0.5785\tLR: 0.000010\n",
      "Training Epoch: 19 [7040/72641]\tLoss: 0.5803\tLR: 0.000010\n",
      "Training Epoch: 19 [7360/72641]\tLoss: 0.6726\tLR: 0.000010\n",
      "Training Epoch: 19 [7680/72641]\tLoss: 0.6424\tLR: 0.000010\n",
      "Training Epoch: 19 [8000/72641]\tLoss: 0.6358\tLR: 0.000010\n",
      "Training Epoch: 19 [8320/72641]\tLoss: 0.6487\tLR: 0.000010\n",
      "Training Epoch: 19 [8640/72641]\tLoss: 0.6542\tLR: 0.000010\n",
      "Training Epoch: 19 [8960/72641]\tLoss: 0.5720\tLR: 0.000010\n",
      "Training Epoch: 19 [9280/72641]\tLoss: 0.6597\tLR: 0.000010\n",
      "Training Epoch: 19 [9600/72641]\tLoss: 0.6350\tLR: 0.000010\n",
      "Training Epoch: 19 [9920/72641]\tLoss: 0.5914\tLR: 0.000010\n",
      "Training Epoch: 19 [10240/72641]\tLoss: 0.6188\tLR: 0.000010\n",
      "Training Epoch: 19 [10560/72641]\tLoss: 0.5949\tLR: 0.000010\n",
      "Training Epoch: 19 [10880/72641]\tLoss: 0.6507\tLR: 0.000010\n",
      "Training Epoch: 19 [11200/72641]\tLoss: 0.6108\tLR: 0.000010\n",
      "Training Epoch: 19 [11520/72641]\tLoss: 0.6270\tLR: 0.000010\n",
      "Training Epoch: 19 [11840/72641]\tLoss: 0.6220\tLR: 0.000010\n",
      "Training Epoch: 19 [12160/72641]\tLoss: 0.6361\tLR: 0.000010\n",
      "Training Epoch: 19 [12480/72641]\tLoss: 0.6214\tLR: 0.000010\n",
      "Training Epoch: 19 [12800/72641]\tLoss: 0.6089\tLR: 0.000010\n",
      "Training Epoch: 19 [13120/72641]\tLoss: 0.6146\tLR: 0.000010\n",
      "Training Epoch: 19 [13440/72641]\tLoss: 0.6541\tLR: 0.000010\n",
      "Training Epoch: 19 [13760/72641]\tLoss: 0.6675\tLR: 0.000010\n",
      "Training Epoch: 19 [14080/72641]\tLoss: 0.6169\tLR: 0.000010\n",
      "Training Epoch: 19 [14400/72641]\tLoss: 0.6406\tLR: 0.000010\n",
      "Training Epoch: 19 [14720/72641]\tLoss: 0.6099\tLR: 0.000010\n",
      "Training Epoch: 19 [15040/72641]\tLoss: 0.6209\tLR: 0.000010\n",
      "Training Epoch: 19 [15360/72641]\tLoss: 0.6651\tLR: 0.000010\n",
      "Training Epoch: 19 [15680/72641]\tLoss: 0.6358\tLR: 0.000010\n",
      "Training Epoch: 19 [16000/72641]\tLoss: 0.5997\tLR: 0.000010\n",
      "Training Epoch: 19 [16320/72641]\tLoss: 0.6769\tLR: 0.000010\n",
      "Training Epoch: 19 [16640/72641]\tLoss: 0.6516\tLR: 0.000010\n",
      "Training Epoch: 19 [16960/72641]\tLoss: 0.6247\tLR: 0.000010\n",
      "Training Epoch: 19 [17280/72641]\tLoss: 0.6698\tLR: 0.000010\n",
      "Training Epoch: 19 [17600/72641]\tLoss: 0.5818\tLR: 0.000010\n",
      "Training Epoch: 19 [17920/72641]\tLoss: 0.6426\tLR: 0.000010\n",
      "Training Epoch: 19 [18240/72641]\tLoss: 0.6378\tLR: 0.000010\n",
      "Training Epoch: 19 [18560/72641]\tLoss: 0.6374\tLR: 0.000010\n",
      "Training Epoch: 19 [18880/72641]\tLoss: 0.6039\tLR: 0.000010\n",
      "Training Epoch: 19 [19200/72641]\tLoss: 0.6400\tLR: 0.000010\n",
      "Training Epoch: 19 [19520/72641]\tLoss: 0.6308\tLR: 0.000010\n",
      "Training Epoch: 19 [19840/72641]\tLoss: 0.5756\tLR: 0.000010\n",
      "Training Epoch: 19 [20160/72641]\tLoss: 0.6331\tLR: 0.000010\n",
      "Training Epoch: 19 [20480/72641]\tLoss: 0.6434\tLR: 0.000010\n",
      "Training Epoch: 19 [20800/72641]\tLoss: 0.6556\tLR: 0.000010\n",
      "Training Epoch: 19 [21120/72641]\tLoss: 0.6267\tLR: 0.000010\n",
      "Training Epoch: 19 [21440/72641]\tLoss: 0.6050\tLR: 0.000010\n",
      "Training Epoch: 19 [21760/72641]\tLoss: 0.6492\tLR: 0.000010\n",
      "Training Epoch: 19 [22080/72641]\tLoss: 0.7082\tLR: 0.000010\n",
      "Training Epoch: 19 [22400/72641]\tLoss: 0.6168\tLR: 0.000010\n",
      "Training Epoch: 19 [22720/72641]\tLoss: 0.6047\tLR: 0.000010\n",
      "Training Epoch: 19 [23040/72641]\tLoss: 0.6220\tLR: 0.000010\n",
      "Training Epoch: 19 [23360/72641]\tLoss: 0.6435\tLR: 0.000010\n",
      "Training Epoch: 19 [23680/72641]\tLoss: 0.6942\tLR: 0.000010\n",
      "Training Epoch: 19 [24000/72641]\tLoss: 0.6124\tLR: 0.000010\n",
      "Training Epoch: 19 [24320/72641]\tLoss: 0.6782\tLR: 0.000010\n",
      "Training Epoch: 19 [24640/72641]\tLoss: 0.6241\tLR: 0.000010\n",
      "Training Epoch: 19 [24960/72641]\tLoss: 0.6546\tLR: 0.000010\n",
      "Training Epoch: 19 [25280/72641]\tLoss: 0.6211\tLR: 0.000010\n",
      "Training Epoch: 19 [25600/72641]\tLoss: 0.5836\tLR: 0.000010\n",
      "Training Epoch: 19 [25920/72641]\tLoss: 0.6419\tLR: 0.000010\n",
      "Training Epoch: 19 [26240/72641]\tLoss: 0.6133\tLR: 0.000010\n",
      "Training Epoch: 19 [26560/72641]\tLoss: 0.6778\tLR: 0.000010\n",
      "Training Epoch: 19 [26880/72641]\tLoss: 0.6568\tLR: 0.000010\n",
      "Training Epoch: 19 [27200/72641]\tLoss: 0.6603\tLR: 0.000010\n",
      "Training Epoch: 19 [27520/72641]\tLoss: 0.6613\tLR: 0.000010\n",
      "Training Epoch: 19 [27840/72641]\tLoss: 0.6831\tLR: 0.000010\n",
      "Training Epoch: 19 [28160/72641]\tLoss: 0.6000\tLR: 0.000010\n",
      "Training Epoch: 19 [28480/72641]\tLoss: 0.6250\tLR: 0.000010\n",
      "Training Epoch: 19 [28800/72641]\tLoss: 0.6056\tLR: 0.000010\n",
      "Training Epoch: 19 [29120/72641]\tLoss: 0.6274\tLR: 0.000010\n",
      "Training Epoch: 19 [29440/72641]\tLoss: 0.5990\tLR: 0.000010\n",
      "Training Epoch: 19 [29760/72641]\tLoss: 0.6000\tLR: 0.000010\n",
      "Training Epoch: 19 [30080/72641]\tLoss: 0.6809\tLR: 0.000010\n",
      "Training Epoch: 19 [30400/72641]\tLoss: 0.5852\tLR: 0.000010\n",
      "Training Epoch: 19 [30720/72641]\tLoss: 0.6542\tLR: 0.000010\n",
      "Training Epoch: 19 [31040/72641]\tLoss: 0.6146\tLR: 0.000010\n",
      "Training Epoch: 19 [31360/72641]\tLoss: 0.6070\tLR: 0.000010\n",
      "Training Epoch: 19 [31680/72641]\tLoss: 0.6612\tLR: 0.000010\n",
      "Training Epoch: 19 [32000/72641]\tLoss: 0.5974\tLR: 0.000010\n",
      "Training Epoch: 19 [32320/72641]\tLoss: 0.6100\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [32640/72641]\tLoss: 0.6397\tLR: 0.000010\n",
      "Training Epoch: 19 [32960/72641]\tLoss: 0.6398\tLR: 0.000010\n",
      "Training Epoch: 19 [33280/72641]\tLoss: 0.6356\tLR: 0.000010\n",
      "Training Epoch: 19 [33600/72641]\tLoss: 0.5607\tLR: 0.000010\n",
      "Training Epoch: 19 [33920/72641]\tLoss: 0.6346\tLR: 0.000010\n",
      "Training Epoch: 19 [34240/72641]\tLoss: 0.6672\tLR: 0.000010\n",
      "Training Epoch: 19 [34560/72641]\tLoss: 0.6680\tLR: 0.000010\n",
      "Training Epoch: 19 [34880/72641]\tLoss: 0.6274\tLR: 0.000010\n",
      "Training Epoch: 19 [35200/72641]\tLoss: 0.6410\tLR: 0.000010\n",
      "Training Epoch: 19 [35520/72641]\tLoss: 0.6467\tLR: 0.000010\n",
      "Training Epoch: 19 [35840/72641]\tLoss: 0.6304\tLR: 0.000010\n",
      "Training Epoch: 19 [36160/72641]\tLoss: 0.6571\tLR: 0.000010\n",
      "Training Epoch: 19 [36480/72641]\tLoss: 0.5918\tLR: 0.000010\n",
      "Training Epoch: 19 [36800/72641]\tLoss: 0.6671\tLR: 0.000010\n",
      "Training Epoch: 19 [37120/72641]\tLoss: 0.6508\tLR: 0.000010\n",
      "Training Epoch: 19 [37440/72641]\tLoss: 0.6815\tLR: 0.000010\n",
      "Training Epoch: 19 [37760/72641]\tLoss: 0.6180\tLR: 0.000010\n",
      "Training Epoch: 19 [38080/72641]\tLoss: 0.6425\tLR: 0.000010\n",
      "Training Epoch: 19 [38400/72641]\tLoss: 0.6602\tLR: 0.000010\n",
      "Training Epoch: 19 [38720/72641]\tLoss: 0.6403\tLR: 0.000010\n",
      "Training Epoch: 19 [39040/72641]\tLoss: 0.6375\tLR: 0.000010\n",
      "Training Epoch: 19 [39360/72641]\tLoss: 0.6033\tLR: 0.000010\n",
      "Training Epoch: 19 [39680/72641]\tLoss: 0.5926\tLR: 0.000010\n",
      "Training Epoch: 19 [40000/72641]\tLoss: 0.6276\tLR: 0.000010\n",
      "Training Epoch: 19 [40320/72641]\tLoss: 0.6198\tLR: 0.000010\n",
      "Training Epoch: 19 [40640/72641]\tLoss: 0.6407\tLR: 0.000010\n",
      "Training Epoch: 19 [40960/72641]\tLoss: 0.6653\tLR: 0.000010\n",
      "Training Epoch: 19 [41280/72641]\tLoss: 0.6710\tLR: 0.000010\n",
      "Training Epoch: 19 [41600/72641]\tLoss: 0.6751\tLR: 0.000010\n",
      "Training Epoch: 19 [41920/72641]\tLoss: 0.6231\tLR: 0.000010\n",
      "Training Epoch: 19 [42240/72641]\tLoss: 0.6242\tLR: 0.000010\n",
      "Training Epoch: 19 [42560/72641]\tLoss: 0.6095\tLR: 0.000010\n",
      "Training Epoch: 19 [42880/72641]\tLoss: 0.6393\tLR: 0.000010\n",
      "Training Epoch: 19 [43200/72641]\tLoss: 0.6482\tLR: 0.000010\n",
      "Training Epoch: 19 [43520/72641]\tLoss: 0.6178\tLR: 0.000010\n",
      "Training Epoch: 19 [43840/72641]\tLoss: 0.6338\tLR: 0.000010\n",
      "Training Epoch: 19 [44160/72641]\tLoss: 0.7071\tLR: 0.000010\n",
      "Training Epoch: 19 [44480/72641]\tLoss: 0.6570\tLR: 0.000010\n",
      "Training Epoch: 19 [44800/72641]\tLoss: 0.5897\tLR: 0.000010\n",
      "Training Epoch: 19 [45120/72641]\tLoss: 0.5969\tLR: 0.000010\n",
      "Training Epoch: 19 [45440/72641]\tLoss: 0.6091\tLR: 0.000010\n",
      "Training Epoch: 19 [45760/72641]\tLoss: 0.6536\tLR: 0.000010\n",
      "Training Epoch: 19 [46080/72641]\tLoss: 0.6178\tLR: 0.000010\n",
      "Training Epoch: 19 [46400/72641]\tLoss: 0.6478\tLR: 0.000010\n",
      "Training Epoch: 19 [46720/72641]\tLoss: 0.6559\tLR: 0.000010\n",
      "Training Epoch: 19 [47040/72641]\tLoss: 0.5881\tLR: 0.000010\n",
      "Training Epoch: 19 [47360/72641]\tLoss: 0.6656\tLR: 0.000010\n",
      "Training Epoch: 19 [47680/72641]\tLoss: 0.6043\tLR: 0.000010\n",
      "Training Epoch: 19 [48000/72641]\tLoss: 0.6389\tLR: 0.000010\n",
      "Training Epoch: 19 [48320/72641]\tLoss: 0.6555\tLR: 0.000010\n",
      "Training Epoch: 19 [48640/72641]\tLoss: 0.6122\tLR: 0.000010\n",
      "Training Epoch: 19 [48960/72641]\tLoss: 0.6246\tLR: 0.000010\n",
      "Training Epoch: 19 [49280/72641]\tLoss: 0.6515\tLR: 0.000010\n",
      "Training Epoch: 19 [49600/72641]\tLoss: 0.6684\tLR: 0.000010\n",
      "Training Epoch: 19 [49920/72641]\tLoss: 0.6458\tLR: 0.000010\n",
      "Training Epoch: 19 [50240/72641]\tLoss: 0.6078\tLR: 0.000010\n",
      "Training Epoch: 19 [50560/72641]\tLoss: 0.6094\tLR: 0.000010\n",
      "Training Epoch: 19 [50880/72641]\tLoss: 0.6184\tLR: 0.000010\n",
      "Training Epoch: 19 [51200/72641]\tLoss: 0.6434\tLR: 0.000010\n",
      "Training Epoch: 19 [51520/72641]\tLoss: 0.6238\tLR: 0.000010\n",
      "Training Epoch: 19 [51840/72641]\tLoss: 0.6400\tLR: 0.000010\n",
      "Training Epoch: 19 [52160/72641]\tLoss: 0.6904\tLR: 0.000010\n",
      "Training Epoch: 19 [52480/72641]\tLoss: 0.6098\tLR: 0.000010\n",
      "Training Epoch: 19 [52800/72641]\tLoss: 0.6062\tLR: 0.000010\n",
      "Training Epoch: 19 [53120/72641]\tLoss: 0.6255\tLR: 0.000010\n",
      "Training Epoch: 19 [53440/72641]\tLoss: 0.6104\tLR: 0.000010\n",
      "Training Epoch: 19 [53760/72641]\tLoss: 0.6525\tLR: 0.000010\n",
      "Training Epoch: 19 [54080/72641]\tLoss: 0.6228\tLR: 0.000010\n",
      "Training Epoch: 19 [54400/72641]\tLoss: 0.6501\tLR: 0.000010\n",
      "Training Epoch: 19 [54720/72641]\tLoss: 0.6101\tLR: 0.000010\n",
      "Training Epoch: 19 [55040/72641]\tLoss: 0.7020\tLR: 0.000010\n",
      "Training Epoch: 19 [55360/72641]\tLoss: 0.6548\tLR: 0.000010\n",
      "Training Epoch: 19 [55680/72641]\tLoss: 0.6194\tLR: 0.000010\n",
      "Training Epoch: 19 [56000/72641]\tLoss: 0.6473\tLR: 0.000010\n",
      "Training Epoch: 19 [56320/72641]\tLoss: 0.5998\tLR: 0.000010\n",
      "Training Epoch: 19 [56640/72641]\tLoss: 0.6336\tLR: 0.000010\n",
      "Training Epoch: 19 [56960/72641]\tLoss: 0.6281\tLR: 0.000010\n",
      "Training Epoch: 19 [57280/72641]\tLoss: 0.5782\tLR: 0.000010\n",
      "Training Epoch: 19 [57600/72641]\tLoss: 0.6943\tLR: 0.000010\n",
      "Training Epoch: 19 [57920/72641]\tLoss: 0.6633\tLR: 0.000010\n",
      "Training Epoch: 19 [58240/72641]\tLoss: 0.6075\tLR: 0.000010\n",
      "Training Epoch: 19 [58560/72641]\tLoss: 0.6048\tLR: 0.000010\n",
      "Training Epoch: 19 [58880/72641]\tLoss: 0.5872\tLR: 0.000010\n",
      "Training Epoch: 19 [59200/72641]\tLoss: 0.6395\tLR: 0.000010\n",
      "Training Epoch: 19 [59520/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 19 [59840/72641]\tLoss: 0.6485\tLR: 0.000010\n",
      "Training Epoch: 19 [60160/72641]\tLoss: 0.5979\tLR: 0.000010\n",
      "Training Epoch: 19 [60480/72641]\tLoss: 0.6301\tLR: 0.000010\n",
      "Training Epoch: 19 [60800/72641]\tLoss: 0.6504\tLR: 0.000010\n",
      "Training Epoch: 19 [61120/72641]\tLoss: 0.6140\tLR: 0.000010\n",
      "Training Epoch: 19 [61440/72641]\tLoss: 0.6117\tLR: 0.000010\n",
      "Training Epoch: 19 [61760/72641]\tLoss: 0.6115\tLR: 0.000010\n",
      "Training Epoch: 19 [62080/72641]\tLoss: 0.6607\tLR: 0.000010\n",
      "Training Epoch: 19 [62400/72641]\tLoss: 0.6078\tLR: 0.000010\n",
      "Training Epoch: 19 [62720/72641]\tLoss: 0.5976\tLR: 0.000010\n",
      "Training Epoch: 19 [63040/72641]\tLoss: 0.6066\tLR: 0.000010\n",
      "Training Epoch: 19 [63360/72641]\tLoss: 0.6499\tLR: 0.000010\n",
      "Training Epoch: 19 [63680/72641]\tLoss: 0.6203\tLR: 0.000010\n",
      "Training Epoch: 19 [64000/72641]\tLoss: 0.6839\tLR: 0.000010\n",
      "Training Epoch: 19 [64320/72641]\tLoss: 0.6241\tLR: 0.000010\n",
      "Training Epoch: 19 [64640/72641]\tLoss: 0.6261\tLR: 0.000010\n",
      "Training Epoch: 19 [64960/72641]\tLoss: 0.6469\tLR: 0.000010\n",
      "Training Epoch: 19 [65280/72641]\tLoss: 0.6046\tLR: 0.000010\n",
      "Training Epoch: 19 [65600/72641]\tLoss: 0.6309\tLR: 0.000010\n",
      "Training Epoch: 19 [65920/72641]\tLoss: 0.6472\tLR: 0.000010\n",
      "Training Epoch: 19 [66240/72641]\tLoss: 0.6703\tLR: 0.000010\n",
      "Training Epoch: 19 [66560/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 19 [66880/72641]\tLoss: 0.6339\tLR: 0.000010\n",
      "Training Epoch: 19 [67200/72641]\tLoss: 0.5978\tLR: 0.000010\n",
      "Training Epoch: 19 [67520/72641]\tLoss: 0.6593\tLR: 0.000010\n",
      "Training Epoch: 19 [67840/72641]\tLoss: 0.6612\tLR: 0.000010\n",
      "Training Epoch: 19 [68160/72641]\tLoss: 0.6296\tLR: 0.000010\n",
      "Training Epoch: 19 [68480/72641]\tLoss: 0.6585\tLR: 0.000010\n",
      "Training Epoch: 19 [68800/72641]\tLoss: 0.6722\tLR: 0.000010\n",
      "Training Epoch: 19 [69120/72641]\tLoss: 0.6430\tLR: 0.000010\n",
      "Training Epoch: 19 [69440/72641]\tLoss: 0.6421\tLR: 0.000010\n",
      "Training Epoch: 19 [69760/72641]\tLoss: 0.6343\tLR: 0.000010\n",
      "Training Epoch: 19 [70080/72641]\tLoss: 0.6612\tLR: 0.000010\n",
      "Training Epoch: 19 [70400/72641]\tLoss: 0.6861\tLR: 0.000010\n",
      "Training Epoch: 19 [70720/72641]\tLoss: 0.6685\tLR: 0.000010\n",
      "Training Epoch: 19 [71040/72641]\tLoss: 0.6074\tLR: 0.000010\n",
      "Training Epoch: 19 [71360/72641]\tLoss: 0.6292\tLR: 0.000010\n",
      "Training Epoch: 19 [71680/72641]\tLoss: 0.6401\tLR: 0.000010\n",
      "Training Epoch: 19 [72000/72641]\tLoss: 0.6142\tLR: 0.000010\n",
      "Training Epoch: 19 [72320/72641]\tLoss: 0.5967\tLR: 0.000010\n",
      "Training Epoch: 19 [72640/72641]\tLoss: 0.6255\tLR: 0.000010\n",
      "Val Result: Acc: 0.1464, C_ACC: 0.7046, DOA: 88.3781, ACC_k: 0.1050\n",
      "ext:0.0, cls:0.566138, coar:0.0, fine:0.0,\n",
      "Training Epoch: 20 [320/72641]\tLoss: 0.6670\tLR: 0.000010\n",
      "Training Epoch: 20 [640/72641]\tLoss: 0.6097\tLR: 0.000010\n",
      "Training Epoch: 20 [960/72641]\tLoss: 0.6498\tLR: 0.000010\n",
      "Training Epoch: 20 [1280/72641]\tLoss: 0.6283\tLR: 0.000010\n",
      "Training Epoch: 20 [1600/72641]\tLoss: 0.6639\tLR: 0.000010\n",
      "Training Epoch: 20 [1920/72641]\tLoss: 0.6197\tLR: 0.000010\n",
      "Training Epoch: 20 [2240/72641]\tLoss: 0.6420\tLR: 0.000010\n",
      "Training Epoch: 20 [2560/72641]\tLoss: 0.6303\tLR: 0.000010\n",
      "Training Epoch: 20 [2880/72641]\tLoss: 0.6277\tLR: 0.000010\n",
      "Training Epoch: 20 [3200/72641]\tLoss: 0.6235\tLR: 0.000010\n",
      "Training Epoch: 20 [3520/72641]\tLoss: 0.6601\tLR: 0.000010\n",
      "Training Epoch: 20 [3840/72641]\tLoss: 0.6304\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [4160/72641]\tLoss: 0.6474\tLR: 0.000010\n",
      "Training Epoch: 20 [4480/72641]\tLoss: 0.6576\tLR: 0.000010\n",
      "Training Epoch: 20 [4800/72641]\tLoss: 0.6246\tLR: 0.000010\n",
      "Training Epoch: 20 [5120/72641]\tLoss: 0.6565\tLR: 0.000010\n",
      "Training Epoch: 20 [5440/72641]\tLoss: 0.6088\tLR: 0.000010\n",
      "Training Epoch: 20 [5760/72641]\tLoss: 0.6405\tLR: 0.000010\n",
      "Training Epoch: 20 [6080/72641]\tLoss: 0.6691\tLR: 0.000010\n",
      "Training Epoch: 20 [6400/72641]\tLoss: 0.6052\tLR: 0.000010\n",
      "Training Epoch: 20 [6720/72641]\tLoss: 0.5663\tLR: 0.000010\n",
      "Training Epoch: 20 [7040/72641]\tLoss: 0.6985\tLR: 0.000010\n",
      "Training Epoch: 20 [7360/72641]\tLoss: 0.6384\tLR: 0.000010\n",
      "Training Epoch: 20 [7680/72641]\tLoss: 0.6402\tLR: 0.000010\n",
      "Training Epoch: 20 [8000/72641]\tLoss: 0.6829\tLR: 0.000010\n",
      "Training Epoch: 20 [8320/72641]\tLoss: 0.6425\tLR: 0.000010\n",
      "Training Epoch: 20 [8640/72641]\tLoss: 0.6406\tLR: 0.000010\n",
      "Training Epoch: 20 [8960/72641]\tLoss: 0.5994\tLR: 0.000010\n",
      "Training Epoch: 20 [9280/72641]\tLoss: 0.5882\tLR: 0.000010\n",
      "Training Epoch: 20 [9600/72641]\tLoss: 0.6452\tLR: 0.000010\n",
      "Training Epoch: 20 [9920/72641]\tLoss: 0.6721\tLR: 0.000010\n",
      "Training Epoch: 20 [10240/72641]\tLoss: 0.6151\tLR: 0.000010\n",
      "Training Epoch: 20 [10560/72641]\tLoss: 0.6192\tLR: 0.000010\n",
      "Training Epoch: 20 [10880/72641]\tLoss: 0.6263\tLR: 0.000010\n",
      "Training Epoch: 20 [11200/72641]\tLoss: 0.6396\tLR: 0.000010\n",
      "Training Epoch: 20 [11520/72641]\tLoss: 0.6153\tLR: 0.000010\n",
      "Training Epoch: 20 [11840/72641]\tLoss: 0.6267\tLR: 0.000010\n",
      "Training Epoch: 20 [12160/72641]\tLoss: 0.6573\tLR: 0.000010\n",
      "Training Epoch: 20 [12480/72641]\tLoss: 0.6375\tLR: 0.000010\n",
      "Training Epoch: 20 [12800/72641]\tLoss: 0.6089\tLR: 0.000010\n",
      "Training Epoch: 20 [13120/72641]\tLoss: 0.6124\tLR: 0.000010\n",
      "Training Epoch: 20 [13440/72641]\tLoss: 0.6082\tLR: 0.000010\n",
      "Training Epoch: 20 [13760/72641]\tLoss: 0.6677\tLR: 0.000010\n",
      "Training Epoch: 20 [14080/72641]\tLoss: 0.6262\tLR: 0.000010\n",
      "Training Epoch: 20 [14400/72641]\tLoss: 0.6409\tLR: 0.000010\n",
      "Training Epoch: 20 [14720/72641]\tLoss: 0.6388\tLR: 0.000010\n",
      "Training Epoch: 20 [15040/72641]\tLoss: 0.6540\tLR: 0.000010\n",
      "Training Epoch: 20 [15360/72641]\tLoss: 0.6485\tLR: 0.000010\n",
      "Training Epoch: 20 [15680/72641]\tLoss: 0.6294\tLR: 0.000010\n",
      "Training Epoch: 20 [16000/72641]\tLoss: 0.6264\tLR: 0.000010\n",
      "Training Epoch: 20 [16320/72641]\tLoss: 0.6135\tLR: 0.000010\n",
      "Training Epoch: 20 [16640/72641]\tLoss: 0.6561\tLR: 0.000010\n",
      "Training Epoch: 20 [16960/72641]\tLoss: 0.6682\tLR: 0.000010\n",
      "Training Epoch: 20 [17280/72641]\tLoss: 0.6390\tLR: 0.000010\n",
      "Training Epoch: 20 [17600/72641]\tLoss: 0.6475\tLR: 0.000010\n",
      "Training Epoch: 20 [17920/72641]\tLoss: 0.6512\tLR: 0.000010\n",
      "Training Epoch: 20 [18240/72641]\tLoss: 0.6704\tLR: 0.000010\n",
      "Training Epoch: 20 [18560/72641]\tLoss: 0.6267\tLR: 0.000010\n",
      "Training Epoch: 20 [18880/72641]\tLoss: 0.6124\tLR: 0.000010\n",
      "Training Epoch: 20 [19200/72641]\tLoss: 0.6558\tLR: 0.000010\n",
      "Training Epoch: 20 [19520/72641]\tLoss: 0.6282\tLR: 0.000010\n",
      "Training Epoch: 20 [19840/72641]\tLoss: 0.6167\tLR: 0.000010\n",
      "Training Epoch: 20 [20160/72641]\tLoss: 0.6221\tLR: 0.000010\n",
      "Training Epoch: 20 [20480/72641]\tLoss: 0.6094\tLR: 0.000010\n",
      "Training Epoch: 20 [20800/72641]\tLoss: 0.6498\tLR: 0.000010\n",
      "Training Epoch: 20 [21120/72641]\tLoss: 0.6405\tLR: 0.000010\n",
      "Training Epoch: 20 [21440/72641]\tLoss: 0.5891\tLR: 0.000010\n",
      "Training Epoch: 20 [21760/72641]\tLoss: 0.6329\tLR: 0.000010\n",
      "Training Epoch: 20 [22080/72641]\tLoss: 0.6500\tLR: 0.000010\n",
      "Training Epoch: 20 [22400/72641]\tLoss: 0.6586\tLR: 0.000010\n",
      "Training Epoch: 20 [22720/72641]\tLoss: 0.6172\tLR: 0.000010\n",
      "Training Epoch: 20 [23040/72641]\tLoss: 0.6256\tLR: 0.000010\n",
      "Training Epoch: 20 [23360/72641]\tLoss: 0.6069\tLR: 0.000010\n",
      "Training Epoch: 20 [23680/72641]\tLoss: 0.6307\tLR: 0.000010\n",
      "Training Epoch: 20 [24000/72641]\tLoss: 0.6205\tLR: 0.000010\n",
      "Training Epoch: 20 [24320/72641]\tLoss: 0.5952\tLR: 0.000010\n",
      "Training Epoch: 20 [24640/72641]\tLoss: 0.5998\tLR: 0.000010\n",
      "Training Epoch: 20 [24960/72641]\tLoss: 0.6489\tLR: 0.000010\n",
      "Training Epoch: 20 [25280/72641]\tLoss: 0.5835\tLR: 0.000010\n",
      "Training Epoch: 20 [25600/72641]\tLoss: 0.6688\tLR: 0.000010\n",
      "Training Epoch: 20 [25920/72641]\tLoss: 0.6352\tLR: 0.000010\n",
      "Training Epoch: 20 [26240/72641]\tLoss: 0.6455\tLR: 0.000010\n",
      "Training Epoch: 20 [26560/72641]\tLoss: 0.5947\tLR: 0.000010\n",
      "Training Epoch: 20 [26880/72641]\tLoss: 0.6326\tLR: 0.000010\n",
      "Training Epoch: 20 [27200/72641]\tLoss: 0.6307\tLR: 0.000010\n",
      "Training Epoch: 20 [27520/72641]\tLoss: 0.6437\tLR: 0.000010\n",
      "Training Epoch: 20 [27840/72641]\tLoss: 0.6120\tLR: 0.000010\n",
      "Training Epoch: 20 [28160/72641]\tLoss: 0.6362\tLR: 0.000010\n",
      "Training Epoch: 20 [28480/72641]\tLoss: 0.6080\tLR: 0.000010\n",
      "Training Epoch: 20 [28800/72641]\tLoss: 0.6215\tLR: 0.000010\n",
      "Training Epoch: 20 [29120/72641]\tLoss: 0.6750\tLR: 0.000010\n",
      "Training Epoch: 20 [29440/72641]\tLoss: 0.6077\tLR: 0.000010\n",
      "Training Epoch: 20 [29760/72641]\tLoss: 0.6445\tLR: 0.000010\n",
      "Training Epoch: 20 [30080/72641]\tLoss: 0.6345\tLR: 0.000010\n",
      "Training Epoch: 20 [30400/72641]\tLoss: 0.6637\tLR: 0.000010\n",
      "Training Epoch: 20 [30720/72641]\tLoss: 0.6408\tLR: 0.000010\n",
      "Training Epoch: 20 [31040/72641]\tLoss: 0.6378\tLR: 0.000010\n",
      "Training Epoch: 20 [31360/72641]\tLoss: 0.6324\tLR: 0.000010\n",
      "Training Epoch: 20 [31680/72641]\tLoss: 0.6594\tLR: 0.000010\n",
      "Training Epoch: 20 [32000/72641]\tLoss: 0.6063\tLR: 0.000010\n",
      "Training Epoch: 20 [32320/72641]\tLoss: 0.6340\tLR: 0.000010\n",
      "Training Epoch: 20 [32640/72641]\tLoss: 0.6456\tLR: 0.000010\n",
      "Training Epoch: 20 [32960/72641]\tLoss: 0.6568\tLR: 0.000010\n",
      "Training Epoch: 20 [33280/72641]\tLoss: 0.6196\tLR: 0.000010\n",
      "Training Epoch: 20 [33600/72641]\tLoss: 0.6090\tLR: 0.000010\n",
      "Training Epoch: 20 [33920/72641]\tLoss: 0.5823\tLR: 0.000010\n",
      "Training Epoch: 20 [34240/72641]\tLoss: 0.6090\tLR: 0.000010\n",
      "Training Epoch: 20 [34560/72641]\tLoss: 0.6535\tLR: 0.000010\n",
      "Training Epoch: 20 [34880/72641]\tLoss: 0.7150\tLR: 0.000010\n",
      "Training Epoch: 20 [35200/72641]\tLoss: 0.6200\tLR: 0.000010\n",
      "Training Epoch: 20 [35520/72641]\tLoss: 0.6292\tLR: 0.000010\n",
      "Training Epoch: 20 [35840/72641]\tLoss: 0.6812\tLR: 0.000010\n",
      "Training Epoch: 20 [36160/72641]\tLoss: 0.6638\tLR: 0.000010\n",
      "Training Epoch: 20 [36480/72641]\tLoss: 0.6086\tLR: 0.000010\n",
      "Training Epoch: 20 [36800/72641]\tLoss: 0.6324\tLR: 0.000010\n",
      "Training Epoch: 20 [37120/72641]\tLoss: 0.6890\tLR: 0.000010\n",
      "Training Epoch: 20 [37440/72641]\tLoss: 0.6690\tLR: 0.000010\n",
      "Training Epoch: 20 [37760/72641]\tLoss: 0.5954\tLR: 0.000010\n",
      "Training Epoch: 20 [38080/72641]\tLoss: 0.6367\tLR: 0.000010\n",
      "Training Epoch: 20 [38400/72641]\tLoss: 0.6916\tLR: 0.000010\n",
      "Training Epoch: 20 [38720/72641]\tLoss: 0.6097\tLR: 0.000010\n",
      "Training Epoch: 20 [39040/72641]\tLoss: 0.6599\tLR: 0.000010\n",
      "Training Epoch: 20 [39360/72641]\tLoss: 0.6361\tLR: 0.000010\n",
      "Training Epoch: 20 [39680/72641]\tLoss: 0.6077\tLR: 0.000010\n",
      "Training Epoch: 20 [40000/72641]\tLoss: 0.6722\tLR: 0.000010\n",
      "Training Epoch: 20 [40320/72641]\tLoss: 0.6249\tLR: 0.000010\n",
      "Training Epoch: 20 [40640/72641]\tLoss: 0.6063\tLR: 0.000010\n",
      "Training Epoch: 20 [40960/72641]\tLoss: 0.6594\tLR: 0.000010\n",
      "Training Epoch: 20 [41280/72641]\tLoss: 0.6437\tLR: 0.000010\n",
      "Training Epoch: 20 [41600/72641]\tLoss: 0.5760\tLR: 0.000010\n",
      "Training Epoch: 20 [41920/72641]\tLoss: 0.6587\tLR: 0.000010\n",
      "Training Epoch: 20 [42240/72641]\tLoss: 0.6212\tLR: 0.000010\n",
      "Training Epoch: 20 [42560/72641]\tLoss: 0.6221\tLR: 0.000010\n",
      "Training Epoch: 20 [42880/72641]\tLoss: 0.7156\tLR: 0.000010\n",
      "Training Epoch: 20 [43200/72641]\tLoss: 0.6633\tLR: 0.000010\n",
      "Training Epoch: 20 [43520/72641]\tLoss: 0.5928\tLR: 0.000010\n",
      "Training Epoch: 20 [43840/72641]\tLoss: 0.6260\tLR: 0.000010\n",
      "Training Epoch: 20 [44160/72641]\tLoss: 0.6640\tLR: 0.000010\n",
      "Training Epoch: 20 [44480/72641]\tLoss: 0.6487\tLR: 0.000010\n",
      "Training Epoch: 20 [44800/72641]\tLoss: 0.5870\tLR: 0.000010\n",
      "Training Epoch: 20 [45120/72641]\tLoss: 0.6021\tLR: 0.000010\n",
      "Training Epoch: 20 [45440/72641]\tLoss: 0.6262\tLR: 0.000010\n",
      "Training Epoch: 20 [45760/72641]\tLoss: 0.6389\tLR: 0.000010\n",
      "Training Epoch: 20 [46080/72641]\tLoss: 0.6590\tLR: 0.000010\n",
      "Training Epoch: 20 [46400/72641]\tLoss: 0.6199\tLR: 0.000010\n",
      "Training Epoch: 20 [46720/72641]\tLoss: 0.6605\tLR: 0.000010\n",
      "Training Epoch: 20 [47040/72641]\tLoss: 0.6444\tLR: 0.000010\n",
      "Training Epoch: 20 [47360/72641]\tLoss: 0.6612\tLR: 0.000010\n",
      "Training Epoch: 20 [47680/72641]\tLoss: 0.6067\tLR: 0.000010\n",
      "Training Epoch: 20 [48000/72641]\tLoss: 0.6202\tLR: 0.000010\n",
      "Training Epoch: 20 [48320/72641]\tLoss: 0.6546\tLR: 0.000010\n",
      "Training Epoch: 20 [48640/72641]\tLoss: 0.6545\tLR: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [48960/72641]\tLoss: 0.6296\tLR: 0.000010\n",
      "Training Epoch: 20 [49280/72641]\tLoss: 0.6324\tLR: 0.000010\n",
      "Training Epoch: 20 [49600/72641]\tLoss: 0.6500\tLR: 0.000010\n",
      "Training Epoch: 20 [49920/72641]\tLoss: 0.6521\tLR: 0.000010\n",
      "Training Epoch: 20 [50240/72641]\tLoss: 0.6045\tLR: 0.000010\n",
      "Training Epoch: 20 [50560/72641]\tLoss: 0.6273\tLR: 0.000010\n",
      "Training Epoch: 20 [50880/72641]\tLoss: 0.6625\tLR: 0.000010\n",
      "Training Epoch: 20 [51200/72641]\tLoss: 0.6698\tLR: 0.000010\n",
      "Training Epoch: 20 [51520/72641]\tLoss: 0.5792\tLR: 0.000010\n",
      "Training Epoch: 20 [51840/72641]\tLoss: 0.6286\tLR: 0.000010\n",
      "Training Epoch: 20 [52160/72641]\tLoss: 0.6379\tLR: 0.000010\n",
      "Training Epoch: 20 [52480/72641]\tLoss: 0.6359\tLR: 0.000010\n",
      "Training Epoch: 20 [52800/72641]\tLoss: 0.6302\tLR: 0.000010\n",
      "Training Epoch: 20 [53120/72641]\tLoss: 0.6154\tLR: 0.000010\n",
      "Training Epoch: 20 [53440/72641]\tLoss: 0.6228\tLR: 0.000010\n",
      "Training Epoch: 20 [53760/72641]\tLoss: 0.6657\tLR: 0.000010\n",
      "Training Epoch: 20 [54080/72641]\tLoss: 0.6394\tLR: 0.000010\n",
      "Training Epoch: 20 [54400/72641]\tLoss: 0.6343\tLR: 0.000010\n",
      "Training Epoch: 20 [54720/72641]\tLoss: 0.6380\tLR: 0.000010\n",
      "Training Epoch: 20 [55040/72641]\tLoss: 0.6431\tLR: 0.000010\n",
      "Training Epoch: 20 [55360/72641]\tLoss: 0.5926\tLR: 0.000010\n",
      "Training Epoch: 20 [55680/72641]\tLoss: 0.6411\tLR: 0.000010\n",
      "Training Epoch: 20 [56000/72641]\tLoss: 0.6081\tLR: 0.000010\n",
      "Training Epoch: 20 [56320/72641]\tLoss: 0.5787\tLR: 0.000010\n",
      "Training Epoch: 20 [56640/72641]\tLoss: 0.6854\tLR: 0.000010\n",
      "Training Epoch: 20 [56960/72641]\tLoss: 0.6909\tLR: 0.000010\n",
      "Training Epoch: 20 [57280/72641]\tLoss: 0.6389\tLR: 0.000010\n",
      "Training Epoch: 20 [57600/72641]\tLoss: 0.6509\tLR: 0.000010\n",
      "Training Epoch: 20 [57920/72641]\tLoss: 0.6339\tLR: 0.000010\n",
      "Training Epoch: 20 [58240/72641]\tLoss: 0.6002\tLR: 0.000010\n",
      "Training Epoch: 20 [58560/72641]\tLoss: 0.6211\tLR: 0.000010\n",
      "Training Epoch: 20 [58880/72641]\tLoss: 0.6406\tLR: 0.000010\n",
      "Training Epoch: 20 [59200/72641]\tLoss: 0.6510\tLR: 0.000010\n",
      "Training Epoch: 20 [59520/72641]\tLoss: 0.6267\tLR: 0.000010\n",
      "Training Epoch: 20 [59840/72641]\tLoss: 0.6189\tLR: 0.000010\n",
      "Training Epoch: 20 [60160/72641]\tLoss: 0.6384\tLR: 0.000010\n",
      "Training Epoch: 20 [60480/72641]\tLoss: 0.6380\tLR: 0.000010\n",
      "Training Epoch: 20 [60800/72641]\tLoss: 0.6206\tLR: 0.000010\n",
      "Training Epoch: 20 [61120/72641]\tLoss: 0.6287\tLR: 0.000010\n",
      "Training Epoch: 20 [61440/72641]\tLoss: 0.6495\tLR: 0.000010\n",
      "Training Epoch: 20 [61760/72641]\tLoss: 0.6114\tLR: 0.000010\n",
      "Training Epoch: 20 [62080/72641]\tLoss: 0.6929\tLR: 0.000010\n",
      "Training Epoch: 20 [62400/72641]\tLoss: 0.5718\tLR: 0.000010\n",
      "Training Epoch: 20 [62720/72641]\tLoss: 0.6254\tLR: 0.000010\n",
      "Training Epoch: 20 [63040/72641]\tLoss: 0.6453\tLR: 0.000010\n",
      "Training Epoch: 20 [63360/72641]\tLoss: 0.6511\tLR: 0.000010\n",
      "Training Epoch: 20 [63680/72641]\tLoss: 0.6207\tLR: 0.000010\n",
      "Training Epoch: 20 [64000/72641]\tLoss: 0.6588\tLR: 0.000010\n",
      "Training Epoch: 20 [64320/72641]\tLoss: 0.5964\tLR: 0.000010\n",
      "Training Epoch: 20 [64640/72641]\tLoss: 0.6239\tLR: 0.000010\n",
      "Training Epoch: 20 [64960/72641]\tLoss: 0.6311\tLR: 0.000010\n",
      "Training Epoch: 20 [65280/72641]\tLoss: 0.6552\tLR: 0.000010\n",
      "Training Epoch: 20 [65600/72641]\tLoss: 0.6448\tLR: 0.000010\n",
      "Training Epoch: 20 [65920/72641]\tLoss: 0.6486\tLR: 0.000010\n",
      "Training Epoch: 20 [66240/72641]\tLoss: 0.6803\tLR: 0.000010\n",
      "Training Epoch: 20 [66560/72641]\tLoss: 0.5998\tLR: 0.000010\n",
      "Training Epoch: 20 [66880/72641]\tLoss: 0.5935\tLR: 0.000010\n",
      "Training Epoch: 20 [67200/72641]\tLoss: 0.6118\tLR: 0.000010\n",
      "Training Epoch: 20 [67520/72641]\tLoss: 0.6753\tLR: 0.000010\n",
      "Training Epoch: 20 [67840/72641]\tLoss: 0.6627\tLR: 0.000010\n",
      "Training Epoch: 20 [68160/72641]\tLoss: 0.6447\tLR: 0.000010\n",
      "Training Epoch: 20 [68480/72641]\tLoss: 0.6184\tLR: 0.000010\n",
      "Training Epoch: 20 [68800/72641]\tLoss: 0.6640\tLR: 0.000010\n",
      "Training Epoch: 20 [69120/72641]\tLoss: 0.6669\tLR: 0.000010\n",
      "Training Epoch: 20 [69440/72641]\tLoss: 0.6228\tLR: 0.000010\n",
      "Training Epoch: 20 [69760/72641]\tLoss: 0.6231\tLR: 0.000010\n",
      "Training Epoch: 20 [70080/72641]\tLoss: 0.6565\tLR: 0.000010\n",
      "Training Epoch: 20 [70400/72641]\tLoss: 0.6704\tLR: 0.000010\n",
      "Training Epoch: 20 [70720/72641]\tLoss: 0.6262\tLR: 0.000010\n",
      "Training Epoch: 20 [71040/72641]\tLoss: 0.6234\tLR: 0.000010\n",
      "Training Epoch: 20 [71360/72641]\tLoss: 0.6090\tLR: 0.000010\n",
      "Training Epoch: 20 [71680/72641]\tLoss: 0.6408\tLR: 0.000010\n",
      "Training Epoch: 20 [72000/72641]\tLoss: 0.6257\tLR: 0.000010\n",
      "Training Epoch: 20 [72320/72641]\tLoss: 0.6512\tLR: 0.000010\n",
      "Training Epoch: 20 [72640/72641]\tLoss: 0.5724\tLR: 0.000010\n",
      "Val Result: Acc: 0.1427, C_ACC: 0.6939, DOA: 89.5559, ACC_k: 0.0971\n",
      "ext:0.0, cls:0.585126, coar:0.0, fine:0.0,\n",
      "Training Epoch: 21 [320/72641]\tLoss: 0.6592\tLR: 0.000001\n",
      "Training Epoch: 21 [640/72641]\tLoss: 0.6438\tLR: 0.000001\n",
      "Training Epoch: 21 [960/72641]\tLoss: 0.6009\tLR: 0.000001\n",
      "Training Epoch: 21 [1280/72641]\tLoss: 0.6419\tLR: 0.000001\n",
      "Training Epoch: 21 [1600/72641]\tLoss: 0.6006\tLR: 0.000001\n",
      "Training Epoch: 21 [1920/72641]\tLoss: 0.6575\tLR: 0.000001\n",
      "Training Epoch: 21 [2240/72641]\tLoss: 0.6356\tLR: 0.000001\n",
      "Training Epoch: 21 [2560/72641]\tLoss: 0.5899\tLR: 0.000001\n",
      "Training Epoch: 21 [2880/72641]\tLoss: 0.5880\tLR: 0.000001\n",
      "Training Epoch: 21 [3200/72641]\tLoss: 0.6456\tLR: 0.000001\n",
      "Training Epoch: 21 [3520/72641]\tLoss: 0.6393\tLR: 0.000001\n",
      "Training Epoch: 21 [3840/72641]\tLoss: 0.6171\tLR: 0.000001\n",
      "Training Epoch: 21 [4160/72641]\tLoss: 0.6672\tLR: 0.000001\n",
      "Training Epoch: 21 [4480/72641]\tLoss: 0.6143\tLR: 0.000001\n",
      "Training Epoch: 21 [4800/72641]\tLoss: 0.6176\tLR: 0.000001\n",
      "Training Epoch: 21 [5120/72641]\tLoss: 0.5786\tLR: 0.000001\n",
      "Training Epoch: 21 [5440/72641]\tLoss: 0.6678\tLR: 0.000001\n",
      "Training Epoch: 21 [5760/72641]\tLoss: 0.6190\tLR: 0.000001\n",
      "Training Epoch: 21 [6080/72641]\tLoss: 0.5919\tLR: 0.000001\n",
      "Training Epoch: 21 [6400/72641]\tLoss: 0.5930\tLR: 0.000001\n",
      "Training Epoch: 21 [6720/72641]\tLoss: 0.6388\tLR: 0.000001\n",
      "Training Epoch: 21 [7040/72641]\tLoss: 0.6426\tLR: 0.000001\n",
      "Training Epoch: 21 [7360/72641]\tLoss: 0.6360\tLR: 0.000001\n",
      "Training Epoch: 21 [7680/72641]\tLoss: 0.6306\tLR: 0.000001\n",
      "Training Epoch: 21 [8000/72641]\tLoss: 0.6391\tLR: 0.000001\n",
      "Training Epoch: 21 [8320/72641]\tLoss: 0.6239\tLR: 0.000001\n",
      "Training Epoch: 21 [8640/72641]\tLoss: 0.6292\tLR: 0.000001\n",
      "Training Epoch: 21 [8960/72641]\tLoss: 0.6112\tLR: 0.000001\n",
      "Training Epoch: 21 [9280/72641]\tLoss: 0.6322\tLR: 0.000001\n",
      "Training Epoch: 21 [9600/72641]\tLoss: 0.6235\tLR: 0.000001\n",
      "Training Epoch: 21 [9920/72641]\tLoss: 0.6212\tLR: 0.000001\n",
      "Training Epoch: 21 [10240/72641]\tLoss: 0.6694\tLR: 0.000001\n",
      "Training Epoch: 21 [10560/72641]\tLoss: 0.5686\tLR: 0.000001\n",
      "Training Epoch: 21 [10880/72641]\tLoss: 0.6709\tLR: 0.000001\n",
      "Training Epoch: 21 [11200/72641]\tLoss: 0.6337\tLR: 0.000001\n",
      "Training Epoch: 21 [11520/72641]\tLoss: 0.6285\tLR: 0.000001\n",
      "Training Epoch: 21 [11840/72641]\tLoss: 0.6085\tLR: 0.000001\n",
      "Training Epoch: 21 [12160/72641]\tLoss: 0.6383\tLR: 0.000001\n",
      "Training Epoch: 21 [12480/72641]\tLoss: 0.6204\tLR: 0.000001\n",
      "Training Epoch: 21 [12800/72641]\tLoss: 0.6758\tLR: 0.000001\n",
      "Training Epoch: 21 [13120/72641]\tLoss: 0.6420\tLR: 0.000001\n",
      "Training Epoch: 21 [13440/72641]\tLoss: 0.6245\tLR: 0.000001\n",
      "Training Epoch: 21 [13760/72641]\tLoss: 0.6105\tLR: 0.000001\n",
      "Training Epoch: 21 [14080/72641]\tLoss: 0.5993\tLR: 0.000001\n",
      "Training Epoch: 21 [14400/72641]\tLoss: 0.6732\tLR: 0.000001\n",
      "Training Epoch: 21 [14720/72641]\tLoss: 0.6372\tLR: 0.000001\n",
      "Training Epoch: 21 [15040/72641]\tLoss: 0.6326\tLR: 0.000001\n",
      "Training Epoch: 21 [15360/72641]\tLoss: 0.6655\tLR: 0.000001\n",
      "Training Epoch: 21 [15680/72641]\tLoss: 0.6509\tLR: 0.000001\n",
      "Training Epoch: 21 [16000/72641]\tLoss: 0.6509\tLR: 0.000001\n",
      "Training Epoch: 21 [16320/72641]\tLoss: 0.6481\tLR: 0.000001\n",
      "Training Epoch: 21 [16640/72641]\tLoss: 0.6329\tLR: 0.000001\n",
      "Training Epoch: 21 [16960/72641]\tLoss: 0.6376\tLR: 0.000001\n",
      "Training Epoch: 21 [17280/72641]\tLoss: 0.6254\tLR: 0.000001\n",
      "Training Epoch: 21 [17600/72641]\tLoss: 0.5852\tLR: 0.000001\n",
      "Training Epoch: 21 [17920/72641]\tLoss: 0.6286\tLR: 0.000001\n",
      "Training Epoch: 21 [18240/72641]\tLoss: 0.6169\tLR: 0.000001\n",
      "Training Epoch: 21 [18560/72641]\tLoss: 0.6710\tLR: 0.000001\n",
      "Training Epoch: 21 [18880/72641]\tLoss: 0.6023\tLR: 0.000001\n",
      "Training Epoch: 21 [19200/72641]\tLoss: 0.6653\tLR: 0.000001\n",
      "Training Epoch: 21 [19520/72641]\tLoss: 0.6363\tLR: 0.000001\n",
      "Training Epoch: 21 [19840/72641]\tLoss: 0.6082\tLR: 0.000001\n",
      "Training Epoch: 21 [20160/72641]\tLoss: 0.5918\tLR: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [20480/72641]\tLoss: 0.6188\tLR: 0.000001\n",
      "Training Epoch: 21 [20800/72641]\tLoss: 0.6342\tLR: 0.000001\n",
      "Training Epoch: 21 [21120/72641]\tLoss: 0.6381\tLR: 0.000001\n",
      "Training Epoch: 21 [21440/72641]\tLoss: 0.6702\tLR: 0.000001\n",
      "Training Epoch: 21 [21760/72641]\tLoss: 0.6418\tLR: 0.000001\n",
      "Training Epoch: 21 [22080/72641]\tLoss: 0.6330\tLR: 0.000001\n",
      "Training Epoch: 21 [22400/72641]\tLoss: 0.5781\tLR: 0.000001\n",
      "Training Epoch: 21 [22720/72641]\tLoss: 0.5985\tLR: 0.000001\n",
      "Training Epoch: 21 [23040/72641]\tLoss: 0.6280\tLR: 0.000001\n",
      "Training Epoch: 21 [23360/72641]\tLoss: 0.6539\tLR: 0.000001\n",
      "Training Epoch: 21 [23680/72641]\tLoss: 0.6579\tLR: 0.000001\n",
      "Training Epoch: 21 [24000/72641]\tLoss: 0.6808\tLR: 0.000001\n",
      "Training Epoch: 21 [24320/72641]\tLoss: 0.6093\tLR: 0.000001\n",
      "Training Epoch: 21 [24640/72641]\tLoss: 0.5895\tLR: 0.000001\n",
      "Training Epoch: 21 [24960/72641]\tLoss: 0.6582\tLR: 0.000001\n",
      "Training Epoch: 21 [25280/72641]\tLoss: 0.5723\tLR: 0.000001\n",
      "Training Epoch: 21 [25600/72641]\tLoss: 0.6135\tLR: 0.000001\n",
      "Training Epoch: 21 [25920/72641]\tLoss: 0.6130\tLR: 0.000001\n",
      "Training Epoch: 21 [26240/72641]\tLoss: 0.6258\tLR: 0.000001\n",
      "Training Epoch: 21 [26560/72641]\tLoss: 0.6337\tLR: 0.000001\n",
      "Training Epoch: 21 [26880/72641]\tLoss: 0.6323\tLR: 0.000001\n",
      "Training Epoch: 21 [27200/72641]\tLoss: 0.6254\tLR: 0.000001\n",
      "Training Epoch: 21 [27520/72641]\tLoss: 0.6524\tLR: 0.000001\n",
      "Training Epoch: 21 [27840/72641]\tLoss: 0.6279\tLR: 0.000001\n",
      "Training Epoch: 21 [28160/72641]\tLoss: 0.6382\tLR: 0.000001\n",
      "Training Epoch: 21 [28480/72641]\tLoss: 0.6206\tLR: 0.000001\n",
      "Training Epoch: 21 [28800/72641]\tLoss: 0.6339\tLR: 0.000001\n",
      "Training Epoch: 21 [29120/72641]\tLoss: 0.6460\tLR: 0.000001\n",
      "Training Epoch: 21 [29440/72641]\tLoss: 0.6206\tLR: 0.000001\n",
      "Training Epoch: 21 [29760/72641]\tLoss: 0.6373\tLR: 0.000001\n",
      "Training Epoch: 21 [30080/72641]\tLoss: 0.6037\tLR: 0.000001\n",
      "Training Epoch: 21 [30400/72641]\tLoss: 0.6711\tLR: 0.000001\n",
      "Training Epoch: 21 [30720/72641]\tLoss: 0.6610\tLR: 0.000001\n",
      "Training Epoch: 21 [31040/72641]\tLoss: 0.6125\tLR: 0.000001\n",
      "Training Epoch: 21 [31360/72641]\tLoss: 0.5968\tLR: 0.000001\n",
      "Training Epoch: 21 [31680/72641]\tLoss: 0.6052\tLR: 0.000001\n",
      "Training Epoch: 21 [32000/72641]\tLoss: 0.6512\tLR: 0.000001\n",
      "Training Epoch: 21 [32320/72641]\tLoss: 0.6022\tLR: 0.000001\n",
      "Training Epoch: 21 [32640/72641]\tLoss: 0.6016\tLR: 0.000001\n",
      "Training Epoch: 21 [32960/72641]\tLoss: 0.6613\tLR: 0.000001\n",
      "Training Epoch: 21 [33280/72641]\tLoss: 0.6421\tLR: 0.000001\n",
      "Training Epoch: 21 [33600/72641]\tLoss: 0.6131\tLR: 0.000001\n",
      "Training Epoch: 21 [33920/72641]\tLoss: 0.6460\tLR: 0.000001\n",
      "Training Epoch: 21 [34240/72641]\tLoss: 0.6021\tLR: 0.000001\n",
      "Training Epoch: 21 [34560/72641]\tLoss: 0.6752\tLR: 0.000001\n",
      "Training Epoch: 21 [34880/72641]\tLoss: 0.6267\tLR: 0.000001\n",
      "Training Epoch: 21 [35200/72641]\tLoss: 0.6328\tLR: 0.000001\n",
      "Training Epoch: 21 [35520/72641]\tLoss: 0.6240\tLR: 0.000001\n",
      "Training Epoch: 21 [35840/72641]\tLoss: 0.6733\tLR: 0.000001\n",
      "Training Epoch: 21 [36160/72641]\tLoss: 0.6326\tLR: 0.000001\n",
      "Training Epoch: 21 [36480/72641]\tLoss: 0.5780\tLR: 0.000001\n",
      "Training Epoch: 21 [36800/72641]\tLoss: 0.6387\tLR: 0.000001\n",
      "Training Epoch: 21 [37120/72641]\tLoss: 0.6117\tLR: 0.000001\n",
      "Training Epoch: 21 [37440/72641]\tLoss: 0.6306\tLR: 0.000001\n",
      "Training Epoch: 21 [37760/72641]\tLoss: 0.6166\tLR: 0.000001\n",
      "Training Epoch: 21 [38080/72641]\tLoss: 0.5956\tLR: 0.000001\n",
      "Training Epoch: 21 [38400/72641]\tLoss: 0.6769\tLR: 0.000001\n",
      "Training Epoch: 21 [38720/72641]\tLoss: 0.5858\tLR: 0.000001\n",
      "Training Epoch: 21 [39040/72641]\tLoss: 0.6493\tLR: 0.000001\n",
      "Training Epoch: 21 [39360/72641]\tLoss: 0.6559\tLR: 0.000001\n",
      "Training Epoch: 21 [39680/72641]\tLoss: 0.6091\tLR: 0.000001\n",
      "Training Epoch: 21 [40000/72641]\tLoss: 0.6526\tLR: 0.000001\n",
      "Training Epoch: 21 [40320/72641]\tLoss: 0.6302\tLR: 0.000001\n",
      "Training Epoch: 21 [40640/72641]\tLoss: 0.6476\tLR: 0.000001\n",
      "Training Epoch: 21 [40960/72641]\tLoss: 0.6191\tLR: 0.000001\n",
      "Training Epoch: 21 [41280/72641]\tLoss: 0.6266\tLR: 0.000001\n",
      "Training Epoch: 21 [41600/72641]\tLoss: 0.5755\tLR: 0.000001\n",
      "Training Epoch: 21 [41920/72641]\tLoss: 0.6387\tLR: 0.000001\n",
      "Training Epoch: 21 [42240/72641]\tLoss: 0.5816\tLR: 0.000001\n",
      "Training Epoch: 21 [42560/72641]\tLoss: 0.5912\tLR: 0.000001\n",
      "Training Epoch: 21 [42880/72641]\tLoss: 0.6496\tLR: 0.000001\n",
      "Training Epoch: 21 [43200/72641]\tLoss: 0.6311\tLR: 0.000001\n",
      "Training Epoch: 21 [43520/72641]\tLoss: 0.5710\tLR: 0.000001\n",
      "Training Epoch: 21 [43840/72641]\tLoss: 0.6072\tLR: 0.000001\n",
      "Training Epoch: 21 [44160/72641]\tLoss: 0.6332\tLR: 0.000001\n",
      "Training Epoch: 21 [44480/72641]\tLoss: 0.6141\tLR: 0.000001\n",
      "Training Epoch: 21 [44800/72641]\tLoss: 0.6383\tLR: 0.000001\n",
      "Training Epoch: 21 [45120/72641]\tLoss: 0.5882\tLR: 0.000001\n",
      "Training Epoch: 21 [45440/72641]\tLoss: 0.6369\tLR: 0.000001\n",
      "Training Epoch: 21 [45760/72641]\tLoss: 0.6333\tLR: 0.000001\n",
      "Training Epoch: 21 [46080/72641]\tLoss: 0.6182\tLR: 0.000001\n",
      "Training Epoch: 21 [46400/72641]\tLoss: 0.6485\tLR: 0.000001\n",
      "Training Epoch: 21 [46720/72641]\tLoss: 0.6276\tLR: 0.000001\n",
      "Training Epoch: 21 [47040/72641]\tLoss: 0.6491\tLR: 0.000001\n",
      "Training Epoch: 21 [47360/72641]\tLoss: 0.6242\tLR: 0.000001\n",
      "Training Epoch: 21 [47680/72641]\tLoss: 0.6133\tLR: 0.000001\n",
      "Training Epoch: 21 [48000/72641]\tLoss: 0.6105\tLR: 0.000001\n",
      "Training Epoch: 21 [48320/72641]\tLoss: 0.6447\tLR: 0.000001\n",
      "Training Epoch: 21 [48640/72641]\tLoss: 0.6141\tLR: 0.000001\n",
      "Training Epoch: 21 [48960/72641]\tLoss: 0.6187\tLR: 0.000001\n",
      "Training Epoch: 21 [49280/72641]\tLoss: 0.6043\tLR: 0.000001\n",
      "Training Epoch: 21 [49600/72641]\tLoss: 0.6121\tLR: 0.000001\n",
      "Training Epoch: 21 [49920/72641]\tLoss: 0.6040\tLR: 0.000001\n",
      "Training Epoch: 21 [50240/72641]\tLoss: 0.5678\tLR: 0.000001\n",
      "Training Epoch: 21 [50560/72641]\tLoss: 0.6063\tLR: 0.000001\n",
      "Training Epoch: 21 [50880/72641]\tLoss: 0.6349\tLR: 0.000001\n",
      "Training Epoch: 21 [51200/72641]\tLoss: 0.6450\tLR: 0.000001\n",
      "Training Epoch: 21 [51520/72641]\tLoss: 0.6042\tLR: 0.000001\n",
      "Training Epoch: 21 [51840/72641]\tLoss: 0.5742\tLR: 0.000001\n",
      "Training Epoch: 21 [52160/72641]\tLoss: 0.6294\tLR: 0.000001\n",
      "Training Epoch: 21 [52480/72641]\tLoss: 0.6554\tLR: 0.000001\n",
      "Training Epoch: 21 [52800/72641]\tLoss: 0.6599\tLR: 0.000001\n",
      "Training Epoch: 21 [53120/72641]\tLoss: 0.6417\tLR: 0.000001\n",
      "Training Epoch: 21 [53440/72641]\tLoss: 0.5965\tLR: 0.000001\n",
      "Training Epoch: 21 [53760/72641]\tLoss: 0.6650\tLR: 0.000001\n",
      "Training Epoch: 21 [54080/72641]\tLoss: 0.6058\tLR: 0.000001\n",
      "Training Epoch: 21 [54400/72641]\tLoss: 0.6594\tLR: 0.000001\n",
      "Training Epoch: 21 [54720/72641]\tLoss: 0.6664\tLR: 0.000001\n",
      "Training Epoch: 21 [55040/72641]\tLoss: 0.6203\tLR: 0.000001\n",
      "Training Epoch: 21 [55360/72641]\tLoss: 0.6501\tLR: 0.000001\n",
      "Training Epoch: 21 [55680/72641]\tLoss: 0.5787\tLR: 0.000001\n",
      "Training Epoch: 21 [56000/72641]\tLoss: 0.5866\tLR: 0.000001\n",
      "Training Epoch: 21 [56320/72641]\tLoss: 0.6503\tLR: 0.000001\n",
      "Training Epoch: 21 [56640/72641]\tLoss: 0.6558\tLR: 0.000001\n",
      "Training Epoch: 21 [56960/72641]\tLoss: 0.6188\tLR: 0.000001\n",
      "Training Epoch: 21 [57280/72641]\tLoss: 0.6119\tLR: 0.000001\n",
      "Training Epoch: 21 [57600/72641]\tLoss: 0.6347\tLR: 0.000001\n",
      "Training Epoch: 21 [57920/72641]\tLoss: 0.6543\tLR: 0.000001\n",
      "Training Epoch: 21 [58240/72641]\tLoss: 0.6098\tLR: 0.000001\n",
      "Training Epoch: 21 [58560/72641]\tLoss: 0.5763\tLR: 0.000001\n",
      "Training Epoch: 21 [58880/72641]\tLoss: 0.6305\tLR: 0.000001\n",
      "Training Epoch: 21 [59200/72641]\tLoss: 0.6318\tLR: 0.000001\n",
      "Training Epoch: 21 [59520/72641]\tLoss: 0.6295\tLR: 0.000001\n",
      "Training Epoch: 21 [59840/72641]\tLoss: 0.5971\tLR: 0.000001\n",
      "Training Epoch: 21 [60160/72641]\tLoss: 0.6103\tLR: 0.000001\n",
      "Training Epoch: 21 [60480/72641]\tLoss: 0.6463\tLR: 0.000001\n",
      "Training Epoch: 21 [60800/72641]\tLoss: 0.5999\tLR: 0.000001\n",
      "Training Epoch: 21 [61120/72641]\tLoss: 0.6682\tLR: 0.000001\n",
      "Training Epoch: 21 [61440/72641]\tLoss: 0.6403\tLR: 0.000001\n",
      "Training Epoch: 21 [61760/72641]\tLoss: 0.6178\tLR: 0.000001\n",
      "Training Epoch: 21 [62080/72641]\tLoss: 0.6982\tLR: 0.000001\n",
      "Training Epoch: 21 [62400/72641]\tLoss: 0.6328\tLR: 0.000001\n",
      "Training Epoch: 21 [62720/72641]\tLoss: 0.5972\tLR: 0.000001\n",
      "Training Epoch: 21 [63040/72641]\tLoss: 0.6454\tLR: 0.000001\n",
      "Training Epoch: 21 [63360/72641]\tLoss: 0.7082\tLR: 0.000001\n",
      "Training Epoch: 21 [63680/72641]\tLoss: 0.6667\tLR: 0.000001\n",
      "Training Epoch: 21 [64000/72641]\tLoss: 0.6075\tLR: 0.000001\n",
      "Training Epoch: 21 [64320/72641]\tLoss: 0.6286\tLR: 0.000001\n",
      "Training Epoch: 21 [64640/72641]\tLoss: 0.6309\tLR: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [64960/72641]\tLoss: 0.6045\tLR: 0.000001\n",
      "Training Epoch: 21 [65280/72641]\tLoss: 0.6344\tLR: 0.000001\n",
      "Training Epoch: 21 [65600/72641]\tLoss: 0.6250\tLR: 0.000001\n",
      "Training Epoch: 21 [65920/72641]\tLoss: 0.6377\tLR: 0.000001\n",
      "Training Epoch: 21 [66240/72641]\tLoss: 0.6292\tLR: 0.000001\n",
      "Training Epoch: 21 [66560/72641]\tLoss: 0.6277\tLR: 0.000001\n",
      "Training Epoch: 21 [66880/72641]\tLoss: 0.5942\tLR: 0.000001\n",
      "Training Epoch: 21 [67200/72641]\tLoss: 0.5936\tLR: 0.000001\n",
      "Training Epoch: 21 [67520/72641]\tLoss: 0.6380\tLR: 0.000001\n",
      "Training Epoch: 21 [67840/72641]\tLoss: 0.6252\tLR: 0.000001\n",
      "Training Epoch: 21 [68160/72641]\tLoss: 0.6263\tLR: 0.000001\n",
      "Training Epoch: 21 [68480/72641]\tLoss: 0.6370\tLR: 0.000001\n",
      "Training Epoch: 21 [68800/72641]\tLoss: 0.6632\tLR: 0.000001\n",
      "Training Epoch: 21 [69120/72641]\tLoss: 0.6418\tLR: 0.000001\n",
      "Training Epoch: 21 [69440/72641]\tLoss: 0.6012\tLR: 0.000001\n",
      "Training Epoch: 21 [69760/72641]\tLoss: 0.6317\tLR: 0.000001\n",
      "Training Epoch: 21 [70080/72641]\tLoss: 0.6187\tLR: 0.000001\n",
      "Training Epoch: 21 [70400/72641]\tLoss: 0.6464\tLR: 0.000001\n",
      "Training Epoch: 21 [70720/72641]\tLoss: 0.6042\tLR: 0.000001\n",
      "Training Epoch: 21 [71040/72641]\tLoss: 0.6177\tLR: 0.000001\n",
      "Training Epoch: 21 [71360/72641]\tLoss: 0.6185\tLR: 0.000001\n",
      "Training Epoch: 21 [71680/72641]\tLoss: 0.6089\tLR: 0.000001\n",
      "Training Epoch: 21 [72000/72641]\tLoss: 0.6180\tLR: 0.000001\n",
      "Training Epoch: 21 [72320/72641]\tLoss: 0.5853\tLR: 0.000001\n",
      "Training Epoch: 21 [72640/72641]\tLoss: 0.5958\tLR: 0.000001\n",
      "Val Result: Acc: 0.1480, C_ACC: 0.7153, DOA: 88.4341, ACC_k: 0.1016\n",
      "ext:0.0, cls:0.558008, coar:0.0, fine:0.0,\n",
      "Training Epoch: 22 [320/72641]\tLoss: 0.6031\tLR: 0.000001\n",
      "Training Epoch: 22 [640/72641]\tLoss: 0.6121\tLR: 0.000001\n",
      "Training Epoch: 22 [960/72641]\tLoss: 0.6066\tLR: 0.000001\n",
      "Training Epoch: 22 [1280/72641]\tLoss: 0.6414\tLR: 0.000001\n",
      "Training Epoch: 22 [1600/72641]\tLoss: 0.7289\tLR: 0.000001\n",
      "Training Epoch: 22 [1920/72641]\tLoss: 0.6277\tLR: 0.000001\n",
      "Training Epoch: 22 [2240/72641]\tLoss: 0.6069\tLR: 0.000001\n",
      "Training Epoch: 22 [2560/72641]\tLoss: 0.6717\tLR: 0.000001\n",
      "Training Epoch: 22 [2880/72641]\tLoss: 0.5798\tLR: 0.000001\n",
      "Training Epoch: 22 [3200/72641]\tLoss: 0.6477\tLR: 0.000001\n",
      "Training Epoch: 22 [3520/72641]\tLoss: 0.6624\tLR: 0.000001\n",
      "Training Epoch: 22 [3840/72641]\tLoss: 0.5905\tLR: 0.000001\n",
      "Training Epoch: 22 [4160/72641]\tLoss: 0.6309\tLR: 0.000001\n",
      "Training Epoch: 22 [4480/72641]\tLoss: 0.5980\tLR: 0.000001\n",
      "Training Epoch: 22 [4800/72641]\tLoss: 0.6295\tLR: 0.000001\n",
      "Training Epoch: 22 [5120/72641]\tLoss: 0.5875\tLR: 0.000001\n",
      "Training Epoch: 22 [5440/72641]\tLoss: 0.6370\tLR: 0.000001\n",
      "Training Epoch: 22 [5760/72641]\tLoss: 0.6209\tLR: 0.000001\n",
      "Training Epoch: 22 [6080/72641]\tLoss: 0.6282\tLR: 0.000001\n",
      "Training Epoch: 22 [6400/72641]\tLoss: 0.6546\tLR: 0.000001\n",
      "Training Epoch: 22 [6720/72641]\tLoss: 0.6825\tLR: 0.000001\n",
      "Training Epoch: 22 [7040/72641]\tLoss: 0.5953\tLR: 0.000001\n",
      "Training Epoch: 22 [7360/72641]\tLoss: 0.6243\tLR: 0.000001\n",
      "Training Epoch: 22 [7680/72641]\tLoss: 0.6411\tLR: 0.000001\n",
      "Training Epoch: 22 [8000/72641]\tLoss: 0.6207\tLR: 0.000001\n",
      "Training Epoch: 22 [8320/72641]\tLoss: 0.6662\tLR: 0.000001\n",
      "Training Epoch: 22 [8640/72641]\tLoss: 0.6178\tLR: 0.000001\n",
      "Training Epoch: 22 [8960/72641]\tLoss: 0.6052\tLR: 0.000001\n",
      "Training Epoch: 22 [9280/72641]\tLoss: 0.5993\tLR: 0.000001\n",
      "Training Epoch: 22 [9600/72641]\tLoss: 0.6325\tLR: 0.000001\n",
      "Training Epoch: 22 [9920/72641]\tLoss: 0.6480\tLR: 0.000001\n",
      "Training Epoch: 22 [10240/72641]\tLoss: 0.6420\tLR: 0.000001\n",
      "Training Epoch: 22 [10560/72641]\tLoss: 0.5879\tLR: 0.000001\n",
      "Training Epoch: 22 [10880/72641]\tLoss: 0.6317\tLR: 0.000001\n",
      "Training Epoch: 22 [11200/72641]\tLoss: 0.6675\tLR: 0.000001\n",
      "Training Epoch: 22 [11520/72641]\tLoss: 0.6521\tLR: 0.000001\n",
      "Training Epoch: 22 [11840/72641]\tLoss: 0.6246\tLR: 0.000001\n",
      "Training Epoch: 22 [12160/72641]\tLoss: 0.6146\tLR: 0.000001\n",
      "Training Epoch: 22 [12480/72641]\tLoss: 0.5888\tLR: 0.000001\n",
      "Training Epoch: 22 [12800/72641]\tLoss: 0.6344\tLR: 0.000001\n",
      "Training Epoch: 22 [13120/72641]\tLoss: 0.5942\tLR: 0.000001\n",
      "Training Epoch: 22 [13440/72641]\tLoss: 0.6447\tLR: 0.000001\n",
      "Training Epoch: 22 [13760/72641]\tLoss: 0.6434\tLR: 0.000001\n",
      "Training Epoch: 22 [14080/72641]\tLoss: 0.6057\tLR: 0.000001\n",
      "Training Epoch: 22 [14400/72641]\tLoss: 0.6419\tLR: 0.000001\n",
      "Training Epoch: 22 [14720/72641]\tLoss: 0.5925\tLR: 0.000001\n",
      "Training Epoch: 22 [15040/72641]\tLoss: 0.6422\tLR: 0.000001\n",
      "Training Epoch: 22 [15360/72641]\tLoss: 0.6485\tLR: 0.000001\n",
      "Training Epoch: 22 [15680/72641]\tLoss: 0.6398\tLR: 0.000001\n",
      "Training Epoch: 22 [16000/72641]\tLoss: 0.6360\tLR: 0.000001\n",
      "Training Epoch: 22 [16320/72641]\tLoss: 0.6339\tLR: 0.000001\n",
      "Training Epoch: 22 [16640/72641]\tLoss: 0.6448\tLR: 0.000001\n",
      "Training Epoch: 22 [16960/72641]\tLoss: 0.6204\tLR: 0.000001\n",
      "Training Epoch: 22 [17280/72641]\tLoss: 0.6079\tLR: 0.000001\n",
      "Training Epoch: 22 [17600/72641]\tLoss: 0.6082\tLR: 0.000001\n",
      "Training Epoch: 22 [17920/72641]\tLoss: 0.6544\tLR: 0.000001\n",
      "Training Epoch: 22 [18240/72641]\tLoss: 0.6581\tLR: 0.000001\n",
      "Training Epoch: 22 [18560/72641]\tLoss: 0.6009\tLR: 0.000001\n",
      "Training Epoch: 22 [18880/72641]\tLoss: 0.6924\tLR: 0.000001\n",
      "Training Epoch: 22 [19200/72641]\tLoss: 0.6356\tLR: 0.000001\n",
      "Training Epoch: 22 [19520/72641]\tLoss: 0.6508\tLR: 0.000001\n",
      "Training Epoch: 22 [19840/72641]\tLoss: 0.6405\tLR: 0.000001\n",
      "Training Epoch: 22 [20160/72641]\tLoss: 0.5734\tLR: 0.000001\n",
      "Training Epoch: 22 [20480/72641]\tLoss: 0.5863\tLR: 0.000001\n",
      "Training Epoch: 22 [20800/72641]\tLoss: 0.6688\tLR: 0.000001\n",
      "Training Epoch: 22 [21120/72641]\tLoss: 0.6546\tLR: 0.000001\n",
      "Training Epoch: 22 [21440/72641]\tLoss: 0.5714\tLR: 0.000001\n",
      "Training Epoch: 22 [21760/72641]\tLoss: 0.6107\tLR: 0.000001\n",
      "Training Epoch: 22 [22080/72641]\tLoss: 0.6351\tLR: 0.000001\n",
      "Training Epoch: 22 [22400/72641]\tLoss: 0.6508\tLR: 0.000001\n",
      "Training Epoch: 22 [22720/72641]\tLoss: 0.6266\tLR: 0.000001\n",
      "Training Epoch: 22 [23040/72641]\tLoss: 0.5812\tLR: 0.000001\n",
      "Training Epoch: 22 [23360/72641]\tLoss: 0.6245\tLR: 0.000001\n",
      "Training Epoch: 22 [23680/72641]\tLoss: 0.6720\tLR: 0.000001\n",
      "Training Epoch: 22 [24000/72641]\tLoss: 0.6342\tLR: 0.000001\n",
      "Training Epoch: 22 [24320/72641]\tLoss: 0.5676\tLR: 0.000001\n",
      "Training Epoch: 22 [24640/72641]\tLoss: 0.7206\tLR: 0.000001\n",
      "Training Epoch: 22 [24960/72641]\tLoss: 0.6286\tLR: 0.000001\n",
      "Training Epoch: 22 [25280/72641]\tLoss: 0.6068\tLR: 0.000001\n",
      "Training Epoch: 22 [25600/72641]\tLoss: 0.6117\tLR: 0.000001\n",
      "Training Epoch: 22 [25920/72641]\tLoss: 0.6324\tLR: 0.000001\n",
      "Training Epoch: 22 [26240/72641]\tLoss: 0.6546\tLR: 0.000001\n",
      "Training Epoch: 22 [26560/72641]\tLoss: 0.6376\tLR: 0.000001\n",
      "Training Epoch: 22 [26880/72641]\tLoss: 0.6674\tLR: 0.000001\n",
      "Training Epoch: 22 [27200/72641]\tLoss: 0.6117\tLR: 0.000001\n",
      "Training Epoch: 22 [27520/72641]\tLoss: 0.6436\tLR: 0.000001\n",
      "Training Epoch: 22 [27840/72641]\tLoss: 0.6335\tLR: 0.000001\n",
      "Training Epoch: 22 [28160/72641]\tLoss: 0.6142\tLR: 0.000001\n",
      "Training Epoch: 22 [28480/72641]\tLoss: 0.6157\tLR: 0.000001\n",
      "Training Epoch: 22 [28800/72641]\tLoss: 0.6131\tLR: 0.000001\n",
      "Training Epoch: 22 [29120/72641]\tLoss: 0.7077\tLR: 0.000001\n",
      "Training Epoch: 22 [29440/72641]\tLoss: 0.6312\tLR: 0.000001\n",
      "Training Epoch: 22 [29760/72641]\tLoss: 0.6428\tLR: 0.000001\n",
      "Training Epoch: 22 [30080/72641]\tLoss: 0.6841\tLR: 0.000001\n",
      "Training Epoch: 22 [30400/72641]\tLoss: 0.6131\tLR: 0.000001\n",
      "Training Epoch: 22 [30720/72641]\tLoss: 0.6722\tLR: 0.000001\n",
      "Training Epoch: 22 [31040/72641]\tLoss: 0.6371\tLR: 0.000001\n",
      "Training Epoch: 22 [31360/72641]\tLoss: 0.5913\tLR: 0.000001\n",
      "Training Epoch: 22 [31680/72641]\tLoss: 0.6244\tLR: 0.000001\n",
      "Training Epoch: 22 [32000/72641]\tLoss: 0.6803\tLR: 0.000001\n",
      "Training Epoch: 22 [32320/72641]\tLoss: 0.6565\tLR: 0.000001\n",
      "Training Epoch: 22 [32640/72641]\tLoss: 0.6134\tLR: 0.000001\n",
      "Training Epoch: 22 [32960/72641]\tLoss: 0.7054\tLR: 0.000001\n",
      "Training Epoch: 22 [33280/72641]\tLoss: 0.6571\tLR: 0.000001\n",
      "Training Epoch: 22 [33600/72641]\tLoss: 0.6350\tLR: 0.000001\n",
      "Training Epoch: 22 [33920/72641]\tLoss: 0.6231\tLR: 0.000001\n",
      "Training Epoch: 22 [34240/72641]\tLoss: 0.6148\tLR: 0.000001\n",
      "Training Epoch: 22 [34560/72641]\tLoss: 0.6462\tLR: 0.000001\n",
      "Training Epoch: 22 [34880/72641]\tLoss: 0.5975\tLR: 0.000001\n",
      "Training Epoch: 22 [35200/72641]\tLoss: 0.6560\tLR: 0.000001\n",
      "Training Epoch: 22 [35520/72641]\tLoss: 0.6262\tLR: 0.000001\n",
      "Training Epoch: 22 [35840/72641]\tLoss: 0.6222\tLR: 0.000001\n",
      "Training Epoch: 22 [36160/72641]\tLoss: 0.6123\tLR: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 22 [36480/72641]\tLoss: 0.5935\tLR: 0.000001\n",
      "Training Epoch: 22 [36800/72641]\tLoss: 0.5872\tLR: 0.000001\n",
      "Training Epoch: 22 [37120/72641]\tLoss: 0.6321\tLR: 0.000001\n",
      "Training Epoch: 22 [37440/72641]\tLoss: 0.5907\tLR: 0.000001\n",
      "Training Epoch: 22 [37760/72641]\tLoss: 0.6176\tLR: 0.000001\n",
      "Training Epoch: 22 [38080/72641]\tLoss: 0.6307\tLR: 0.000001\n",
      "Training Epoch: 22 [38400/72641]\tLoss: 0.6396\tLR: 0.000001\n",
      "Training Epoch: 22 [38720/72641]\tLoss: 0.6664\tLR: 0.000001\n",
      "Training Epoch: 22 [39040/72641]\tLoss: 0.5809\tLR: 0.000001\n",
      "Training Epoch: 22 [39360/72641]\tLoss: 0.6435\tLR: 0.000001\n",
      "Training Epoch: 22 [39680/72641]\tLoss: 0.5750\tLR: 0.000001\n",
      "Training Epoch: 22 [40000/72641]\tLoss: 0.6800\tLR: 0.000001\n",
      "Training Epoch: 22 [40320/72641]\tLoss: 0.6439\tLR: 0.000001\n",
      "Training Epoch: 22 [40640/72641]\tLoss: 0.6290\tLR: 0.000001\n",
      "Training Epoch: 22 [40960/72641]\tLoss: 0.6416\tLR: 0.000001\n",
      "Training Epoch: 22 [41280/72641]\tLoss: 0.6534\tLR: 0.000001\n",
      "Training Epoch: 22 [41600/72641]\tLoss: 0.6266\tLR: 0.000001\n",
      "Training Epoch: 22 [41920/72641]\tLoss: 0.6504\tLR: 0.000001\n",
      "Training Epoch: 22 [42240/72641]\tLoss: 0.5799\tLR: 0.000001\n",
      "Training Epoch: 22 [42560/72641]\tLoss: 0.6371\tLR: 0.000001\n",
      "Training Epoch: 22 [42880/72641]\tLoss: 0.6398\tLR: 0.000001\n",
      "Training Epoch: 22 [43200/72641]\tLoss: 0.6092\tLR: 0.000001\n",
      "Training Epoch: 22 [43520/72641]\tLoss: 0.6757\tLR: 0.000001\n",
      "Training Epoch: 22 [43840/72641]\tLoss: 0.6194\tLR: 0.000001\n",
      "Training Epoch: 22 [44160/72641]\tLoss: 0.6407\tLR: 0.000001\n",
      "Training Epoch: 22 [44480/72641]\tLoss: 0.6057\tLR: 0.000001\n",
      "Training Epoch: 22 [44800/72641]\tLoss: 0.6108\tLR: 0.000001\n",
      "Training Epoch: 22 [45120/72641]\tLoss: 0.5896\tLR: 0.000001\n",
      "Training Epoch: 22 [45440/72641]\tLoss: 0.6036\tLR: 0.000001\n",
      "Training Epoch: 22 [45760/72641]\tLoss: 0.6112\tLR: 0.000001\n",
      "Training Epoch: 22 [46080/72641]\tLoss: 0.6292\tLR: 0.000001\n",
      "Training Epoch: 22 [46400/72641]\tLoss: 0.6054\tLR: 0.000001\n",
      "Training Epoch: 22 [46720/72641]\tLoss: 0.6441\tLR: 0.000001\n",
      "Training Epoch: 22 [47040/72641]\tLoss: 0.5961\tLR: 0.000001\n",
      "Training Epoch: 22 [47360/72641]\tLoss: 0.6637\tLR: 0.000001\n",
      "Training Epoch: 22 [47680/72641]\tLoss: 0.6182\tLR: 0.000001\n",
      "Training Epoch: 22 [48000/72641]\tLoss: 0.6559\tLR: 0.000001\n",
      "Training Epoch: 22 [48320/72641]\tLoss: 0.6692\tLR: 0.000001\n",
      "Training Epoch: 22 [48640/72641]\tLoss: 0.6085\tLR: 0.000001\n",
      "Training Epoch: 22 [48960/72641]\tLoss: 0.6398\tLR: 0.000001\n",
      "Training Epoch: 22 [49280/72641]\tLoss: 0.5819\tLR: 0.000001\n",
      "Training Epoch: 22 [49600/72641]\tLoss: 0.6502\tLR: 0.000001\n",
      "Training Epoch: 22 [49920/72641]\tLoss: 0.6126\tLR: 0.000001\n",
      "Training Epoch: 22 [50240/72641]\tLoss: 0.5630\tLR: 0.000001\n",
      "Training Epoch: 22 [50560/72641]\tLoss: 0.5990\tLR: 0.000001\n",
      "Training Epoch: 22 [50880/72641]\tLoss: 0.6451\tLR: 0.000001\n",
      "Training Epoch: 22 [51200/72641]\tLoss: 0.6157\tLR: 0.000001\n",
      "Training Epoch: 22 [51520/72641]\tLoss: 0.6444\tLR: 0.000001\n",
      "Training Epoch: 22 [51840/72641]\tLoss: 0.6119\tLR: 0.000001\n",
      "Training Epoch: 22 [52160/72641]\tLoss: 0.6538\tLR: 0.000001\n",
      "Training Epoch: 22 [52480/72641]\tLoss: 0.6061\tLR: 0.000001\n",
      "Training Epoch: 22 [52800/72641]\tLoss: 0.6341\tLR: 0.000001\n",
      "Training Epoch: 22 [53120/72641]\tLoss: 0.6029\tLR: 0.000001\n",
      "Training Epoch: 22 [53440/72641]\tLoss: 0.5990\tLR: 0.000001\n",
      "Training Epoch: 22 [53760/72641]\tLoss: 0.6811\tLR: 0.000001\n",
      "Training Epoch: 22 [54080/72641]\tLoss: 0.6361\tLR: 0.000001\n",
      "Training Epoch: 22 [54400/72641]\tLoss: 0.6660\tLR: 0.000001\n",
      "Training Epoch: 22 [54720/72641]\tLoss: 0.6365\tLR: 0.000001\n",
      "Training Epoch: 22 [55040/72641]\tLoss: 0.6510\tLR: 0.000001\n",
      "Training Epoch: 22 [55360/72641]\tLoss: 0.5701\tLR: 0.000001\n",
      "Training Epoch: 22 [55680/72641]\tLoss: 0.6002\tLR: 0.000001\n",
      "Training Epoch: 22 [56000/72641]\tLoss: 0.6319\tLR: 0.000001\n",
      "Training Epoch: 22 [56320/72641]\tLoss: 0.6400\tLR: 0.000001\n",
      "Training Epoch: 22 [56640/72641]\tLoss: 0.6610\tLR: 0.000001\n",
      "Training Epoch: 22 [56960/72641]\tLoss: 0.6115\tLR: 0.000001\n",
      "Training Epoch: 22 [57280/72641]\tLoss: 0.6269\tLR: 0.000001\n",
      "Training Epoch: 22 [57600/72641]\tLoss: 0.6385\tLR: 0.000001\n",
      "Training Epoch: 22 [57920/72641]\tLoss: 0.6607\tLR: 0.000001\n",
      "Training Epoch: 22 [58240/72641]\tLoss: 0.6079\tLR: 0.000001\n",
      "Training Epoch: 22 [58560/72641]\tLoss: 0.6406\tLR: 0.000001\n",
      "Training Epoch: 22 [58880/72641]\tLoss: 0.6096\tLR: 0.000001\n",
      "Training Epoch: 22 [59200/72641]\tLoss: 0.6256\tLR: 0.000001\n",
      "Training Epoch: 22 [59520/72641]\tLoss: 0.6147\tLR: 0.000001\n",
      "Training Epoch: 22 [59840/72641]\tLoss: 0.6683\tLR: 0.000001\n",
      "Training Epoch: 22 [60160/72641]\tLoss: 0.5875\tLR: 0.000001\n",
      "Training Epoch: 22 [60480/72641]\tLoss: 0.6380\tLR: 0.000001\n",
      "Training Epoch: 22 [60800/72641]\tLoss: 0.6046\tLR: 0.000001\n",
      "Training Epoch: 22 [61120/72641]\tLoss: 0.5889\tLR: 0.000001\n",
      "Training Epoch: 22 [61440/72641]\tLoss: 0.5727\tLR: 0.000001\n",
      "Training Epoch: 22 [61760/72641]\tLoss: 0.6275\tLR: 0.000001\n",
      "Training Epoch: 22 [62080/72641]\tLoss: 0.6629\tLR: 0.000001\n",
      "Training Epoch: 22 [62400/72641]\tLoss: 0.6157\tLR: 0.000001\n",
      "Training Epoch: 22 [62720/72641]\tLoss: 0.5766\tLR: 0.000001\n",
      "Training Epoch: 22 [63040/72641]\tLoss: 0.6639\tLR: 0.000001\n",
      "Training Epoch: 22 [63360/72641]\tLoss: 0.6392\tLR: 0.000001\n",
      "Training Epoch: 22 [63680/72641]\tLoss: 0.6101\tLR: 0.000001\n",
      "Training Epoch: 22 [64000/72641]\tLoss: 0.6033\tLR: 0.000001\n",
      "Training Epoch: 22 [64320/72641]\tLoss: 0.6258\tLR: 0.000001\n",
      "Training Epoch: 22 [64640/72641]\tLoss: 0.6097\tLR: 0.000001\n",
      "Training Epoch: 22 [64960/72641]\tLoss: 0.6256\tLR: 0.000001\n",
      "Training Epoch: 22 [65280/72641]\tLoss: 0.6153\tLR: 0.000001\n",
      "Training Epoch: 22 [65600/72641]\tLoss: 0.5996\tLR: 0.000001\n",
      "Training Epoch: 22 [65920/72641]\tLoss: 0.6489\tLR: 0.000001\n",
      "Training Epoch: 22 [66240/72641]\tLoss: 0.7007\tLR: 0.000001\n",
      "Training Epoch: 22 [66560/72641]\tLoss: 0.6422\tLR: 0.000001\n",
      "Training Epoch: 22 [66880/72641]\tLoss: 0.5715\tLR: 0.000001\n",
      "Training Epoch: 22 [67200/72641]\tLoss: 0.6196\tLR: 0.000001\n",
      "Training Epoch: 22 [67520/72641]\tLoss: 0.6039\tLR: 0.000001\n",
      "Training Epoch: 22 [67840/72641]\tLoss: 0.6398\tLR: 0.000001\n",
      "Training Epoch: 22 [68160/72641]\tLoss: 0.6038\tLR: 0.000001\n",
      "Training Epoch: 22 [68480/72641]\tLoss: 0.6204\tLR: 0.000001\n",
      "Training Epoch: 22 [68800/72641]\tLoss: 0.6293\tLR: 0.000001\n",
      "Training Epoch: 22 [69120/72641]\tLoss: 0.6487\tLR: 0.000001\n",
      "Training Epoch: 22 [69440/72641]\tLoss: 0.6173\tLR: 0.000001\n",
      "Training Epoch: 22 [69760/72641]\tLoss: 0.6651\tLR: 0.000001\n",
      "Training Epoch: 22 [70080/72641]\tLoss: 0.5915\tLR: 0.000001\n",
      "Training Epoch: 22 [70400/72641]\tLoss: 0.6095\tLR: 0.000001\n",
      "Training Epoch: 22 [70720/72641]\tLoss: 0.6278\tLR: 0.000001\n",
      "Training Epoch: 22 [71040/72641]\tLoss: 0.6135\tLR: 0.000001\n",
      "Training Epoch: 22 [71360/72641]\tLoss: 0.6529\tLR: 0.000001\n",
      "Training Epoch: 22 [71680/72641]\tLoss: 0.6366\tLR: 0.000001\n",
      "Training Epoch: 22 [72000/72641]\tLoss: 0.6244\tLR: 0.000001\n",
      "Training Epoch: 22 [72320/72641]\tLoss: 0.6157\tLR: 0.000001\n",
      "Training Epoch: 22 [72640/72641]\tLoss: 0.6042\tLR: 0.000001\n",
      "Val Result: Acc: 0.1470, C_ACC: 0.7087, DOA: 88.8231, ACC_k: 0.1016\n",
      "ext:0.0, cls:0.564386, coar:0.0, fine:0.0,\n",
      "Training Epoch: 23 [320/72641]\tLoss: 0.6181\tLR: 0.000001\n",
      "Training Epoch: 23 [640/72641]\tLoss: 0.5858\tLR: 0.000001\n",
      "Training Epoch: 23 [960/72641]\tLoss: 0.5888\tLR: 0.000001\n",
      "Training Epoch: 23 [1280/72641]\tLoss: 0.6644\tLR: 0.000001\n",
      "Training Epoch: 23 [1600/72641]\tLoss: 0.6900\tLR: 0.000001\n",
      "Training Epoch: 23 [1920/72641]\tLoss: 0.6199\tLR: 0.000001\n",
      "Training Epoch: 23 [2240/72641]\tLoss: 0.6366\tLR: 0.000001\n",
      "Training Epoch: 23 [2560/72641]\tLoss: 0.6264\tLR: 0.000001\n",
      "Training Epoch: 23 [2880/72641]\tLoss: 0.6757\tLR: 0.000001\n",
      "Training Epoch: 23 [3200/72641]\tLoss: 0.6325\tLR: 0.000001\n",
      "Training Epoch: 23 [3520/72641]\tLoss: 0.5965\tLR: 0.000001\n",
      "Training Epoch: 23 [3840/72641]\tLoss: 0.6220\tLR: 0.000001\n",
      "Training Epoch: 23 [4160/72641]\tLoss: 0.5993\tLR: 0.000001\n",
      "Training Epoch: 23 [4480/72641]\tLoss: 0.6224\tLR: 0.000001\n",
      "Training Epoch: 23 [4800/72641]\tLoss: 0.6310\tLR: 0.000001\n",
      "Training Epoch: 23 [5120/72641]\tLoss: 0.6103\tLR: 0.000001\n",
      "Training Epoch: 23 [5440/72641]\tLoss: 0.6581\tLR: 0.000001\n",
      "Training Epoch: 23 [5760/72641]\tLoss: 0.6376\tLR: 0.000001\n",
      "Training Epoch: 23 [6080/72641]\tLoss: 0.6221\tLR: 0.000001\n",
      "Training Epoch: 23 [6400/72641]\tLoss: 0.6122\tLR: 0.000001\n",
      "Training Epoch: 23 [6720/72641]\tLoss: 0.5788\tLR: 0.000001\n",
      "Training Epoch: 23 [7040/72641]\tLoss: 0.6529\tLR: 0.000001\n",
      "Training Epoch: 23 [7360/72641]\tLoss: 0.6000\tLR: 0.000001\n",
      "Training Epoch: 23 [7680/72641]\tLoss: 0.6315\tLR: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [8000/72641]\tLoss: 0.6431\tLR: 0.000001\n",
      "Training Epoch: 23 [8320/72641]\tLoss: 0.6546\tLR: 0.000001\n",
      "Training Epoch: 23 [8640/72641]\tLoss: 0.6494\tLR: 0.000001\n",
      "Training Epoch: 23 [8960/72641]\tLoss: 0.5950\tLR: 0.000001\n",
      "Training Epoch: 23 [9280/72641]\tLoss: 0.6432\tLR: 0.000001\n",
      "Training Epoch: 23 [9600/72641]\tLoss: 0.6630\tLR: 0.000001\n",
      "Training Epoch: 23 [9920/72641]\tLoss: 0.6558\tLR: 0.000001\n",
      "Training Epoch: 23 [10240/72641]\tLoss: 0.6775\tLR: 0.000001\n",
      "Training Epoch: 23 [10560/72641]\tLoss: 0.6261\tLR: 0.000001\n",
      "Training Epoch: 23 [10880/72641]\tLoss: 0.6501\tLR: 0.000001\n",
      "Training Epoch: 23 [11200/72641]\tLoss: 0.6195\tLR: 0.000001\n",
      "Training Epoch: 23 [11520/72641]\tLoss: 0.6221\tLR: 0.000001\n",
      "Training Epoch: 23 [11840/72641]\tLoss: 0.5732\tLR: 0.000001\n",
      "Training Epoch: 23 [12160/72641]\tLoss: 0.6209\tLR: 0.000001\n",
      "Training Epoch: 23 [12480/72641]\tLoss: 0.6629\tLR: 0.000001\n",
      "Training Epoch: 23 [12800/72641]\tLoss: 0.6330\tLR: 0.000001\n",
      "Training Epoch: 23 [13120/72641]\tLoss: 0.5894\tLR: 0.000001\n",
      "Training Epoch: 23 [13440/72641]\tLoss: 0.6226\tLR: 0.000001\n",
      "Training Epoch: 23 [13760/72641]\tLoss: 0.6526\tLR: 0.000001\n",
      "Training Epoch: 23 [14080/72641]\tLoss: 0.6477\tLR: 0.000001\n",
      "Training Epoch: 23 [14400/72641]\tLoss: 0.6511\tLR: 0.000001\n",
      "Training Epoch: 23 [14720/72641]\tLoss: 0.6083\tLR: 0.000001\n",
      "Training Epoch: 23 [15040/72641]\tLoss: 0.6349\tLR: 0.000001\n",
      "Training Epoch: 23 [15360/72641]\tLoss: 0.6637\tLR: 0.000001\n",
      "Training Epoch: 23 [15680/72641]\tLoss: 0.6162\tLR: 0.000001\n",
      "Training Epoch: 23 [16000/72641]\tLoss: 0.6339\tLR: 0.000001\n",
      "Training Epoch: 23 [16320/72641]\tLoss: 0.6144\tLR: 0.000001\n",
      "Training Epoch: 23 [16640/72641]\tLoss: 0.6452\tLR: 0.000001\n",
      "Training Epoch: 23 [16960/72641]\tLoss: 0.6498\tLR: 0.000001\n",
      "Training Epoch: 23 [17280/72641]\tLoss: 0.6203\tLR: 0.000001\n",
      "Training Epoch: 23 [17600/72641]\tLoss: 0.5795\tLR: 0.000001\n",
      "Training Epoch: 23 [17920/72641]\tLoss: 0.6316\tLR: 0.000001\n",
      "Training Epoch: 23 [18240/72641]\tLoss: 0.6128\tLR: 0.000001\n",
      "Training Epoch: 23 [18560/72641]\tLoss: 0.6275\tLR: 0.000001\n",
      "Training Epoch: 23 [18880/72641]\tLoss: 0.5821\tLR: 0.000001\n",
      "Training Epoch: 23 [19200/72641]\tLoss: 0.5921\tLR: 0.000001\n",
      "Training Epoch: 23 [19520/72641]\tLoss: 0.5855\tLR: 0.000001\n",
      "Training Epoch: 23 [19840/72641]\tLoss: 0.6620\tLR: 0.000001\n",
      "Training Epoch: 23 [20160/72641]\tLoss: 0.5918\tLR: 0.000001\n",
      "Training Epoch: 23 [20480/72641]\tLoss: 0.6415\tLR: 0.000001\n",
      "Training Epoch: 23 [20800/72641]\tLoss: 0.6344\tLR: 0.000001\n",
      "Training Epoch: 23 [21120/72641]\tLoss: 0.6678\tLR: 0.000001\n",
      "Training Epoch: 23 [21440/72641]\tLoss: 0.6012\tLR: 0.000001\n",
      "Training Epoch: 23 [21760/72641]\tLoss: 0.6408\tLR: 0.000001\n",
      "Training Epoch: 23 [22080/72641]\tLoss: 0.6577\tLR: 0.000001\n",
      "Training Epoch: 23 [22400/72641]\tLoss: 0.6038\tLR: 0.000001\n",
      "Training Epoch: 23 [22720/72641]\tLoss: 0.6078\tLR: 0.000001\n",
      "Training Epoch: 23 [23040/72641]\tLoss: 0.6342\tLR: 0.000001\n",
      "Training Epoch: 23 [23360/72641]\tLoss: 0.6162\tLR: 0.000001\n",
      "Training Epoch: 23 [23680/72641]\tLoss: 0.6584\tLR: 0.000001\n",
      "Training Epoch: 23 [24000/72641]\tLoss: 0.6048\tLR: 0.000001\n",
      "Training Epoch: 23 [24320/72641]\tLoss: 0.6415\tLR: 0.000001\n",
      "Training Epoch: 23 [24640/72641]\tLoss: 0.6601\tLR: 0.000001\n",
      "Training Epoch: 23 [24960/72641]\tLoss: 0.6215\tLR: 0.000001\n",
      "Training Epoch: 23 [25280/72641]\tLoss: 0.5930\tLR: 0.000001\n",
      "Training Epoch: 23 [25600/72641]\tLoss: 0.6374\tLR: 0.000001\n",
      "Training Epoch: 23 [25920/72641]\tLoss: 0.6372\tLR: 0.000001\n",
      "Training Epoch: 23 [26240/72641]\tLoss: 0.6564\tLR: 0.000001\n",
      "Training Epoch: 23 [26560/72641]\tLoss: 0.6368\tLR: 0.000001\n",
      "Training Epoch: 23 [26880/72641]\tLoss: 0.6196\tLR: 0.000001\n",
      "Training Epoch: 23 [27200/72641]\tLoss: 0.5958\tLR: 0.000001\n",
      "Training Epoch: 23 [27520/72641]\tLoss: 0.6840\tLR: 0.000001\n",
      "Training Epoch: 23 [27840/72641]\tLoss: 0.6578\tLR: 0.000001\n",
      "Training Epoch: 23 [28160/72641]\tLoss: 0.6308\tLR: 0.000001\n",
      "Training Epoch: 23 [28480/72641]\tLoss: 0.6136\tLR: 0.000001\n",
      "Training Epoch: 23 [28800/72641]\tLoss: 0.5883\tLR: 0.000001\n",
      "Training Epoch: 23 [29120/72641]\tLoss: 0.6314\tLR: 0.000001\n",
      "Training Epoch: 23 [29440/72641]\tLoss: 0.6173\tLR: 0.000001\n",
      "Training Epoch: 23 [29760/72641]\tLoss: 0.6245\tLR: 0.000001\n",
      "Training Epoch: 23 [30080/72641]\tLoss: 0.6680\tLR: 0.000001\n",
      "Training Epoch: 23 [30400/72641]\tLoss: 0.6351\tLR: 0.000001\n",
      "Training Epoch: 23 [30720/72641]\tLoss: 0.6396\tLR: 0.000001\n",
      "Training Epoch: 23 [31040/72641]\tLoss: 0.6106\tLR: 0.000001\n",
      "Training Epoch: 23 [31360/72641]\tLoss: 0.6362\tLR: 0.000001\n",
      "Training Epoch: 23 [31680/72641]\tLoss: 0.5963\tLR: 0.000001\n",
      "Training Epoch: 23 [32000/72641]\tLoss: 0.6476\tLR: 0.000001\n",
      "Training Epoch: 23 [32320/72641]\tLoss: 0.6590\tLR: 0.000001\n",
      "Training Epoch: 23 [32640/72641]\tLoss: 0.5985\tLR: 0.000001\n",
      "Training Epoch: 23 [32960/72641]\tLoss: 0.6151\tLR: 0.000001\n",
      "Training Epoch: 23 [33280/72641]\tLoss: 0.6229\tLR: 0.000001\n",
      "Training Epoch: 23 [33600/72641]\tLoss: 0.6120\tLR: 0.000001\n",
      "Training Epoch: 23 [33920/72641]\tLoss: 0.6182\tLR: 0.000001\n",
      "Training Epoch: 23 [34240/72641]\tLoss: 0.5902\tLR: 0.000001\n",
      "Training Epoch: 23 [34560/72641]\tLoss: 0.6445\tLR: 0.000001\n",
      "Training Epoch: 23 [34880/72641]\tLoss: 0.6692\tLR: 0.000001\n",
      "Training Epoch: 23 [35200/72641]\tLoss: 0.5981\tLR: 0.000001\n",
      "Training Epoch: 23 [35520/72641]\tLoss: 0.6292\tLR: 0.000001\n",
      "Training Epoch: 23 [35840/72641]\tLoss: 0.6594\tLR: 0.000001\n",
      "Training Epoch: 23 [36160/72641]\tLoss: 0.6653\tLR: 0.000001\n",
      "Training Epoch: 23 [36480/72641]\tLoss: 0.6286\tLR: 0.000001\n",
      "Training Epoch: 23 [36800/72641]\tLoss: 0.6283\tLR: 0.000001\n",
      "Training Epoch: 23 [37120/72641]\tLoss: 0.6235\tLR: 0.000001\n",
      "Training Epoch: 23 [37440/72641]\tLoss: 0.6646\tLR: 0.000001\n",
      "Training Epoch: 23 [37760/72641]\tLoss: 0.6130\tLR: 0.000001\n",
      "Training Epoch: 23 [38080/72641]\tLoss: 0.6317\tLR: 0.000001\n",
      "Training Epoch: 23 [38400/72641]\tLoss: 0.6385\tLR: 0.000001\n",
      "Training Epoch: 23 [38720/72641]\tLoss: 0.6456\tLR: 0.000001\n",
      "Training Epoch: 23 [39040/72641]\tLoss: 0.6211\tLR: 0.000001\n",
      "Training Epoch: 23 [39360/72641]\tLoss: 0.6178\tLR: 0.000001\n",
      "Training Epoch: 23 [39680/72641]\tLoss: 0.6336\tLR: 0.000001\n",
      "Training Epoch: 23 [40000/72641]\tLoss: 0.6267\tLR: 0.000001\n",
      "Training Epoch: 23 [40320/72641]\tLoss: 0.6479\tLR: 0.000001\n",
      "Training Epoch: 23 [40640/72641]\tLoss: 0.6119\tLR: 0.000001\n",
      "Training Epoch: 23 [40960/72641]\tLoss: 0.6312\tLR: 0.000001\n",
      "Training Epoch: 23 [41280/72641]\tLoss: 0.6336\tLR: 0.000001\n",
      "Training Epoch: 23 [41600/72641]\tLoss: 0.6446\tLR: 0.000001\n",
      "Training Epoch: 23 [41920/72641]\tLoss: 0.6027\tLR: 0.000001\n",
      "Training Epoch: 23 [42240/72641]\tLoss: 0.6035\tLR: 0.000001\n",
      "Training Epoch: 23 [42560/72641]\tLoss: 0.5928\tLR: 0.000001\n",
      "Training Epoch: 23 [42880/72641]\tLoss: 0.6573\tLR: 0.000001\n",
      "Training Epoch: 23 [43200/72641]\tLoss: 0.6452\tLR: 0.000001\n",
      "Training Epoch: 23 [43520/72641]\tLoss: 0.6399\tLR: 0.000001\n",
      "Training Epoch: 23 [43840/72641]\tLoss: 0.6409\tLR: 0.000001\n",
      "Training Epoch: 23 [44160/72641]\tLoss: 0.6421\tLR: 0.000001\n",
      "Training Epoch: 23 [44480/72641]\tLoss: 0.6197\tLR: 0.000001\n",
      "Training Epoch: 23 [44800/72641]\tLoss: 0.5571\tLR: 0.000001\n",
      "Training Epoch: 23 [45120/72641]\tLoss: 0.5852\tLR: 0.000001\n",
      "Training Epoch: 23 [45440/72641]\tLoss: 0.6415\tLR: 0.000001\n",
      "Training Epoch: 23 [45760/72641]\tLoss: 0.6161\tLR: 0.000001\n",
      "Training Epoch: 23 [46080/72641]\tLoss: 0.6195\tLR: 0.000001\n",
      "Training Epoch: 23 [46400/72641]\tLoss: 0.6841\tLR: 0.000001\n",
      "Training Epoch: 23 [46720/72641]\tLoss: 0.6453\tLR: 0.000001\n",
      "Training Epoch: 23 [47040/72641]\tLoss: 0.6204\tLR: 0.000001\n",
      "Training Epoch: 23 [47360/72641]\tLoss: 0.6052\tLR: 0.000001\n",
      "Training Epoch: 23 [47680/72641]\tLoss: 0.5945\tLR: 0.000001\n",
      "Training Epoch: 23 [48000/72641]\tLoss: 0.6260\tLR: 0.000001\n",
      "Training Epoch: 23 [48320/72641]\tLoss: 0.6753\tLR: 0.000001\n",
      "Training Epoch: 23 [48640/72641]\tLoss: 0.6458\tLR: 0.000001\n",
      "Training Epoch: 23 [48960/72641]\tLoss: 0.6437\tLR: 0.000001\n",
      "Training Epoch: 23 [49280/72641]\tLoss: 0.6487\tLR: 0.000001\n",
      "Training Epoch: 23 [49600/72641]\tLoss: 0.6217\tLR: 0.000001\n",
      "Training Epoch: 23 [49920/72641]\tLoss: 0.5883\tLR: 0.000001\n",
      "Training Epoch: 23 [50240/72641]\tLoss: 0.5932\tLR: 0.000001\n",
      "Training Epoch: 23 [50560/72641]\tLoss: 0.5901\tLR: 0.000001\n",
      "Training Epoch: 23 [50880/72641]\tLoss: 0.5938\tLR: 0.000001\n",
      "Training Epoch: 23 [51200/72641]\tLoss: 0.5993\tLR: 0.000001\n",
      "Training Epoch: 23 [51520/72641]\tLoss: 0.6066\tLR: 0.000001\n",
      "Training Epoch: 23 [51840/72641]\tLoss: 0.6206\tLR: 0.000001\n",
      "Training Epoch: 23 [52160/72641]\tLoss: 0.6289\tLR: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [52480/72641]\tLoss: 0.6632\tLR: 0.000001\n",
      "Training Epoch: 23 [52800/72641]\tLoss: 0.6234\tLR: 0.000001\n",
      "Training Epoch: 23 [53120/72641]\tLoss: 0.6129\tLR: 0.000001\n",
      "Training Epoch: 23 [53440/72641]\tLoss: 0.5903\tLR: 0.000001\n",
      "Training Epoch: 23 [53760/72641]\tLoss: 0.6671\tLR: 0.000001\n",
      "Training Epoch: 23 [54080/72641]\tLoss: 0.6152\tLR: 0.000001\n",
      "Training Epoch: 23 [54400/72641]\tLoss: 0.6500\tLR: 0.000001\n",
      "Training Epoch: 23 [54720/72641]\tLoss: 0.5725\tLR: 0.000001\n",
      "Training Epoch: 23 [55040/72641]\tLoss: 0.6826\tLR: 0.000001\n",
      "Training Epoch: 23 [55360/72641]\tLoss: 0.6251\tLR: 0.000001\n",
      "Training Epoch: 23 [55680/72641]\tLoss: 0.6077\tLR: 0.000001\n",
      "Training Epoch: 23 [56000/72641]\tLoss: 0.6117\tLR: 0.000001\n",
      "Training Epoch: 23 [56320/72641]\tLoss: 0.6282\tLR: 0.000001\n",
      "Training Epoch: 23 [56640/72641]\tLoss: 0.5982\tLR: 0.000001\n",
      "Training Epoch: 23 [56960/72641]\tLoss: 0.6284\tLR: 0.000001\n",
      "Training Epoch: 23 [57280/72641]\tLoss: 0.5778\tLR: 0.000001\n",
      "Training Epoch: 23 [57600/72641]\tLoss: 0.6036\tLR: 0.000001\n",
      "Training Epoch: 23 [57920/72641]\tLoss: 0.6496\tLR: 0.000001\n",
      "Training Epoch: 23 [58240/72641]\tLoss: 0.6045\tLR: 0.000001\n",
      "Training Epoch: 23 [58560/72641]\tLoss: 0.6086\tLR: 0.000001\n",
      "Training Epoch: 23 [58880/72641]\tLoss: 0.6207\tLR: 0.000001\n",
      "Training Epoch: 23 [59200/72641]\tLoss: 0.6242\tLR: 0.000001\n",
      "Training Epoch: 23 [59520/72641]\tLoss: 0.5823\tLR: 0.000001\n",
      "Training Epoch: 23 [59840/72641]\tLoss: 0.6311\tLR: 0.000001\n",
      "Training Epoch: 23 [60160/72641]\tLoss: 0.6412\tLR: 0.000001\n",
      "Training Epoch: 23 [60480/72641]\tLoss: 0.6219\tLR: 0.000001\n",
      "Training Epoch: 23 [60800/72641]\tLoss: 0.5827\tLR: 0.000001\n",
      "Training Epoch: 23 [61120/72641]\tLoss: 0.6194\tLR: 0.000001\n",
      "Training Epoch: 23 [61440/72641]\tLoss: 0.6052\tLR: 0.000001\n",
      "Training Epoch: 23 [61760/72641]\tLoss: 0.5869\tLR: 0.000001\n",
      "Training Epoch: 23 [62080/72641]\tLoss: 0.6566\tLR: 0.000001\n",
      "Training Epoch: 23 [62400/72641]\tLoss: 0.6401\tLR: 0.000001\n",
      "Training Epoch: 23 [62720/72641]\tLoss: 0.6802\tLR: 0.000001\n",
      "Training Epoch: 23 [63040/72641]\tLoss: 0.6223\tLR: 0.000001\n",
      "Training Epoch: 23 [63360/72641]\tLoss: 0.6615\tLR: 0.000001\n",
      "Training Epoch: 23 [63680/72641]\tLoss: 0.6421\tLR: 0.000001\n",
      "Training Epoch: 23 [64000/72641]\tLoss: 0.6287\tLR: 0.000001\n",
      "Training Epoch: 23 [64320/72641]\tLoss: 0.6596\tLR: 0.000001\n",
      "Training Epoch: 23 [64640/72641]\tLoss: 0.6264\tLR: 0.000001\n",
      "Training Epoch: 23 [64960/72641]\tLoss: 0.6417\tLR: 0.000001\n",
      "Training Epoch: 23 [65280/72641]\tLoss: 0.6150\tLR: 0.000001\n",
      "Training Epoch: 23 [65600/72641]\tLoss: 0.6273\tLR: 0.000001\n",
      "Training Epoch: 23 [65920/72641]\tLoss: 0.6385\tLR: 0.000001\n",
      "Training Epoch: 23 [66240/72641]\tLoss: 0.6422\tLR: 0.000001\n",
      "Training Epoch: 23 [66560/72641]\tLoss: 0.5963\tLR: 0.000001\n",
      "Training Epoch: 23 [66880/72641]\tLoss: 0.5716\tLR: 0.000001\n",
      "Training Epoch: 23 [67200/72641]\tLoss: 0.5985\tLR: 0.000001\n",
      "Training Epoch: 23 [67520/72641]\tLoss: 0.6540\tLR: 0.000001\n",
      "Training Epoch: 23 [67840/72641]\tLoss: 0.6273\tLR: 0.000001\n",
      "Training Epoch: 23 [68160/72641]\tLoss: 0.6497\tLR: 0.000001\n",
      "Training Epoch: 23 [68480/72641]\tLoss: 0.5863\tLR: 0.000001\n",
      "Training Epoch: 23 [68800/72641]\tLoss: 0.6510\tLR: 0.000001\n",
      "Training Epoch: 23 [69120/72641]\tLoss: 0.6943\tLR: 0.000001\n",
      "Training Epoch: 23 [69440/72641]\tLoss: 0.6226\tLR: 0.000001\n",
      "Training Epoch: 23 [69760/72641]\tLoss: 0.6233\tLR: 0.000001\n",
      "Training Epoch: 23 [70080/72641]\tLoss: 0.5912\tLR: 0.000001\n",
      "Training Epoch: 23 [70400/72641]\tLoss: 0.6241\tLR: 0.000001\n",
      "Training Epoch: 23 [70720/72641]\tLoss: 0.6520\tLR: 0.000001\n",
      "Training Epoch: 23 [71040/72641]\tLoss: 0.5977\tLR: 0.000001\n",
      "Training Epoch: 23 [71360/72641]\tLoss: 0.6527\tLR: 0.000001\n",
      "Training Epoch: 23 [71680/72641]\tLoss: 0.6881\tLR: 0.000001\n",
      "Training Epoch: 23 [72000/72641]\tLoss: 0.6196\tLR: 0.000001\n",
      "Training Epoch: 23 [72320/72641]\tLoss: 0.6549\tLR: 0.000001\n",
      "Training Epoch: 23 [72640/72641]\tLoss: 0.6254\tLR: 0.000001\n",
      "Val Result: Acc: 0.1472, C_ACC: 0.7087, DOA: 87.8230, ACC_k: 0.1066\n",
      "ext:0.0, cls:0.560756, coar:0.0, fine:0.0,\n",
      "Training Epoch: 24 [320/72641]\tLoss: 0.6132\tLR: 0.000001\n",
      "Training Epoch: 24 [640/72641]\tLoss: 0.6182\tLR: 0.000001\n",
      "Training Epoch: 24 [960/72641]\tLoss: 0.5867\tLR: 0.000001\n",
      "Training Epoch: 24 [1280/72641]\tLoss: 0.6168\tLR: 0.000001\n",
      "Training Epoch: 24 [1600/72641]\tLoss: 0.6299\tLR: 0.000001\n",
      "Training Epoch: 24 [1920/72641]\tLoss: 0.6250\tLR: 0.000001\n",
      "Training Epoch: 24 [2240/72641]\tLoss: 0.6061\tLR: 0.000001\n",
      "Training Epoch: 24 [2560/72641]\tLoss: 0.6340\tLR: 0.000001\n",
      "Training Epoch: 24 [2880/72641]\tLoss: 0.6789\tLR: 0.000001\n",
      "Training Epoch: 24 [3200/72641]\tLoss: 0.6126\tLR: 0.000001\n",
      "Training Epoch: 24 [3520/72641]\tLoss: 0.6440\tLR: 0.000001\n",
      "Training Epoch: 24 [3840/72641]\tLoss: 0.5876\tLR: 0.000001\n",
      "Training Epoch: 24 [4160/72641]\tLoss: 0.6287\tLR: 0.000001\n",
      "Training Epoch: 24 [4480/72641]\tLoss: 0.6192\tLR: 0.000001\n",
      "Training Epoch: 24 [4800/72641]\tLoss: 0.5881\tLR: 0.000001\n",
      "Training Epoch: 24 [5120/72641]\tLoss: 0.6351\tLR: 0.000001\n",
      "Training Epoch: 24 [5440/72641]\tLoss: 0.6557\tLR: 0.000001\n",
      "Training Epoch: 24 [5760/72641]\tLoss: 0.6266\tLR: 0.000001\n",
      "Training Epoch: 24 [6080/72641]\tLoss: 0.6024\tLR: 0.000001\n",
      "Training Epoch: 24 [6400/72641]\tLoss: 0.5737\tLR: 0.000001\n",
      "Training Epoch: 24 [6720/72641]\tLoss: 0.5854\tLR: 0.000001\n",
      "Training Epoch: 24 [7040/72641]\tLoss: 0.6354\tLR: 0.000001\n",
      "Training Epoch: 24 [7360/72641]\tLoss: 0.6130\tLR: 0.000001\n",
      "Training Epoch: 24 [7680/72641]\tLoss: 0.6314\tLR: 0.000001\n",
      "Training Epoch: 24 [8000/72641]\tLoss: 0.6093\tLR: 0.000001\n",
      "Training Epoch: 24 [8320/72641]\tLoss: 0.6505\tLR: 0.000001\n",
      "Training Epoch: 24 [8640/72641]\tLoss: 0.6132\tLR: 0.000001\n",
      "Training Epoch: 24 [8960/72641]\tLoss: 0.6021\tLR: 0.000001\n",
      "Training Epoch: 24 [9280/72641]\tLoss: 0.6071\tLR: 0.000001\n",
      "Training Epoch: 24 [9600/72641]\tLoss: 0.6345\tLR: 0.000001\n",
      "Training Epoch: 24 [9920/72641]\tLoss: 0.6401\tLR: 0.000001\n",
      "Training Epoch: 24 [10240/72641]\tLoss: 0.6197\tLR: 0.000001\n",
      "Training Epoch: 24 [10560/72641]\tLoss: 0.5878\tLR: 0.000001\n",
      "Training Epoch: 24 [10880/72641]\tLoss: 0.6359\tLR: 0.000001\n",
      "Training Epoch: 24 [11200/72641]\tLoss: 0.6360\tLR: 0.000001\n",
      "Training Epoch: 24 [11520/72641]\tLoss: 0.6106\tLR: 0.000001\n",
      "Training Epoch: 24 [11840/72641]\tLoss: 0.5835\tLR: 0.000001\n",
      "Training Epoch: 24 [12160/72641]\tLoss: 0.6106\tLR: 0.000001\n",
      "Training Epoch: 24 [12480/72641]\tLoss: 0.6188\tLR: 0.000001\n",
      "Training Epoch: 24 [12800/72641]\tLoss: 0.6192\tLR: 0.000001\n",
      "Training Epoch: 24 [13120/72641]\tLoss: 0.6658\tLR: 0.000001\n",
      "Training Epoch: 24 [13440/72641]\tLoss: 0.6138\tLR: 0.000001\n",
      "Training Epoch: 24 [13760/72641]\tLoss: 0.6858\tLR: 0.000001\n",
      "Training Epoch: 24 [14080/72641]\tLoss: 0.6597\tLR: 0.000001\n",
      "Training Epoch: 24 [14400/72641]\tLoss: 0.6214\tLR: 0.000001\n",
      "Training Epoch: 24 [14720/72641]\tLoss: 0.6095\tLR: 0.000001\n",
      "Training Epoch: 24 [15040/72641]\tLoss: 0.5599\tLR: 0.000001\n",
      "Training Epoch: 24 [15360/72641]\tLoss: 0.6012\tLR: 0.000001\n",
      "Training Epoch: 24 [15680/72641]\tLoss: 0.6672\tLR: 0.000001\n",
      "Training Epoch: 24 [16000/72641]\tLoss: 0.6259\tLR: 0.000001\n",
      "Training Epoch: 24 [16320/72641]\tLoss: 0.6436\tLR: 0.000001\n",
      "Training Epoch: 24 [16640/72641]\tLoss: 0.6542\tLR: 0.000001\n",
      "Training Epoch: 24 [16960/72641]\tLoss: 0.6327\tLR: 0.000001\n",
      "Training Epoch: 24 [17280/72641]\tLoss: 0.6555\tLR: 0.000001\n",
      "Training Epoch: 24 [17600/72641]\tLoss: 0.6017\tLR: 0.000001\n",
      "Training Epoch: 24 [17920/72641]\tLoss: 0.6799\tLR: 0.000001\n",
      "Training Epoch: 24 [18240/72641]\tLoss: 0.6437\tLR: 0.000001\n",
      "Training Epoch: 24 [18560/72641]\tLoss: 0.5854\tLR: 0.000001\n",
      "Training Epoch: 24 [18880/72641]\tLoss: 0.6330\tLR: 0.000001\n",
      "Training Epoch: 24 [19200/72641]\tLoss: 0.6373\tLR: 0.000001\n",
      "Training Epoch: 24 [19520/72641]\tLoss: 0.6101\tLR: 0.000001\n",
      "Training Epoch: 24 [19840/72641]\tLoss: 0.6449\tLR: 0.000001\n",
      "Training Epoch: 24 [20160/72641]\tLoss: 0.6151\tLR: 0.000001\n",
      "Training Epoch: 24 [20480/72641]\tLoss: 0.6190\tLR: 0.000001\n",
      "Training Epoch: 24 [20800/72641]\tLoss: 0.6191\tLR: 0.000001\n",
      "Training Epoch: 24 [21120/72641]\tLoss: 0.6157\tLR: 0.000001\n",
      "Training Epoch: 24 [21440/72641]\tLoss: 0.6166\tLR: 0.000001\n",
      "Training Epoch: 24 [21760/72641]\tLoss: 0.6453\tLR: 0.000001\n",
      "Training Epoch: 24 [22080/72641]\tLoss: 0.6332\tLR: 0.000001\n",
      "Training Epoch: 24 [22400/72641]\tLoss: 0.6046\tLR: 0.000001\n",
      "Training Epoch: 24 [22720/72641]\tLoss: 0.6137\tLR: 0.000001\n",
      "Training Epoch: 24 [23040/72641]\tLoss: 0.6273\tLR: 0.000001\n",
      "Training Epoch: 24 [23360/72641]\tLoss: 0.6040\tLR: 0.000001\n",
      "Training Epoch: 24 [23680/72641]\tLoss: 0.6421\tLR: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [24000/72641]\tLoss: 0.6359\tLR: 0.000001\n",
      "Training Epoch: 24 [24320/72641]\tLoss: 0.5960\tLR: 0.000001\n",
      "Training Epoch: 24 [24640/72641]\tLoss: 0.6376\tLR: 0.000001\n",
      "Training Epoch: 24 [24960/72641]\tLoss: 0.6269\tLR: 0.000001\n",
      "Training Epoch: 24 [25280/72641]\tLoss: 0.6207\tLR: 0.000001\n",
      "Training Epoch: 24 [25600/72641]\tLoss: 0.6722\tLR: 0.000001\n",
      "Training Epoch: 24 [25920/72641]\tLoss: 0.6556\tLR: 0.000001\n",
      "Training Epoch: 24 [26240/72641]\tLoss: 0.6476\tLR: 0.000001\n",
      "Training Epoch: 24 [26560/72641]\tLoss: 0.6215\tLR: 0.000001\n",
      "Training Epoch: 24 [26880/72641]\tLoss: 0.6566\tLR: 0.000001\n",
      "Training Epoch: 24 [27200/72641]\tLoss: 0.6242\tLR: 0.000001\n",
      "Training Epoch: 24 [27520/72641]\tLoss: 0.6753\tLR: 0.000001\n",
      "Training Epoch: 24 [27840/72641]\tLoss: 0.6229\tLR: 0.000001\n",
      "Training Epoch: 24 [28160/72641]\tLoss: 0.6228\tLR: 0.000001\n",
      "Training Epoch: 24 [28480/72641]\tLoss: 0.6334\tLR: 0.000001\n",
      "Training Epoch: 24 [28800/72641]\tLoss: 0.6212\tLR: 0.000001\n",
      "Training Epoch: 24 [29120/72641]\tLoss: 0.6131\tLR: 0.000001\n",
      "Training Epoch: 24 [29440/72641]\tLoss: 0.5847\tLR: 0.000001\n",
      "Training Epoch: 24 [29760/72641]\tLoss: 0.6200\tLR: 0.000001\n",
      "Training Epoch: 24 [30080/72641]\tLoss: 0.6538\tLR: 0.000001\n",
      "Training Epoch: 24 [30400/72641]\tLoss: 0.5925\tLR: 0.000001\n",
      "Training Epoch: 24 [30720/72641]\tLoss: 0.6129\tLR: 0.000001\n",
      "Training Epoch: 24 [31040/72641]\tLoss: 0.6044\tLR: 0.000001\n",
      "Training Epoch: 24 [31360/72641]\tLoss: 0.5651\tLR: 0.000001\n",
      "Training Epoch: 24 [31680/72641]\tLoss: 0.6174\tLR: 0.000001\n",
      "Training Epoch: 24 [32000/72641]\tLoss: 0.6757\tLR: 0.000001\n",
      "Training Epoch: 24 [32320/72641]\tLoss: 0.6961\tLR: 0.000001\n",
      "Training Epoch: 24 [32640/72641]\tLoss: 0.6221\tLR: 0.000001\n",
      "Training Epoch: 24 [32960/72641]\tLoss: 0.6555\tLR: 0.000001\n",
      "Training Epoch: 24 [33280/72641]\tLoss: 0.6428\tLR: 0.000001\n",
      "Training Epoch: 24 [33600/72641]\tLoss: 0.5935\tLR: 0.000001\n",
      "Training Epoch: 24 [33920/72641]\tLoss: 0.6439\tLR: 0.000001\n",
      "Training Epoch: 24 [34240/72641]\tLoss: 0.5990\tLR: 0.000001\n",
      "Training Epoch: 24 [34560/72641]\tLoss: 0.6848\tLR: 0.000001\n",
      "Training Epoch: 24 [34880/72641]\tLoss: 0.6160\tLR: 0.000001\n",
      "Training Epoch: 24 [35200/72641]\tLoss: 0.6380\tLR: 0.000001\n",
      "Training Epoch: 24 [35520/72641]\tLoss: 0.6143\tLR: 0.000001\n",
      "Training Epoch: 24 [35840/72641]\tLoss: 0.6185\tLR: 0.000001\n",
      "Training Epoch: 24 [36160/72641]\tLoss: 0.6679\tLR: 0.000001\n",
      "Training Epoch: 24 [36480/72641]\tLoss: 0.6498\tLR: 0.000001\n",
      "Training Epoch: 24 [36800/72641]\tLoss: 0.6197\tLR: 0.000001\n",
      "Training Epoch: 24 [37120/72641]\tLoss: 0.6471\tLR: 0.000001\n",
      "Training Epoch: 24 [37440/72641]\tLoss: 0.6280\tLR: 0.000001\n",
      "Training Epoch: 24 [37760/72641]\tLoss: 0.6601\tLR: 0.000001\n",
      "Training Epoch: 24 [38080/72641]\tLoss: 0.6128\tLR: 0.000001\n",
      "Training Epoch: 24 [38400/72641]\tLoss: 0.6618\tLR: 0.000001\n",
      "Training Epoch: 24 [38720/72641]\tLoss: 0.6410\tLR: 0.000001\n",
      "Training Epoch: 24 [39040/72641]\tLoss: 0.6233\tLR: 0.000001\n",
      "Training Epoch: 24 [39360/72641]\tLoss: 0.5845\tLR: 0.000001\n",
      "Training Epoch: 24 [39680/72641]\tLoss: 0.5609\tLR: 0.000001\n",
      "Training Epoch: 24 [40000/72641]\tLoss: 0.6087\tLR: 0.000001\n",
      "Training Epoch: 24 [40320/72641]\tLoss: 0.6326\tLR: 0.000001\n",
      "Training Epoch: 24 [40640/72641]\tLoss: 0.6219\tLR: 0.000001\n",
      "Training Epoch: 24 [40960/72641]\tLoss: 0.6075\tLR: 0.000001\n",
      "Training Epoch: 24 [41280/72641]\tLoss: 0.6459\tLR: 0.000001\n",
      "Training Epoch: 24 [41600/72641]\tLoss: 0.6038\tLR: 0.000001\n",
      "Training Epoch: 24 [41920/72641]\tLoss: 0.5981\tLR: 0.000001\n",
      "Training Epoch: 24 [42240/72641]\tLoss: 0.5651\tLR: 0.000001\n",
      "Training Epoch: 24 [42560/72641]\tLoss: 0.6042\tLR: 0.000001\n",
      "Training Epoch: 24 [42880/72641]\tLoss: 0.6641\tLR: 0.000001\n",
      "Training Epoch: 24 [43200/72641]\tLoss: 0.6623\tLR: 0.000001\n",
      "Training Epoch: 24 [43520/72641]\tLoss: 0.6524\tLR: 0.000001\n",
      "Training Epoch: 24 [43840/72641]\tLoss: 0.6181\tLR: 0.000001\n",
      "Training Epoch: 24 [44160/72641]\tLoss: 0.6695\tLR: 0.000001\n",
      "Training Epoch: 24 [44480/72641]\tLoss: 0.6594\tLR: 0.000001\n",
      "Training Epoch: 24 [44800/72641]\tLoss: 0.6328\tLR: 0.000001\n",
      "Training Epoch: 24 [45120/72641]\tLoss: 0.5794\tLR: 0.000001\n",
      "Training Epoch: 24 [45440/72641]\tLoss: 0.6439\tLR: 0.000001\n",
      "Training Epoch: 24 [45760/72641]\tLoss: 0.6275\tLR: 0.000001\n",
      "Training Epoch: 24 [46080/72641]\tLoss: 0.6255\tLR: 0.000001\n",
      "Training Epoch: 24 [46400/72641]\tLoss: 0.6179\tLR: 0.000001\n",
      "Training Epoch: 24 [46720/72641]\tLoss: 0.6936\tLR: 0.000001\n",
      "Training Epoch: 24 [47040/72641]\tLoss: 0.6121\tLR: 0.000001\n",
      "Training Epoch: 24 [47360/72641]\tLoss: 0.6452\tLR: 0.000001\n",
      "Training Epoch: 24 [47680/72641]\tLoss: 0.6078\tLR: 0.000001\n",
      "Training Epoch: 24 [48000/72641]\tLoss: 0.6381\tLR: 0.000001\n",
      "Training Epoch: 24 [48320/72641]\tLoss: 0.6111\tLR: 0.000001\n",
      "Training Epoch: 24 [48640/72641]\tLoss: 0.6324\tLR: 0.000001\n",
      "Training Epoch: 24 [48960/72641]\tLoss: 0.6299\tLR: 0.000001\n",
      "Training Epoch: 24 [49280/72641]\tLoss: 0.5909\tLR: 0.000001\n",
      "Training Epoch: 24 [49600/72641]\tLoss: 0.6189\tLR: 0.000001\n",
      "Training Epoch: 24 [49920/72641]\tLoss: 0.6143\tLR: 0.000001\n",
      "Training Epoch: 24 [50240/72641]\tLoss: 0.5929\tLR: 0.000001\n",
      "Training Epoch: 24 [50560/72641]\tLoss: 0.5789\tLR: 0.000001\n",
      "Training Epoch: 24 [50880/72641]\tLoss: 0.6058\tLR: 0.000001\n",
      "Training Epoch: 24 [51200/72641]\tLoss: 0.6566\tLR: 0.000001\n",
      "Training Epoch: 24 [51520/72641]\tLoss: 0.6504\tLR: 0.000001\n",
      "Training Epoch: 24 [51840/72641]\tLoss: 0.6232\tLR: 0.000001\n",
      "Training Epoch: 24 [52160/72641]\tLoss: 0.6696\tLR: 0.000001\n",
      "Training Epoch: 24 [52480/72641]\tLoss: 0.5894\tLR: 0.000001\n",
      "Training Epoch: 24 [52800/72641]\tLoss: 0.6401\tLR: 0.000001\n",
      "Training Epoch: 24 [53120/72641]\tLoss: 0.6159\tLR: 0.000001\n",
      "Training Epoch: 24 [53440/72641]\tLoss: 0.6328\tLR: 0.000001\n",
      "Training Epoch: 24 [53760/72641]\tLoss: 0.6390\tLR: 0.000001\n",
      "Training Epoch: 24 [54080/72641]\tLoss: 0.6495\tLR: 0.000001\n",
      "Training Epoch: 24 [54400/72641]\tLoss: 0.5857\tLR: 0.000001\n",
      "Training Epoch: 24 [54720/72641]\tLoss: 0.6199\tLR: 0.000001\n",
      "Training Epoch: 24 [55040/72641]\tLoss: 0.6504\tLR: 0.000001\n",
      "Training Epoch: 24 [55360/72641]\tLoss: 0.6144\tLR: 0.000001\n",
      "Training Epoch: 24 [55680/72641]\tLoss: 0.5864\tLR: 0.000001\n",
      "Training Epoch: 24 [56000/72641]\tLoss: 0.6396\tLR: 0.000001\n",
      "Training Epoch: 24 [56320/72641]\tLoss: 0.6531\tLR: 0.000001\n",
      "Training Epoch: 24 [56640/72641]\tLoss: 0.6027\tLR: 0.000001\n",
      "Training Epoch: 24 [56960/72641]\tLoss: 0.6555\tLR: 0.000001\n",
      "Training Epoch: 24 [57280/72641]\tLoss: 0.6216\tLR: 0.000001\n",
      "Training Epoch: 24 [57600/72641]\tLoss: 0.6138\tLR: 0.000001\n",
      "Training Epoch: 24 [57920/72641]\tLoss: 0.6191\tLR: 0.000001\n",
      "Training Epoch: 24 [58240/72641]\tLoss: 0.6257\tLR: 0.000001\n",
      "Training Epoch: 24 [58560/72641]\tLoss: 0.6042\tLR: 0.000001\n",
      "Training Epoch: 24 [58880/72641]\tLoss: 0.6142\tLR: 0.000001\n",
      "Training Epoch: 24 [59200/72641]\tLoss: 0.6594\tLR: 0.000001\n",
      "Training Epoch: 24 [59520/72641]\tLoss: 0.6512\tLR: 0.000001\n",
      "Training Epoch: 24 [59840/72641]\tLoss: 0.6158\tLR: 0.000001\n",
      "Training Epoch: 24 [60160/72641]\tLoss: 0.6412\tLR: 0.000001\n",
      "Training Epoch: 24 [60480/72641]\tLoss: 0.6678\tLR: 0.000001\n",
      "Training Epoch: 24 [60800/72641]\tLoss: 0.5866\tLR: 0.000001\n",
      "Training Epoch: 24 [61120/72641]\tLoss: 0.5793\tLR: 0.000001\n",
      "Training Epoch: 24 [61440/72641]\tLoss: 0.5655\tLR: 0.000001\n",
      "Training Epoch: 24 [61760/72641]\tLoss: 0.6389\tLR: 0.000001\n",
      "Training Epoch: 24 [62080/72641]\tLoss: 0.6287\tLR: 0.000001\n",
      "Training Epoch: 24 [62400/72641]\tLoss: 0.6705\tLR: 0.000001\n",
      "Training Epoch: 24 [62720/72641]\tLoss: 0.6031\tLR: 0.000001\n",
      "Training Epoch: 24 [63040/72641]\tLoss: 0.6483\tLR: 0.000001\n",
      "Training Epoch: 24 [63360/72641]\tLoss: 0.6652\tLR: 0.000001\n",
      "Training Epoch: 24 [63680/72641]\tLoss: 0.5900\tLR: 0.000001\n",
      "Training Epoch: 24 [64000/72641]\tLoss: 0.6109\tLR: 0.000001\n",
      "Training Epoch: 24 [64320/72641]\tLoss: 0.6437\tLR: 0.000001\n",
      "Training Epoch: 24 [64640/72641]\tLoss: 0.6259\tLR: 0.000001\n",
      "Training Epoch: 24 [64960/72641]\tLoss: 0.6845\tLR: 0.000001\n",
      "Training Epoch: 24 [65280/72641]\tLoss: 0.5857\tLR: 0.000001\n",
      "Training Epoch: 24 [65600/72641]\tLoss: 0.6794\tLR: 0.000001\n",
      "Training Epoch: 24 [65920/72641]\tLoss: 0.6504\tLR: 0.000001\n",
      "Training Epoch: 24 [66240/72641]\tLoss: 0.6676\tLR: 0.000001\n",
      "Training Epoch: 24 [66560/72641]\tLoss: 0.6546\tLR: 0.000001\n",
      "Training Epoch: 24 [66880/72641]\tLoss: 0.6131\tLR: 0.000001\n",
      "Training Epoch: 24 [67200/72641]\tLoss: 0.6224\tLR: 0.000001\n",
      "Training Epoch: 24 [67520/72641]\tLoss: 0.6364\tLR: 0.000001\n",
      "Training Epoch: 24 [67840/72641]\tLoss: 0.6014\tLR: 0.000001\n",
      "Training Epoch: 24 [68160/72641]\tLoss: 0.6493\tLR: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [68480/72641]\tLoss: 0.6682\tLR: 0.000001\n",
      "Training Epoch: 24 [68800/72641]\tLoss: 0.6205\tLR: 0.000001\n",
      "Training Epoch: 24 [69120/72641]\tLoss: 0.6601\tLR: 0.000001\n",
      "Training Epoch: 24 [69440/72641]\tLoss: 0.6527\tLR: 0.000001\n",
      "Training Epoch: 24 [69760/72641]\tLoss: 0.6347\tLR: 0.000001\n",
      "Training Epoch: 24 [70080/72641]\tLoss: 0.5807\tLR: 0.000001\n",
      "Training Epoch: 24 [70400/72641]\tLoss: 0.6464\tLR: 0.000001\n",
      "Training Epoch: 24 [70720/72641]\tLoss: 0.6657\tLR: 0.000001\n",
      "Training Epoch: 24 [71040/72641]\tLoss: 0.5983\tLR: 0.000001\n",
      "Training Epoch: 24 [71360/72641]\tLoss: 0.6714\tLR: 0.000001\n",
      "Training Epoch: 24 [71680/72641]\tLoss: 0.6499\tLR: 0.000001\n",
      "Training Epoch: 24 [72000/72641]\tLoss: 0.6447\tLR: 0.000001\n",
      "Training Epoch: 24 [72320/72641]\tLoss: 0.6214\tLR: 0.000001\n",
      "Training Epoch: 24 [72640/72641]\tLoss: 0.6215\tLR: 0.000001\n",
      "Val Result: Acc: 0.1462, C_ACC: 0.7002, DOA: 88.5072, ACC_k: 0.1043\n",
      "ext:0.0, cls:0.579333, coar:0.0, fine:0.0,\n",
      "Training Epoch: 25 [320/72641]\tLoss: 0.6406\tLR: 0.000001\n",
      "Training Epoch: 25 [640/72641]\tLoss: 0.6172\tLR: 0.000001\n",
      "Training Epoch: 25 [960/72641]\tLoss: 0.6189\tLR: 0.000001\n",
      "Training Epoch: 25 [1280/72641]\tLoss: 0.6447\tLR: 0.000001\n",
      "Training Epoch: 25 [1600/72641]\tLoss: 0.6525\tLR: 0.000001\n",
      "Training Epoch: 25 [1920/72641]\tLoss: 0.6357\tLR: 0.000001\n",
      "Training Epoch: 25 [2240/72641]\tLoss: 0.6389\tLR: 0.000001\n",
      "Training Epoch: 25 [2560/72641]\tLoss: 0.6737\tLR: 0.000001\n",
      "Training Epoch: 25 [2880/72641]\tLoss: 0.6322\tLR: 0.000001\n",
      "Training Epoch: 25 [3200/72641]\tLoss: 0.6183\tLR: 0.000001\n",
      "Training Epoch: 25 [3520/72641]\tLoss: 0.6057\tLR: 0.000001\n",
      "Training Epoch: 25 [3840/72641]\tLoss: 0.5768\tLR: 0.000001\n",
      "Training Epoch: 25 [4160/72641]\tLoss: 0.6264\tLR: 0.000001\n",
      "Training Epoch: 25 [4480/72641]\tLoss: 0.6276\tLR: 0.000001\n",
      "Training Epoch: 25 [4800/72641]\tLoss: 0.5962\tLR: 0.000001\n",
      "Training Epoch: 25 [5120/72641]\tLoss: 0.6085\tLR: 0.000001\n",
      "Training Epoch: 25 [5440/72641]\tLoss: 0.6056\tLR: 0.000001\n",
      "Training Epoch: 25 [5760/72641]\tLoss: 0.6139\tLR: 0.000001\n",
      "Training Epoch: 25 [6080/72641]\tLoss: 0.5730\tLR: 0.000001\n",
      "Training Epoch: 25 [6400/72641]\tLoss: 0.6201\tLR: 0.000001\n",
      "Training Epoch: 25 [6720/72641]\tLoss: 0.6266\tLR: 0.000001\n",
      "Training Epoch: 25 [7040/72641]\tLoss: 0.6368\tLR: 0.000001\n",
      "Training Epoch: 25 [7360/72641]\tLoss: 0.6095\tLR: 0.000001\n",
      "Training Epoch: 25 [7680/72641]\tLoss: 0.6643\tLR: 0.000001\n",
      "Training Epoch: 25 [8000/72641]\tLoss: 0.6255\tLR: 0.000001\n",
      "Training Epoch: 25 [8320/72641]\tLoss: 0.6179\tLR: 0.000001\n",
      "Training Epoch: 25 [8640/72641]\tLoss: 0.6728\tLR: 0.000001\n",
      "Training Epoch: 25 [8960/72641]\tLoss: 0.6055\tLR: 0.000001\n",
      "Training Epoch: 25 [9280/72641]\tLoss: 0.5817\tLR: 0.000001\n",
      "Training Epoch: 25 [9600/72641]\tLoss: 0.6446\tLR: 0.000001\n",
      "Training Epoch: 25 [9920/72641]\tLoss: 0.6531\tLR: 0.000001\n",
      "Training Epoch: 25 [10240/72641]\tLoss: 0.6291\tLR: 0.000001\n",
      "Training Epoch: 25 [10560/72641]\tLoss: 0.6134\tLR: 0.000001\n",
      "Training Epoch: 25 [10880/72641]\tLoss: 0.5943\tLR: 0.000001\n",
      "Training Epoch: 25 [11200/72641]\tLoss: 0.5469\tLR: 0.000001\n",
      "Training Epoch: 25 [11520/72641]\tLoss: 0.5981\tLR: 0.000001\n",
      "Training Epoch: 25 [11840/72641]\tLoss: 0.6243\tLR: 0.000001\n",
      "Training Epoch: 25 [12160/72641]\tLoss: 0.5829\tLR: 0.000001\n",
      "Training Epoch: 25 [12480/72641]\tLoss: 0.6298\tLR: 0.000001\n",
      "Training Epoch: 25 [12800/72641]\tLoss: 0.6356\tLR: 0.000001\n",
      "Training Epoch: 25 [13120/72641]\tLoss: 0.5962\tLR: 0.000001\n",
      "Training Epoch: 25 [13440/72641]\tLoss: 0.6177\tLR: 0.000001\n",
      "Training Epoch: 25 [13760/72641]\tLoss: 0.6544\tLR: 0.000001\n",
      "Training Epoch: 25 [14080/72641]\tLoss: 0.6664\tLR: 0.000001\n",
      "Training Epoch: 25 [14400/72641]\tLoss: 0.5924\tLR: 0.000001\n",
      "Training Epoch: 25 [14720/72641]\tLoss: 0.6348\tLR: 0.000001\n",
      "Training Epoch: 25 [15040/72641]\tLoss: 0.6263\tLR: 0.000001\n",
      "Training Epoch: 25 [15360/72641]\tLoss: 0.6793\tLR: 0.000001\n",
      "Training Epoch: 25 [15680/72641]\tLoss: 0.6061\tLR: 0.000001\n",
      "Training Epoch: 25 [16000/72641]\tLoss: 0.6282\tLR: 0.000001\n",
      "Training Epoch: 25 [16320/72641]\tLoss: 0.6584\tLR: 0.000001\n",
      "Training Epoch: 25 [16640/72641]\tLoss: 0.6438\tLR: 0.000001\n",
      "Training Epoch: 25 [16960/72641]\tLoss: 0.6216\tLR: 0.000001\n",
      "Training Epoch: 25 [17280/72641]\tLoss: 0.5768\tLR: 0.000001\n",
      "Training Epoch: 25 [17600/72641]\tLoss: 0.5958\tLR: 0.000001\n",
      "Training Epoch: 25 [17920/72641]\tLoss: 0.6283\tLR: 0.000001\n",
      "Training Epoch: 25 [18240/72641]\tLoss: 0.6191\tLR: 0.000001\n",
      "Training Epoch: 25 [18560/72641]\tLoss: 0.6061\tLR: 0.000001\n",
      "Training Epoch: 25 [18880/72641]\tLoss: 0.6429\tLR: 0.000001\n",
      "Training Epoch: 25 [19200/72641]\tLoss: 0.6734\tLR: 0.000001\n",
      "Training Epoch: 25 [19520/72641]\tLoss: 0.6397\tLR: 0.000001\n",
      "Training Epoch: 25 [19840/72641]\tLoss: 0.6184\tLR: 0.000001\n",
      "Training Epoch: 25 [20160/72641]\tLoss: 0.5897\tLR: 0.000001\n",
      "Training Epoch: 25 [20480/72641]\tLoss: 0.6149\tLR: 0.000001\n",
      "Training Epoch: 25 [20800/72641]\tLoss: 0.6445\tLR: 0.000001\n",
      "Training Epoch: 25 [21120/72641]\tLoss: 0.6136\tLR: 0.000001\n",
      "Training Epoch: 25 [21440/72641]\tLoss: 0.6394\tLR: 0.000001\n",
      "Training Epoch: 25 [21760/72641]\tLoss: 0.6655\tLR: 0.000001\n",
      "Training Epoch: 25 [22080/72641]\tLoss: 0.6494\tLR: 0.000001\n",
      "Training Epoch: 25 [22400/72641]\tLoss: 0.5630\tLR: 0.000001\n",
      "Training Epoch: 25 [22720/72641]\tLoss: 0.5655\tLR: 0.000001\n",
      "Training Epoch: 25 [23040/72641]\tLoss: 0.6310\tLR: 0.000001\n",
      "Training Epoch: 25 [23360/72641]\tLoss: 0.6347\tLR: 0.000001\n",
      "Training Epoch: 25 [23680/72641]\tLoss: 0.5823\tLR: 0.000001\n",
      "Training Epoch: 25 [24000/72641]\tLoss: 0.6172\tLR: 0.000001\n",
      "Training Epoch: 25 [24320/72641]\tLoss: 0.6023\tLR: 0.000001\n",
      "Training Epoch: 25 [24640/72641]\tLoss: 0.6617\tLR: 0.000001\n",
      "Training Epoch: 25 [24960/72641]\tLoss: 0.6550\tLR: 0.000001\n",
      "Training Epoch: 25 [25280/72641]\tLoss: 0.5689\tLR: 0.000001\n",
      "Training Epoch: 25 [25600/72641]\tLoss: 0.6195\tLR: 0.000001\n",
      "Training Epoch: 25 [25920/72641]\tLoss: 0.6033\tLR: 0.000001\n",
      "Training Epoch: 25 [26240/72641]\tLoss: 0.6389\tLR: 0.000001\n",
      "Training Epoch: 25 [26560/72641]\tLoss: 0.6333\tLR: 0.000001\n",
      "Training Epoch: 25 [26880/72641]\tLoss: 0.5830\tLR: 0.000001\n",
      "Training Epoch: 25 [27200/72641]\tLoss: 0.5783\tLR: 0.000001\n",
      "Training Epoch: 25 [27520/72641]\tLoss: 0.6612\tLR: 0.000001\n",
      "Training Epoch: 25 [27840/72641]\tLoss: 0.6490\tLR: 0.000001\n",
      "Training Epoch: 25 [28160/72641]\tLoss: 0.6376\tLR: 0.000001\n",
      "Training Epoch: 25 [28480/72641]\tLoss: 0.6164\tLR: 0.000001\n",
      "Training Epoch: 25 [28800/72641]\tLoss: 0.5979\tLR: 0.000001\n",
      "Training Epoch: 25 [29120/72641]\tLoss: 0.6469\tLR: 0.000001\n",
      "Training Epoch: 25 [29440/72641]\tLoss: 0.5982\tLR: 0.000001\n",
      "Training Epoch: 25 [29760/72641]\tLoss: 0.6279\tLR: 0.000001\n",
      "Training Epoch: 25 [30080/72641]\tLoss: 0.6625\tLR: 0.000001\n",
      "Training Epoch: 25 [30400/72641]\tLoss: 0.5987\tLR: 0.000001\n",
      "Training Epoch: 25 [30720/72641]\tLoss: 0.6039\tLR: 0.000001\n",
      "Training Epoch: 25 [31040/72641]\tLoss: 0.5998\tLR: 0.000001\n",
      "Training Epoch: 25 [31360/72641]\tLoss: 0.6393\tLR: 0.000001\n",
      "Training Epoch: 25 [31680/72641]\tLoss: 0.6247\tLR: 0.000001\n",
      "Training Epoch: 25 [32000/72641]\tLoss: 0.6197\tLR: 0.000001\n",
      "Training Epoch: 25 [32320/72641]\tLoss: 0.6432\tLR: 0.000001\n",
      "Training Epoch: 25 [32640/72641]\tLoss: 0.6008\tLR: 0.000001\n",
      "Training Epoch: 25 [32960/72641]\tLoss: 0.6349\tLR: 0.000001\n",
      "Training Epoch: 25 [33280/72641]\tLoss: 0.6661\tLR: 0.000001\n",
      "Training Epoch: 25 [33600/72641]\tLoss: 0.6061\tLR: 0.000001\n",
      "Training Epoch: 25 [33920/72641]\tLoss: 0.6393\tLR: 0.000001\n",
      "Training Epoch: 25 [34240/72641]\tLoss: 0.6015\tLR: 0.000001\n",
      "Training Epoch: 25 [34560/72641]\tLoss: 0.6294\tLR: 0.000001\n",
      "Training Epoch: 25 [34880/72641]\tLoss: 0.6408\tLR: 0.000001\n",
      "Training Epoch: 25 [35200/72641]\tLoss: 0.6597\tLR: 0.000001\n",
      "Training Epoch: 25 [35520/72641]\tLoss: 0.6191\tLR: 0.000001\n",
      "Training Epoch: 25 [35840/72641]\tLoss: 0.6204\tLR: 0.000001\n",
      "Training Epoch: 25 [36160/72641]\tLoss: 0.6447\tLR: 0.000001\n",
      "Training Epoch: 25 [36480/72641]\tLoss: 0.6184\tLR: 0.000001\n",
      "Training Epoch: 25 [36800/72641]\tLoss: 0.6253\tLR: 0.000001\n",
      "Training Epoch: 25 [37120/72641]\tLoss: 0.6210\tLR: 0.000001\n",
      "Training Epoch: 25 [37440/72641]\tLoss: 0.6523\tLR: 0.000001\n",
      "Training Epoch: 25 [37760/72641]\tLoss: 0.6098\tLR: 0.000001\n",
      "Training Epoch: 25 [38080/72641]\tLoss: 0.6469\tLR: 0.000001\n",
      "Training Epoch: 25 [38400/72641]\tLoss: 0.6654\tLR: 0.000001\n",
      "Training Epoch: 25 [38720/72641]\tLoss: 0.6381\tLR: 0.000001\n",
      "Training Epoch: 25 [39040/72641]\tLoss: 0.6443\tLR: 0.000001\n",
      "Training Epoch: 25 [39360/72641]\tLoss: 0.6209\tLR: 0.000001\n",
      "Training Epoch: 25 [39680/72641]\tLoss: 0.6015\tLR: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [40000/72641]\tLoss: 0.6755\tLR: 0.000001\n",
      "Training Epoch: 25 [40320/72641]\tLoss: 0.6078\tLR: 0.000001\n",
      "Training Epoch: 25 [40640/72641]\tLoss: 0.6031\tLR: 0.000001\n",
      "Training Epoch: 25 [40960/72641]\tLoss: 0.6399\tLR: 0.000001\n",
      "Training Epoch: 25 [41280/72641]\tLoss: 0.6046\tLR: 0.000001\n",
      "Training Epoch: 25 [41600/72641]\tLoss: 0.6248\tLR: 0.000001\n",
      "Training Epoch: 25 [41920/72641]\tLoss: 0.6167\tLR: 0.000001\n",
      "Training Epoch: 25 [42240/72641]\tLoss: 0.6314\tLR: 0.000001\n",
      "Training Epoch: 25 [42560/72641]\tLoss: 0.6130\tLR: 0.000001\n",
      "Training Epoch: 25 [42880/72641]\tLoss: 0.6161\tLR: 0.000001\n",
      "Training Epoch: 25 [43200/72641]\tLoss: 0.6361\tLR: 0.000001\n",
      "Training Epoch: 25 [43520/72641]\tLoss: 0.6640\tLR: 0.000001\n",
      "Training Epoch: 25 [43840/72641]\tLoss: 0.6448\tLR: 0.000001\n",
      "Training Epoch: 25 [44160/72641]\tLoss: 0.6544\tLR: 0.000001\n",
      "Training Epoch: 25 [44480/72641]\tLoss: 0.6395\tLR: 0.000001\n",
      "Training Epoch: 25 [44800/72641]\tLoss: 0.6269\tLR: 0.000001\n",
      "Training Epoch: 25 [45120/72641]\tLoss: 0.5994\tLR: 0.000001\n",
      "Training Epoch: 25 [45440/72641]\tLoss: 0.6546\tLR: 0.000001\n",
      "Training Epoch: 25 [45760/72641]\tLoss: 0.6337\tLR: 0.000001\n",
      "Training Epoch: 25 [46080/72641]\tLoss: 0.6459\tLR: 0.000001\n",
      "Training Epoch: 25 [46400/72641]\tLoss: 0.6133\tLR: 0.000001\n",
      "Training Epoch: 25 [46720/72641]\tLoss: 0.6035\tLR: 0.000001\n",
      "Training Epoch: 25 [47040/72641]\tLoss: 0.6267\tLR: 0.000001\n",
      "Training Epoch: 25 [47360/72641]\tLoss: 0.6600\tLR: 0.000001\n",
      "Training Epoch: 25 [47680/72641]\tLoss: 0.6135\tLR: 0.000001\n",
      "Training Epoch: 25 [48000/72641]\tLoss: 0.6278\tLR: 0.000001\n",
      "Training Epoch: 25 [48320/72641]\tLoss: 0.6628\tLR: 0.000001\n",
      "Training Epoch: 25 [48640/72641]\tLoss: 0.5976\tLR: 0.000001\n",
      "Training Epoch: 25 [48960/72641]\tLoss: 0.6151\tLR: 0.000001\n",
      "Training Epoch: 25 [49280/72641]\tLoss: 0.6075\tLR: 0.000001\n",
      "Training Epoch: 25 [49600/72641]\tLoss: 0.6426\tLR: 0.000001\n",
      "Training Epoch: 25 [49920/72641]\tLoss: 0.6073\tLR: 0.000001\n",
      "Training Epoch: 25 [50240/72641]\tLoss: 0.6237\tLR: 0.000001\n",
      "Training Epoch: 25 [50560/72641]\tLoss: 0.5933\tLR: 0.000001\n",
      "Training Epoch: 25 [50880/72641]\tLoss: 0.6441\tLR: 0.000001\n",
      "Training Epoch: 25 [51200/72641]\tLoss: 0.6569\tLR: 0.000001\n",
      "Training Epoch: 25 [51520/72641]\tLoss: 0.6139\tLR: 0.000001\n",
      "Training Epoch: 25 [51840/72641]\tLoss: 0.6258\tLR: 0.000001\n",
      "Training Epoch: 25 [52160/72641]\tLoss: 0.6475\tLR: 0.000001\n",
      "Training Epoch: 25 [52480/72641]\tLoss: 0.5826\tLR: 0.000001\n",
      "Training Epoch: 25 [52800/72641]\tLoss: 0.6963\tLR: 0.000001\n",
      "Training Epoch: 25 [53120/72641]\tLoss: 0.5813\tLR: 0.000001\n",
      "Training Epoch: 25 [53440/72641]\tLoss: 0.6292\tLR: 0.000001\n",
      "Training Epoch: 25 [53760/72641]\tLoss: 0.6770\tLR: 0.000001\n",
      "Training Epoch: 25 [54080/72641]\tLoss: 0.5995\tLR: 0.000001\n",
      "Training Epoch: 25 [54400/72641]\tLoss: 0.5832\tLR: 0.000001\n",
      "Training Epoch: 25 [54720/72641]\tLoss: 0.6630\tLR: 0.000001\n",
      "Training Epoch: 25 [55040/72641]\tLoss: 0.6393\tLR: 0.000001\n",
      "Training Epoch: 25 [55360/72641]\tLoss: 0.6273\tLR: 0.000001\n",
      "Training Epoch: 25 [55680/72641]\tLoss: 0.6001\tLR: 0.000001\n",
      "Training Epoch: 25 [56000/72641]\tLoss: 0.5765\tLR: 0.000001\n",
      "Training Epoch: 25 [56320/72641]\tLoss: 0.6277\tLR: 0.000001\n",
      "Training Epoch: 25 [56640/72641]\tLoss: 0.6742\tLR: 0.000001\n",
      "Training Epoch: 25 [56960/72641]\tLoss: 0.6096\tLR: 0.000001\n",
      "Training Epoch: 25 [57280/72641]\tLoss: 0.6219\tLR: 0.000001\n",
      "Training Epoch: 25 [57600/72641]\tLoss: 0.6444\tLR: 0.000001\n",
      "Training Epoch: 25 [57920/72641]\tLoss: 0.6157\tLR: 0.000001\n",
      "Training Epoch: 25 [58240/72641]\tLoss: 0.5812\tLR: 0.000001\n",
      "Training Epoch: 25 [58560/72641]\tLoss: 0.6325\tLR: 0.000001\n",
      "Training Epoch: 25 [58880/72641]\tLoss: 0.5820\tLR: 0.000001\n",
      "Training Epoch: 25 [59200/72641]\tLoss: 0.6485\tLR: 0.000001\n",
      "Training Epoch: 25 [59520/72641]\tLoss: 0.6301\tLR: 0.000001\n",
      "Training Epoch: 25 [59840/72641]\tLoss: 0.6220\tLR: 0.000001\n",
      "Training Epoch: 25 [60160/72641]\tLoss: 0.6093\tLR: 0.000001\n",
      "Training Epoch: 25 [60480/72641]\tLoss: 0.6750\tLR: 0.000001\n",
      "Training Epoch: 25 [60800/72641]\tLoss: 0.5838\tLR: 0.000001\n",
      "Training Epoch: 25 [61120/72641]\tLoss: 0.6028\tLR: 0.000001\n",
      "Training Epoch: 25 [61440/72641]\tLoss: 0.5766\tLR: 0.000001\n",
      "Training Epoch: 25 [61760/72641]\tLoss: 0.6332\tLR: 0.000001\n",
      "Training Epoch: 25 [62080/72641]\tLoss: 0.6692\tLR: 0.000001\n",
      "Training Epoch: 25 [62400/72641]\tLoss: 0.6545\tLR: 0.000001\n",
      "Training Epoch: 25 [62720/72641]\tLoss: 0.5978\tLR: 0.000001\n",
      "Training Epoch: 25 [63040/72641]\tLoss: 0.6053\tLR: 0.000001\n",
      "Training Epoch: 25 [63360/72641]\tLoss: 0.6813\tLR: 0.000001\n",
      "Training Epoch: 25 [63680/72641]\tLoss: 0.6251\tLR: 0.000001\n",
      "Training Epoch: 25 [64000/72641]\tLoss: 0.6115\tLR: 0.000001\n",
      "Training Epoch: 25 [64320/72641]\tLoss: 0.6309\tLR: 0.000001\n",
      "Training Epoch: 25 [64640/72641]\tLoss: 0.6210\tLR: 0.000001\n",
      "Training Epoch: 25 [64960/72641]\tLoss: 0.6352\tLR: 0.000001\n",
      "Training Epoch: 25 [65280/72641]\tLoss: 0.6534\tLR: 0.000001\n",
      "Training Epoch: 25 [65600/72641]\tLoss: 0.6194\tLR: 0.000001\n",
      "Training Epoch: 25 [65920/72641]\tLoss: 0.7013\tLR: 0.000001\n",
      "Training Epoch: 25 [66240/72641]\tLoss: 0.6135\tLR: 0.000001\n",
      "Training Epoch: 25 [66560/72641]\tLoss: 0.6515\tLR: 0.000001\n",
      "Training Epoch: 25 [66880/72641]\tLoss: 0.6446\tLR: 0.000001\n",
      "Training Epoch: 25 [67200/72641]\tLoss: 0.6364\tLR: 0.000001\n",
      "Training Epoch: 25 [67520/72641]\tLoss: 0.6437\tLR: 0.000001\n",
      "Training Epoch: 25 [67840/72641]\tLoss: 0.6515\tLR: 0.000001\n",
      "Training Epoch: 25 [68160/72641]\tLoss: 0.6500\tLR: 0.000001\n",
      "Training Epoch: 25 [68480/72641]\tLoss: 0.6230\tLR: 0.000001\n",
      "Training Epoch: 25 [68800/72641]\tLoss: 0.6880\tLR: 0.000001\n",
      "Training Epoch: 25 [69120/72641]\tLoss: 0.6426\tLR: 0.000001\n",
      "Training Epoch: 25 [69440/72641]\tLoss: 0.6156\tLR: 0.000001\n",
      "Training Epoch: 25 [69760/72641]\tLoss: 0.6263\tLR: 0.000001\n",
      "Training Epoch: 25 [70080/72641]\tLoss: 0.6439\tLR: 0.000001\n",
      "Training Epoch: 25 [70400/72641]\tLoss: 0.6434\tLR: 0.000001\n",
      "Training Epoch: 25 [70720/72641]\tLoss: 0.6295\tLR: 0.000001\n",
      "Training Epoch: 25 [71040/72641]\tLoss: 0.6433\tLR: 0.000001\n",
      "Training Epoch: 25 [71360/72641]\tLoss: 0.6374\tLR: 0.000001\n",
      "Training Epoch: 25 [71680/72641]\tLoss: 0.6265\tLR: 0.000001\n",
      "Training Epoch: 25 [72000/72641]\tLoss: 0.6032\tLR: 0.000001\n",
      "Training Epoch: 25 [72320/72641]\tLoss: 0.5909\tLR: 0.000001\n",
      "Training Epoch: 25 [72640/72641]\tLoss: 0.6035\tLR: 0.000001\n",
      "Val Result: Acc: 0.1438, C_ACC: 0.7067, DOA: 89.0472, ACC_k: 0.0986\n",
      "ext:0.0, cls:0.568616, coar:0.0, fine:0.0,\n",
      "Training Epoch: 26 [320/72641]\tLoss: 0.6489\tLR: 0.000000\n",
      "Training Epoch: 26 [640/72641]\tLoss: 0.6141\tLR: 0.000000\n",
      "Training Epoch: 26 [960/72641]\tLoss: 0.6069\tLR: 0.000000\n",
      "Training Epoch: 26 [1280/72641]\tLoss: 0.6061\tLR: 0.000000\n",
      "Training Epoch: 26 [1600/72641]\tLoss: 0.6784\tLR: 0.000000\n",
      "Training Epoch: 26 [1920/72641]\tLoss: 0.5783\tLR: 0.000000\n",
      "Training Epoch: 26 [2240/72641]\tLoss: 0.6030\tLR: 0.000000\n",
      "Training Epoch: 26 [2560/72641]\tLoss: 0.6206\tLR: 0.000000\n",
      "Training Epoch: 26 [2880/72641]\tLoss: 0.6168\tLR: 0.000000\n",
      "Training Epoch: 26 [3200/72641]\tLoss: 0.6646\tLR: 0.000000\n",
      "Training Epoch: 26 [3520/72641]\tLoss: 0.6560\tLR: 0.000000\n",
      "Training Epoch: 26 [3840/72641]\tLoss: 0.6369\tLR: 0.000000\n",
      "Training Epoch: 26 [4160/72641]\tLoss: 0.6455\tLR: 0.000000\n",
      "Training Epoch: 26 [4480/72641]\tLoss: 0.5874\tLR: 0.000000\n",
      "Training Epoch: 26 [4800/72641]\tLoss: 0.6199\tLR: 0.000000\n",
      "Training Epoch: 26 [5120/72641]\tLoss: 0.6019\tLR: 0.000000\n",
      "Training Epoch: 26 [5440/72641]\tLoss: 0.6254\tLR: 0.000000\n",
      "Training Epoch: 26 [5760/72641]\tLoss: 0.6286\tLR: 0.000000\n",
      "Training Epoch: 26 [6080/72641]\tLoss: 0.6300\tLR: 0.000000\n",
      "Training Epoch: 26 [6400/72641]\tLoss: 0.5615\tLR: 0.000000\n",
      "Training Epoch: 26 [6720/72641]\tLoss: 0.6513\tLR: 0.000000\n",
      "Training Epoch: 26 [7040/72641]\tLoss: 0.6191\tLR: 0.000000\n",
      "Training Epoch: 26 [7360/72641]\tLoss: 0.6691\tLR: 0.000000\n",
      "Training Epoch: 26 [7680/72641]\tLoss: 0.6199\tLR: 0.000000\n",
      "Training Epoch: 26 [8000/72641]\tLoss: 0.6162\tLR: 0.000000\n",
      "Training Epoch: 26 [8320/72641]\tLoss: 0.6417\tLR: 0.000000\n",
      "Training Epoch: 26 [8640/72641]\tLoss: 0.6014\tLR: 0.000000\n",
      "Training Epoch: 26 [8960/72641]\tLoss: 0.5800\tLR: 0.000000\n",
      "Training Epoch: 26 [9280/72641]\tLoss: 0.5905\tLR: 0.000000\n",
      "Training Epoch: 26 [9600/72641]\tLoss: 0.6489\tLR: 0.000000\n",
      "Training Epoch: 26 [9920/72641]\tLoss: 0.6112\tLR: 0.000000\n",
      "Training Epoch: 26 [10240/72641]\tLoss: 0.6184\tLR: 0.000000\n",
      "Training Epoch: 26 [10560/72641]\tLoss: 0.6041\tLR: 0.000000\n",
      "Training Epoch: 26 [10880/72641]\tLoss: 0.6637\tLR: 0.000000\n",
      "Training Epoch: 26 [11200/72641]\tLoss: 0.6167\tLR: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [11520/72641]\tLoss: 0.6694\tLR: 0.000000\n",
      "Training Epoch: 26 [11840/72641]\tLoss: 0.6286\tLR: 0.000000\n",
      "Training Epoch: 26 [12160/72641]\tLoss: 0.6291\tLR: 0.000000\n",
      "Training Epoch: 26 [12480/72641]\tLoss: 0.6174\tLR: 0.000000\n",
      "Training Epoch: 26 [12800/72641]\tLoss: 0.6494\tLR: 0.000000\n",
      "Training Epoch: 26 [13120/72641]\tLoss: 0.6030\tLR: 0.000000\n",
      "Training Epoch: 26 [13440/72641]\tLoss: 0.6297\tLR: 0.000000\n",
      "Training Epoch: 26 [13760/72641]\tLoss: 0.6669\tLR: 0.000000\n",
      "Training Epoch: 26 [14080/72641]\tLoss: 0.6325\tLR: 0.000000\n",
      "Training Epoch: 26 [14400/72641]\tLoss: 0.6332\tLR: 0.000000\n",
      "Training Epoch: 26 [14720/72641]\tLoss: 0.6091\tLR: 0.000000\n",
      "Training Epoch: 26 [15040/72641]\tLoss: 0.5910\tLR: 0.000000\n",
      "Training Epoch: 26 [15360/72641]\tLoss: 0.6577\tLR: 0.000000\n",
      "Training Epoch: 26 [15680/72641]\tLoss: 0.5953\tLR: 0.000000\n",
      "Training Epoch: 26 [16000/72641]\tLoss: 0.6430\tLR: 0.000000\n",
      "Training Epoch: 26 [16320/72641]\tLoss: 0.6255\tLR: 0.000000\n",
      "Training Epoch: 26 [16640/72641]\tLoss: 0.6705\tLR: 0.000000\n",
      "Training Epoch: 26 [16960/72641]\tLoss: 0.6398\tLR: 0.000000\n",
      "Training Epoch: 26 [17280/72641]\tLoss: 0.6360\tLR: 0.000000\n",
      "Training Epoch: 26 [17600/72641]\tLoss: 0.6071\tLR: 0.000000\n",
      "Training Epoch: 26 [17920/72641]\tLoss: 0.6214\tLR: 0.000000\n",
      "Training Epoch: 26 [18240/72641]\tLoss: 0.6586\tLR: 0.000000\n",
      "Training Epoch: 26 [18560/72641]\tLoss: 0.6341\tLR: 0.000000\n",
      "Training Epoch: 26 [18880/72641]\tLoss: 0.6482\tLR: 0.000000\n",
      "Training Epoch: 26 [19200/72641]\tLoss: 0.6450\tLR: 0.000000\n",
      "Training Epoch: 26 [19520/72641]\tLoss: 0.6538\tLR: 0.000000\n",
      "Training Epoch: 26 [19840/72641]\tLoss: 0.6532\tLR: 0.000000\n",
      "Training Epoch: 26 [20160/72641]\tLoss: 0.6095\tLR: 0.000000\n",
      "Training Epoch: 26 [20480/72641]\tLoss: 0.6230\tLR: 0.000000\n",
      "Training Epoch: 26 [20800/72641]\tLoss: 0.6172\tLR: 0.000000\n",
      "Training Epoch: 26 [21120/72641]\tLoss: 0.6356\tLR: 0.000000\n",
      "Training Epoch: 26 [21440/72641]\tLoss: 0.6348\tLR: 0.000000\n",
      "Training Epoch: 26 [21760/72641]\tLoss: 0.6583\tLR: 0.000000\n",
      "Training Epoch: 26 [22080/72641]\tLoss: 0.6199\tLR: 0.000000\n",
      "Training Epoch: 26 [22400/72641]\tLoss: 0.5902\tLR: 0.000000\n",
      "Training Epoch: 26 [22720/72641]\tLoss: 0.6175\tLR: 0.000000\n",
      "Training Epoch: 26 [23040/72641]\tLoss: 0.6206\tLR: 0.000000\n",
      "Training Epoch: 26 [23360/72641]\tLoss: 0.6275\tLR: 0.000000\n",
      "Training Epoch: 26 [23680/72641]\tLoss: 0.6058\tLR: 0.000000\n",
      "Training Epoch: 26 [24000/72641]\tLoss: 0.6289\tLR: 0.000000\n",
      "Training Epoch: 26 [24320/72641]\tLoss: 0.6109\tLR: 0.000000\n",
      "Training Epoch: 26 [24640/72641]\tLoss: 0.6746\tLR: 0.000000\n",
      "Training Epoch: 26 [24960/72641]\tLoss: 0.6551\tLR: 0.000000\n",
      "Training Epoch: 26 [25280/72641]\tLoss: 0.6555\tLR: 0.000000\n",
      "Training Epoch: 26 [25600/72641]\tLoss: 0.5908\tLR: 0.000000\n",
      "Training Epoch: 26 [25920/72641]\tLoss: 0.6349\tLR: 0.000000\n",
      "Training Epoch: 26 [26240/72641]\tLoss: 0.6488\tLR: 0.000000\n",
      "Training Epoch: 26 [26560/72641]\tLoss: 0.5939\tLR: 0.000000\n",
      "Training Epoch: 26 [26880/72641]\tLoss: 0.6372\tLR: 0.000000\n",
      "Training Epoch: 26 [27200/72641]\tLoss: 0.6079\tLR: 0.000000\n",
      "Training Epoch: 26 [27520/72641]\tLoss: 0.6296\tLR: 0.000000\n",
      "Training Epoch: 26 [27840/72641]\tLoss: 0.6316\tLR: 0.000000\n",
      "Training Epoch: 26 [28160/72641]\tLoss: 0.6485\tLR: 0.000000\n",
      "Training Epoch: 26 [28480/72641]\tLoss: 0.5788\tLR: 0.000000\n",
      "Training Epoch: 26 [28800/72641]\tLoss: 0.5722\tLR: 0.000000\n",
      "Training Epoch: 26 [29120/72641]\tLoss: 0.7144\tLR: 0.000000\n",
      "Training Epoch: 26 [29440/72641]\tLoss: 0.6300\tLR: 0.000000\n",
      "Training Epoch: 26 [29760/72641]\tLoss: 0.5948\tLR: 0.000000\n",
      "Training Epoch: 26 [30080/72641]\tLoss: 0.6891\tLR: 0.000000\n",
      "Training Epoch: 26 [30400/72641]\tLoss: 0.6268\tLR: 0.000000\n",
      "Training Epoch: 26 [30720/72641]\tLoss: 0.6704\tLR: 0.000000\n",
      "Training Epoch: 26 [31040/72641]\tLoss: 0.5894\tLR: 0.000000\n",
      "Training Epoch: 26 [31360/72641]\tLoss: 0.6321\tLR: 0.000000\n",
      "Training Epoch: 26 [31680/72641]\tLoss: 0.6067\tLR: 0.000000\n",
      "Training Epoch: 26 [32000/72641]\tLoss: 0.6143\tLR: 0.000000\n",
      "Training Epoch: 26 [32320/72641]\tLoss: 0.6319\tLR: 0.000000\n",
      "Training Epoch: 26 [32640/72641]\tLoss: 0.5994\tLR: 0.000000\n",
      "Training Epoch: 26 [32960/72641]\tLoss: 0.6308\tLR: 0.000000\n",
      "Training Epoch: 26 [33280/72641]\tLoss: 0.6283\tLR: 0.000000\n",
      "Training Epoch: 26 [33600/72641]\tLoss: 0.5902\tLR: 0.000000\n",
      "Training Epoch: 26 [33920/72641]\tLoss: 0.6119\tLR: 0.000000\n",
      "Training Epoch: 26 [34240/72641]\tLoss: 0.5847\tLR: 0.000000\n",
      "Training Epoch: 26 [34560/72641]\tLoss: 0.6462\tLR: 0.000000\n",
      "Training Epoch: 26 [34880/72641]\tLoss: 0.5834\tLR: 0.000000\n",
      "Training Epoch: 26 [35200/72641]\tLoss: 0.5936\tLR: 0.000000\n",
      "Training Epoch: 26 [35520/72641]\tLoss: 0.6305\tLR: 0.000000\n",
      "Training Epoch: 26 [35840/72641]\tLoss: 0.6301\tLR: 0.000000\n",
      "Training Epoch: 26 [36160/72641]\tLoss: 0.6625\tLR: 0.000000\n",
      "Training Epoch: 26 [36480/72641]\tLoss: 0.6257\tLR: 0.000000\n",
      "Training Epoch: 26 [36800/72641]\tLoss: 0.6142\tLR: 0.000000\n",
      "Training Epoch: 26 [37120/72641]\tLoss: 0.6438\tLR: 0.000000\n",
      "Training Epoch: 26 [37440/72641]\tLoss: 0.6062\tLR: 0.000000\n",
      "Training Epoch: 26 [37760/72641]\tLoss: 0.6602\tLR: 0.000000\n",
      "Training Epoch: 26 [38080/72641]\tLoss: 0.6100\tLR: 0.000000\n",
      "Training Epoch: 26 [38400/72641]\tLoss: 0.6479\tLR: 0.000000\n",
      "Training Epoch: 26 [38720/72641]\tLoss: 0.6153\tLR: 0.000000\n",
      "Training Epoch: 26 [39040/72641]\tLoss: 0.6638\tLR: 0.000000\n",
      "Training Epoch: 26 [39360/72641]\tLoss: 0.6269\tLR: 0.000000\n",
      "Training Epoch: 26 [39680/72641]\tLoss: 0.6219\tLR: 0.000000\n",
      "Training Epoch: 26 [40000/72641]\tLoss: 0.6533\tLR: 0.000000\n",
      "Training Epoch: 26 [40320/72641]\tLoss: 0.6128\tLR: 0.000000\n",
      "Training Epoch: 26 [40640/72641]\tLoss: 0.6125\tLR: 0.000000\n",
      "Training Epoch: 26 [40960/72641]\tLoss: 0.6360\tLR: 0.000000\n",
      "Training Epoch: 26 [41280/72641]\tLoss: 0.6664\tLR: 0.000000\n",
      "Training Epoch: 26 [41600/72641]\tLoss: 0.6284\tLR: 0.000000\n",
      "Training Epoch: 26 [41920/72641]\tLoss: 0.6195\tLR: 0.000000\n",
      "Training Epoch: 26 [42240/72641]\tLoss: 0.6469\tLR: 0.000000\n",
      "Training Epoch: 26 [42560/72641]\tLoss: 0.5915\tLR: 0.000000\n",
      "Training Epoch: 26 [42880/72641]\tLoss: 0.6036\tLR: 0.000000\n",
      "Training Epoch: 26 [43200/72641]\tLoss: 0.6186\tLR: 0.000000\n",
      "Training Epoch: 26 [43520/72641]\tLoss: 0.6595\tLR: 0.000000\n",
      "Training Epoch: 26 [43840/72641]\tLoss: 0.6397\tLR: 0.000000\n",
      "Training Epoch: 26 [44160/72641]\tLoss: 0.6422\tLR: 0.000000\n",
      "Training Epoch: 26 [44480/72641]\tLoss: 0.6048\tLR: 0.000000\n",
      "Training Epoch: 26 [44800/72641]\tLoss: 0.6316\tLR: 0.000000\n",
      "Training Epoch: 26 [45120/72641]\tLoss: 0.6086\tLR: 0.000000\n",
      "Training Epoch: 26 [45440/72641]\tLoss: 0.5774\tLR: 0.000000\n",
      "Training Epoch: 26 [45760/72641]\tLoss: 0.6046\tLR: 0.000000\n",
      "Training Epoch: 26 [46080/72641]\tLoss: 0.6694\tLR: 0.000000\n",
      "Training Epoch: 26 [46400/72641]\tLoss: 0.6231\tLR: 0.000000\n",
      "Training Epoch: 26 [46720/72641]\tLoss: 0.6556\tLR: 0.000000\n",
      "Training Epoch: 26 [47040/72641]\tLoss: 0.6242\tLR: 0.000000\n",
      "Training Epoch: 26 [47360/72641]\tLoss: 0.6190\tLR: 0.000000\n",
      "Training Epoch: 26 [47680/72641]\tLoss: 0.6557\tLR: 0.000000\n",
      "Training Epoch: 26 [48000/72641]\tLoss: 0.6327\tLR: 0.000000\n",
      "Training Epoch: 26 [48320/72641]\tLoss: 0.6743\tLR: 0.000000\n",
      "Training Epoch: 26 [48640/72641]\tLoss: 0.6452\tLR: 0.000000\n",
      "Training Epoch: 26 [48960/72641]\tLoss: 0.6458\tLR: 0.000000\n",
      "Training Epoch: 26 [49280/72641]\tLoss: 0.6559\tLR: 0.000000\n",
      "Training Epoch: 26 [49600/72641]\tLoss: 0.6463\tLR: 0.000000\n",
      "Training Epoch: 26 [49920/72641]\tLoss: 0.5968\tLR: 0.000000\n",
      "Training Epoch: 26 [50240/72641]\tLoss: 0.6352\tLR: 0.000000\n",
      "Training Epoch: 26 [50560/72641]\tLoss: 0.5884\tLR: 0.000000\n",
      "Training Epoch: 26 [50880/72641]\tLoss: 0.6091\tLR: 0.000000\n",
      "Training Epoch: 26 [51200/72641]\tLoss: 0.6739\tLR: 0.000000\n",
      "Training Epoch: 26 [51520/72641]\tLoss: 0.6300\tLR: 0.000000\n",
      "Training Epoch: 26 [51840/72641]\tLoss: 0.6176\tLR: 0.000000\n",
      "Training Epoch: 26 [52160/72641]\tLoss: 0.6091\tLR: 0.000000\n",
      "Training Epoch: 26 [52480/72641]\tLoss: 0.6270\tLR: 0.000000\n",
      "Training Epoch: 26 [52800/72641]\tLoss: 0.5610\tLR: 0.000000\n",
      "Training Epoch: 26 [53120/72641]\tLoss: 0.5973\tLR: 0.000000\n",
      "Training Epoch: 26 [53440/72641]\tLoss: 0.6129\tLR: 0.000000\n",
      "Training Epoch: 26 [53760/72641]\tLoss: 0.6300\tLR: 0.000000\n",
      "Training Epoch: 26 [54080/72641]\tLoss: 0.6038\tLR: 0.000000\n",
      "Training Epoch: 26 [54400/72641]\tLoss: 0.5961\tLR: 0.000000\n",
      "Training Epoch: 26 [54720/72641]\tLoss: 0.6137\tLR: 0.000000\n",
      "Training Epoch: 26 [55040/72641]\tLoss: 0.6575\tLR: 0.000000\n",
      "Training Epoch: 26 [55360/72641]\tLoss: 0.6220\tLR: 0.000000\n",
      "Training Epoch: 26 [55680/72641]\tLoss: 0.5692\tLR: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [56000/72641]\tLoss: 0.5893\tLR: 0.000000\n",
      "Training Epoch: 26 [56320/72641]\tLoss: 0.6300\tLR: 0.000000\n",
      "Training Epoch: 26 [56640/72641]\tLoss: 0.6485\tLR: 0.000000\n",
      "Training Epoch: 26 [56960/72641]\tLoss: 0.6284\tLR: 0.000000\n",
      "Training Epoch: 26 [57280/72641]\tLoss: 0.5915\tLR: 0.000000\n",
      "Training Epoch: 26 [57600/72641]\tLoss: 0.6328\tLR: 0.000000\n",
      "Training Epoch: 26 [57920/72641]\tLoss: 0.6529\tLR: 0.000000\n",
      "Training Epoch: 26 [58240/72641]\tLoss: 0.6315\tLR: 0.000000\n",
      "Training Epoch: 26 [58560/72641]\tLoss: 0.6394\tLR: 0.000000\n",
      "Training Epoch: 26 [58880/72641]\tLoss: 0.6508\tLR: 0.000000\n",
      "Training Epoch: 26 [59200/72641]\tLoss: 0.6096\tLR: 0.000000\n",
      "Training Epoch: 26 [59520/72641]\tLoss: 0.6533\tLR: 0.000000\n",
      "Training Epoch: 26 [59840/72641]\tLoss: 0.6436\tLR: 0.000000\n",
      "Training Epoch: 26 [60160/72641]\tLoss: 0.6394\tLR: 0.000000\n",
      "Training Epoch: 26 [60480/72641]\tLoss: 0.6772\tLR: 0.000000\n",
      "Training Epoch: 26 [60800/72641]\tLoss: 0.6144\tLR: 0.000000\n",
      "Training Epoch: 26 [61120/72641]\tLoss: 0.5612\tLR: 0.000000\n",
      "Training Epoch: 26 [61440/72641]\tLoss: 0.5817\tLR: 0.000000\n",
      "Training Epoch: 26 [61760/72641]\tLoss: 0.6465\tLR: 0.000000\n",
      "Training Epoch: 26 [62080/72641]\tLoss: 0.6508\tLR: 0.000000\n",
      "Training Epoch: 26 [62400/72641]\tLoss: 0.6302\tLR: 0.000000\n",
      "Training Epoch: 26 [62720/72641]\tLoss: 0.6229\tLR: 0.000000\n",
      "Training Epoch: 26 [63040/72641]\tLoss: 0.6368\tLR: 0.000000\n",
      "Training Epoch: 26 [63360/72641]\tLoss: 0.6727\tLR: 0.000000\n",
      "Training Epoch: 26 [63680/72641]\tLoss: 0.6203\tLR: 0.000000\n",
      "Training Epoch: 26 [64000/72641]\tLoss: 0.6544\tLR: 0.000000\n",
      "Training Epoch: 26 [64320/72641]\tLoss: 0.6580\tLR: 0.000000\n",
      "Training Epoch: 26 [64640/72641]\tLoss: 0.6088\tLR: 0.000000\n",
      "Training Epoch: 26 [64960/72641]\tLoss: 0.6445\tLR: 0.000000\n",
      "Training Epoch: 26 [65280/72641]\tLoss: 0.6288\tLR: 0.000000\n",
      "Training Epoch: 26 [65600/72641]\tLoss: 0.6693\tLR: 0.000000\n",
      "Training Epoch: 26 [65920/72641]\tLoss: 0.6848\tLR: 0.000000\n",
      "Training Epoch: 26 [66240/72641]\tLoss: 0.6123\tLR: 0.000000\n",
      "Training Epoch: 26 [66560/72641]\tLoss: 0.5949\tLR: 0.000000\n",
      "Training Epoch: 26 [66880/72641]\tLoss: 0.6179\tLR: 0.000000\n",
      "Training Epoch: 26 [67200/72641]\tLoss: 0.5901\tLR: 0.000000\n",
      "Training Epoch: 26 [67520/72641]\tLoss: 0.6406\tLR: 0.000000\n",
      "Training Epoch: 26 [67840/72641]\tLoss: 0.6449\tLR: 0.000000\n",
      "Training Epoch: 26 [68160/72641]\tLoss: 0.6030\tLR: 0.000000\n",
      "Training Epoch: 26 [68480/72641]\tLoss: 0.6203\tLR: 0.000000\n",
      "Training Epoch: 26 [68800/72641]\tLoss: 0.6500\tLR: 0.000000\n",
      "Training Epoch: 26 [69120/72641]\tLoss: 0.6650\tLR: 0.000000\n",
      "Training Epoch: 26 [69440/72641]\tLoss: 0.5801\tLR: 0.000000\n",
      "Training Epoch: 26 [69760/72641]\tLoss: 0.6457\tLR: 0.000000\n",
      "Training Epoch: 26 [70080/72641]\tLoss: 0.6171\tLR: 0.000000\n",
      "Training Epoch: 26 [70400/72641]\tLoss: 0.6676\tLR: 0.000000\n",
      "Training Epoch: 26 [70720/72641]\tLoss: 0.6536\tLR: 0.000000\n",
      "Training Epoch: 26 [71040/72641]\tLoss: 0.6321\tLR: 0.000000\n",
      "Training Epoch: 26 [71360/72641]\tLoss: 0.6507\tLR: 0.000000\n",
      "Training Epoch: 26 [71680/72641]\tLoss: 0.6683\tLR: 0.000000\n",
      "Training Epoch: 26 [72000/72641]\tLoss: 0.6372\tLR: 0.000000\n",
      "Training Epoch: 26 [72320/72641]\tLoss: 0.6398\tLR: 0.000000\n",
      "Training Epoch: 26 [72640/72641]\tLoss: 0.6284\tLR: 0.000000\n",
      "Val Result: Acc: 0.1474, C_ACC: 0.7083, DOA: 88.4189, ACC_k: 0.0996\n",
      "ext:0.0, cls:0.561235, coar:0.0, fine:0.0,\n",
      "Training Epoch: 27 [320/72641]\tLoss: 0.6483\tLR: 0.000000\n",
      "Training Epoch: 27 [640/72641]\tLoss: 0.6193\tLR: 0.000000\n",
      "Training Epoch: 27 [960/72641]\tLoss: 0.6417\tLR: 0.000000\n",
      "Training Epoch: 27 [1280/72641]\tLoss: 0.6778\tLR: 0.000000\n",
      "Training Epoch: 27 [1600/72641]\tLoss: 0.6170\tLR: 0.000000\n",
      "Training Epoch: 27 [1920/72641]\tLoss: 0.6105\tLR: 0.000000\n",
      "Training Epoch: 27 [2240/72641]\tLoss: 0.6181\tLR: 0.000000\n",
      "Training Epoch: 27 [2560/72641]\tLoss: 0.6626\tLR: 0.000000\n",
      "Training Epoch: 27 [2880/72641]\tLoss: 0.6252\tLR: 0.000000\n",
      "Training Epoch: 27 [3200/72641]\tLoss: 0.6578\tLR: 0.000000\n",
      "Training Epoch: 27 [3520/72641]\tLoss: 0.6137\tLR: 0.000000\n",
      "Training Epoch: 27 [3840/72641]\tLoss: 0.5991\tLR: 0.000000\n",
      "Training Epoch: 27 [4160/72641]\tLoss: 0.6441\tLR: 0.000000\n",
      "Training Epoch: 27 [4480/72641]\tLoss: 0.6173\tLR: 0.000000\n",
      "Training Epoch: 27 [4800/72641]\tLoss: 0.5766\tLR: 0.000000\n",
      "Training Epoch: 27 [5120/72641]\tLoss: 0.6195\tLR: 0.000000\n",
      "Training Epoch: 27 [5440/72641]\tLoss: 0.6793\tLR: 0.000000\n",
      "Training Epoch: 27 [5760/72641]\tLoss: 0.6324\tLR: 0.000000\n",
      "Training Epoch: 27 [6080/72641]\tLoss: 0.6223\tLR: 0.000000\n",
      "Training Epoch: 27 [6400/72641]\tLoss: 0.5877\tLR: 0.000000\n",
      "Training Epoch: 27 [6720/72641]\tLoss: 0.6272\tLR: 0.000000\n",
      "Training Epoch: 27 [7040/72641]\tLoss: 0.6210\tLR: 0.000000\n",
      "Training Epoch: 27 [7360/72641]\tLoss: 0.6339\tLR: 0.000000\n",
      "Training Epoch: 27 [7680/72641]\tLoss: 0.6735\tLR: 0.000000\n",
      "Training Epoch: 27 [8000/72641]\tLoss: 0.6183\tLR: 0.000000\n",
      "Training Epoch: 27 [8320/72641]\tLoss: 0.5999\tLR: 0.000000\n",
      "Training Epoch: 27 [8640/72641]\tLoss: 0.6223\tLR: 0.000000\n",
      "Training Epoch: 27 [8960/72641]\tLoss: 0.5694\tLR: 0.000000\n",
      "Training Epoch: 27 [9280/72641]\tLoss: 0.6019\tLR: 0.000000\n",
      "Training Epoch: 27 [9600/72641]\tLoss: 0.6734\tLR: 0.000000\n",
      "Training Epoch: 27 [9920/72641]\tLoss: 0.6519\tLR: 0.000000\n",
      "Training Epoch: 27 [10240/72641]\tLoss: 0.6384\tLR: 0.000000\n",
      "Training Epoch: 27 [10560/72641]\tLoss: 0.5977\tLR: 0.000000\n",
      "Training Epoch: 27 [10880/72641]\tLoss: 0.5944\tLR: 0.000000\n",
      "Training Epoch: 27 [11200/72641]\tLoss: 0.6502\tLR: 0.000000\n",
      "Training Epoch: 27 [11520/72641]\tLoss: 0.6309\tLR: 0.000000\n",
      "Training Epoch: 27 [11840/72641]\tLoss: 0.6120\tLR: 0.000000\n",
      "Training Epoch: 27 [12160/72641]\tLoss: 0.6079\tLR: 0.000000\n",
      "Training Epoch: 27 [12480/72641]\tLoss: 0.6222\tLR: 0.000000\n",
      "Training Epoch: 27 [12800/72641]\tLoss: 0.6238\tLR: 0.000000\n",
      "Training Epoch: 27 [13120/72641]\tLoss: 0.6116\tLR: 0.000000\n",
      "Training Epoch: 27 [13440/72641]\tLoss: 0.6166\tLR: 0.000000\n",
      "Training Epoch: 27 [13760/72641]\tLoss: 0.6427\tLR: 0.000000\n",
      "Training Epoch: 27 [14080/72641]\tLoss: 0.6611\tLR: 0.000000\n",
      "Training Epoch: 27 [14400/72641]\tLoss: 0.6215\tLR: 0.000000\n",
      "Training Epoch: 27 [14720/72641]\tLoss: 0.6040\tLR: 0.000000\n",
      "Training Epoch: 27 [15040/72641]\tLoss: 0.6231\tLR: 0.000000\n",
      "Training Epoch: 27 [15360/72641]\tLoss: 0.6205\tLR: 0.000000\n",
      "Training Epoch: 27 [15680/72641]\tLoss: 0.5929\tLR: 0.000000\n",
      "Training Epoch: 27 [16000/72641]\tLoss: 0.6357\tLR: 0.000000\n",
      "Training Epoch: 27 [16320/72641]\tLoss: 0.6421\tLR: 0.000000\n",
      "Training Epoch: 27 [16640/72641]\tLoss: 0.6641\tLR: 0.000000\n",
      "Training Epoch: 27 [16960/72641]\tLoss: 0.5828\tLR: 0.000000\n",
      "Training Epoch: 27 [17280/72641]\tLoss: 0.6169\tLR: 0.000000\n",
      "Training Epoch: 27 [17600/72641]\tLoss: 0.5611\tLR: 0.000000\n",
      "Training Epoch: 27 [17920/72641]\tLoss: 0.6593\tLR: 0.000000\n",
      "Training Epoch: 27 [18240/72641]\tLoss: 0.5958\tLR: 0.000000\n",
      "Training Epoch: 27 [18560/72641]\tLoss: 0.6135\tLR: 0.000000\n",
      "Training Epoch: 27 [18880/72641]\tLoss: 0.6239\tLR: 0.000000\n",
      "Training Epoch: 27 [19200/72641]\tLoss: 0.7066\tLR: 0.000000\n",
      "Training Epoch: 27 [19520/72641]\tLoss: 0.6271\tLR: 0.000000\n",
      "Training Epoch: 27 [19840/72641]\tLoss: 0.5983\tLR: 0.000000\n",
      "Training Epoch: 27 [20160/72641]\tLoss: 0.6103\tLR: 0.000000\n",
      "Training Epoch: 27 [20480/72641]\tLoss: 0.6229\tLR: 0.000000\n",
      "Training Epoch: 27 [20800/72641]\tLoss: 0.6469\tLR: 0.000000\n",
      "Training Epoch: 27 [21120/72641]\tLoss: 0.6577\tLR: 0.000000\n",
      "Training Epoch: 27 [21440/72641]\tLoss: 0.6506\tLR: 0.000000\n",
      "Training Epoch: 27 [21760/72641]\tLoss: 0.6372\tLR: 0.000000\n",
      "Training Epoch: 27 [22080/72641]\tLoss: 0.6358\tLR: 0.000000\n",
      "Training Epoch: 27 [22400/72641]\tLoss: 0.5698\tLR: 0.000000\n",
      "Training Epoch: 27 [22720/72641]\tLoss: 0.6256\tLR: 0.000000\n",
      "Training Epoch: 27 [23040/72641]\tLoss: 0.6364\tLR: 0.000000\n",
      "Training Epoch: 27 [23360/72641]\tLoss: 0.6088\tLR: 0.000000\n",
      "Training Epoch: 27 [23680/72641]\tLoss: 0.6800\tLR: 0.000000\n",
      "Training Epoch: 27 [24000/72641]\tLoss: 0.6423\tLR: 0.000000\n",
      "Training Epoch: 27 [24320/72641]\tLoss: 0.6546\tLR: 0.000000\n",
      "Training Epoch: 27 [24640/72641]\tLoss: 0.6104\tLR: 0.000000\n",
      "Training Epoch: 27 [24960/72641]\tLoss: 0.6530\tLR: 0.000000\n",
      "Training Epoch: 27 [25280/72641]\tLoss: 0.6089\tLR: 0.000000\n",
      "Training Epoch: 27 [25600/72641]\tLoss: 0.6128\tLR: 0.000000\n",
      "Training Epoch: 27 [25920/72641]\tLoss: 0.6301\tLR: 0.000000\n",
      "Training Epoch: 27 [26240/72641]\tLoss: 0.6342\tLR: 0.000000\n",
      "Training Epoch: 27 [26560/72641]\tLoss: 0.6353\tLR: 0.000000\n",
      "Training Epoch: 27 [26880/72641]\tLoss: 0.6175\tLR: 0.000000\n",
      "Training Epoch: 27 [27200/72641]\tLoss: 0.6217\tLR: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [27520/72641]\tLoss: 0.6551\tLR: 0.000000\n",
      "Training Epoch: 27 [27840/72641]\tLoss: 0.6652\tLR: 0.000000\n",
      "Training Epoch: 27 [28160/72641]\tLoss: 0.6118\tLR: 0.000000\n",
      "Training Epoch: 27 [28480/72641]\tLoss: 0.6179\tLR: 0.000000\n",
      "Training Epoch: 27 [28800/72641]\tLoss: 0.6076\tLR: 0.000000\n",
      "Training Epoch: 27 [29120/72641]\tLoss: 0.6195\tLR: 0.000000\n",
      "Training Epoch: 27 [29440/72641]\tLoss: 0.6098\tLR: 0.000000\n",
      "Training Epoch: 27 [29760/72641]\tLoss: 0.6065\tLR: 0.000000\n",
      "Training Epoch: 27 [30080/72641]\tLoss: 0.5964\tLR: 0.000000\n",
      "Training Epoch: 27 [30400/72641]\tLoss: 0.6364\tLR: 0.000000\n",
      "Training Epoch: 27 [30720/72641]\tLoss: 0.6634\tLR: 0.000000\n",
      "Training Epoch: 27 [31040/72641]\tLoss: 0.5893\tLR: 0.000000\n",
      "Training Epoch: 27 [31360/72641]\tLoss: 0.6171\tLR: 0.000000\n",
      "Training Epoch: 27 [31680/72641]\tLoss: 0.6527\tLR: 0.000000\n",
      "Training Epoch: 27 [32000/72641]\tLoss: 0.6396\tLR: 0.000000\n",
      "Training Epoch: 27 [32320/72641]\tLoss: 0.6339\tLR: 0.000000\n",
      "Training Epoch: 27 [32640/72641]\tLoss: 0.6699\tLR: 0.000000\n",
      "Training Epoch: 27 [32960/72641]\tLoss: 0.6719\tLR: 0.000000\n",
      "Training Epoch: 27 [33280/72641]\tLoss: 0.6382\tLR: 0.000000\n",
      "Training Epoch: 27 [33600/72641]\tLoss: 0.6214\tLR: 0.000000\n",
      "Training Epoch: 27 [33920/72641]\tLoss: 0.5914\tLR: 0.000000\n",
      "Training Epoch: 27 [34240/72641]\tLoss: 0.6337\tLR: 0.000000\n",
      "Training Epoch: 27 [34560/72641]\tLoss: 0.6545\tLR: 0.000000\n",
      "Training Epoch: 27 [34880/72641]\tLoss: 0.6723\tLR: 0.000000\n",
      "Training Epoch: 27 [35200/72641]\tLoss: 0.6056\tLR: 0.000000\n",
      "Training Epoch: 27 [35520/72641]\tLoss: 0.6241\tLR: 0.000000\n",
      "Training Epoch: 27 [35840/72641]\tLoss: 0.6337\tLR: 0.000000\n",
      "Training Epoch: 27 [36160/72641]\tLoss: 0.6283\tLR: 0.000000\n",
      "Training Epoch: 27 [36480/72641]\tLoss: 0.5996\tLR: 0.000000\n",
      "Training Epoch: 27 [36800/72641]\tLoss: 0.6244\tLR: 0.000000\n",
      "Training Epoch: 27 [37120/72641]\tLoss: 0.5692\tLR: 0.000000\n",
      "Training Epoch: 27 [37440/72641]\tLoss: 0.6368\tLR: 0.000000\n",
      "Training Epoch: 27 [37760/72641]\tLoss: 0.6158\tLR: 0.000000\n",
      "Training Epoch: 27 [38080/72641]\tLoss: 0.6126\tLR: 0.000000\n",
      "Training Epoch: 27 [38400/72641]\tLoss: 0.6425\tLR: 0.000000\n",
      "Training Epoch: 27 [38720/72641]\tLoss: 0.6484\tLR: 0.000000\n",
      "Training Epoch: 27 [39040/72641]\tLoss: 0.6495\tLR: 0.000000\n",
      "Training Epoch: 27 [39360/72641]\tLoss: 0.6043\tLR: 0.000000\n",
      "Training Epoch: 27 [39680/72641]\tLoss: 0.6105\tLR: 0.000000\n",
      "Training Epoch: 27 [40000/72641]\tLoss: 0.6263\tLR: 0.000000\n",
      "Training Epoch: 27 [40320/72641]\tLoss: 0.6399\tLR: 0.000000\n",
      "Training Epoch: 27 [40640/72641]\tLoss: 0.6426\tLR: 0.000000\n",
      "Training Epoch: 27 [40960/72641]\tLoss: 0.6024\tLR: 0.000000\n",
      "Training Epoch: 27 [41280/72641]\tLoss: 0.6867\tLR: 0.000000\n",
      "Training Epoch: 27 [41600/72641]\tLoss: 0.6524\tLR: 0.000000\n",
      "Training Epoch: 27 [41920/72641]\tLoss: 0.6222\tLR: 0.000000\n",
      "Training Epoch: 27 [42240/72641]\tLoss: 0.5860\tLR: 0.000000\n",
      "Training Epoch: 27 [42560/72641]\tLoss: 0.6032\tLR: 0.000000\n",
      "Training Epoch: 27 [42880/72641]\tLoss: 0.6082\tLR: 0.000000\n",
      "Training Epoch: 27 [43200/72641]\tLoss: 0.6109\tLR: 0.000000\n",
      "Training Epoch: 27 [43520/72641]\tLoss: 0.6155\tLR: 0.000000\n",
      "Training Epoch: 27 [43840/72641]\tLoss: 0.5950\tLR: 0.000000\n",
      "Training Epoch: 27 [44160/72641]\tLoss: 0.7082\tLR: 0.000000\n",
      "Training Epoch: 27 [44480/72641]\tLoss: 0.5888\tLR: 0.000000\n",
      "Training Epoch: 27 [44800/72641]\tLoss: 0.5872\tLR: 0.000000\n",
      "Training Epoch: 27 [45120/72641]\tLoss: 0.6229\tLR: 0.000000\n",
      "Training Epoch: 27 [45440/72641]\tLoss: 0.6003\tLR: 0.000000\n",
      "Training Epoch: 27 [45760/72641]\tLoss: 0.6520\tLR: 0.000000\n",
      "Training Epoch: 27 [46080/72641]\tLoss: 0.6248\tLR: 0.000000\n",
      "Training Epoch: 27 [46400/72641]\tLoss: 0.5971\tLR: 0.000000\n",
      "Training Epoch: 27 [46720/72641]\tLoss: 0.6388\tLR: 0.000000\n",
      "Training Epoch: 27 [47040/72641]\tLoss: 0.6178\tLR: 0.000000\n",
      "Training Epoch: 27 [47360/72641]\tLoss: 0.6238\tLR: 0.000000\n",
      "Training Epoch: 27 [47680/72641]\tLoss: 0.6209\tLR: 0.000000\n",
      "Training Epoch: 27 [48000/72641]\tLoss: 0.6204\tLR: 0.000000\n",
      "Training Epoch: 27 [48320/72641]\tLoss: 0.6013\tLR: 0.000000\n",
      "Training Epoch: 27 [48640/72641]\tLoss: 0.6169\tLR: 0.000000\n",
      "Training Epoch: 27 [48960/72641]\tLoss: 0.6271\tLR: 0.000000\n",
      "Training Epoch: 27 [49280/72641]\tLoss: 0.6247\tLR: 0.000000\n",
      "Training Epoch: 27 [49600/72641]\tLoss: 0.6803\tLR: 0.000000\n",
      "Training Epoch: 27 [49920/72641]\tLoss: 0.5947\tLR: 0.000000\n",
      "Training Epoch: 27 [50240/72641]\tLoss: 0.6289\tLR: 0.000000\n",
      "Training Epoch: 27 [50560/72641]\tLoss: 0.6152\tLR: 0.000000\n",
      "Training Epoch: 27 [50880/72641]\tLoss: 0.6412\tLR: 0.000000\n",
      "Training Epoch: 27 [51200/72641]\tLoss: 0.6689\tLR: 0.000000\n",
      "Training Epoch: 27 [51520/72641]\tLoss: 0.6392\tLR: 0.000000\n",
      "Training Epoch: 27 [51840/72641]\tLoss: 0.6178\tLR: 0.000000\n",
      "Training Epoch: 27 [52160/72641]\tLoss: 0.6811\tLR: 0.000000\n",
      "Training Epoch: 27 [52480/72641]\tLoss: 0.5996\tLR: 0.000000\n",
      "Training Epoch: 27 [52800/72641]\tLoss: 0.5697\tLR: 0.000000\n",
      "Training Epoch: 27 [53120/72641]\tLoss: 0.5856\tLR: 0.000000\n",
      "Training Epoch: 27 [53440/72641]\tLoss: 0.6327\tLR: 0.000000\n",
      "Training Epoch: 27 [53760/72641]\tLoss: 0.7064\tLR: 0.000000\n",
      "Training Epoch: 27 [54080/72641]\tLoss: 0.6362\tLR: 0.000000\n",
      "Training Epoch: 27 [54400/72641]\tLoss: 0.5828\tLR: 0.000000\n",
      "Training Epoch: 27 [54720/72641]\tLoss: 0.6328\tLR: 0.000000\n",
      "Training Epoch: 27 [55040/72641]\tLoss: 0.6930\tLR: 0.000000\n",
      "Training Epoch: 27 [55360/72641]\tLoss: 0.6236\tLR: 0.000000\n",
      "Training Epoch: 27 [55680/72641]\tLoss: 0.6355\tLR: 0.000000\n",
      "Training Epoch: 27 [56000/72641]\tLoss: 0.6063\tLR: 0.000000\n",
      "Training Epoch: 27 [56320/72641]\tLoss: 0.6041\tLR: 0.000000\n",
      "Training Epoch: 27 [56640/72641]\tLoss: 0.6461\tLR: 0.000000\n",
      "Training Epoch: 27 [56960/72641]\tLoss: 0.6495\tLR: 0.000000\n",
      "Training Epoch: 27 [57280/72641]\tLoss: 0.5735\tLR: 0.000000\n",
      "Training Epoch: 27 [57600/72641]\tLoss: 0.6604\tLR: 0.000000\n",
      "Training Epoch: 27 [57920/72641]\tLoss: 0.6625\tLR: 0.000000\n",
      "Training Epoch: 27 [58240/72641]\tLoss: 0.6119\tLR: 0.000000\n",
      "Training Epoch: 27 [58560/72641]\tLoss: 0.6154\tLR: 0.000000\n",
      "Training Epoch: 27 [58880/72641]\tLoss: 0.6072\tLR: 0.000000\n",
      "Training Epoch: 27 [59200/72641]\tLoss: 0.6456\tLR: 0.000000\n",
      "Training Epoch: 27 [59520/72641]\tLoss: 0.6445\tLR: 0.000000\n",
      "Training Epoch: 27 [59840/72641]\tLoss: 0.6265\tLR: 0.000000\n",
      "Training Epoch: 27 [60160/72641]\tLoss: 0.6404\tLR: 0.000000\n",
      "Training Epoch: 27 [60480/72641]\tLoss: 0.6380\tLR: 0.000000\n",
      "Training Epoch: 27 [60800/72641]\tLoss: 0.6042\tLR: 0.000000\n",
      "Training Epoch: 27 [61120/72641]\tLoss: 0.6147\tLR: 0.000000\n",
      "Training Epoch: 27 [61440/72641]\tLoss: 0.5656\tLR: 0.000000\n",
      "Training Epoch: 27 [61760/72641]\tLoss: 0.6012\tLR: 0.000000\n",
      "Training Epoch: 27 [62080/72641]\tLoss: 0.6871\tLR: 0.000000\n",
      "Training Epoch: 27 [62400/72641]\tLoss: 0.6403\tLR: 0.000000\n",
      "Training Epoch: 27 [62720/72641]\tLoss: 0.5886\tLR: 0.000000\n",
      "Training Epoch: 27 [63040/72641]\tLoss: 0.6171\tLR: 0.000000\n",
      "Training Epoch: 27 [63360/72641]\tLoss: 0.6295\tLR: 0.000000\n",
      "Training Epoch: 27 [63680/72641]\tLoss: 0.6237\tLR: 0.000000\n",
      "Training Epoch: 27 [64000/72641]\tLoss: 0.5926\tLR: 0.000000\n",
      "Training Epoch: 27 [64320/72641]\tLoss: 0.6077\tLR: 0.000000\n",
      "Training Epoch: 27 [64640/72641]\tLoss: 0.6152\tLR: 0.000000\n",
      "Training Epoch: 27 [64960/72641]\tLoss: 0.6299\tLR: 0.000000\n",
      "Training Epoch: 27 [65280/72641]\tLoss: 0.6231\tLR: 0.000000\n",
      "Training Epoch: 27 [65600/72641]\tLoss: 0.5908\tLR: 0.000000\n",
      "Training Epoch: 27 [65920/72641]\tLoss: 0.6166\tLR: 0.000000\n",
      "Training Epoch: 27 [66240/72641]\tLoss: 0.6571\tLR: 0.000000\n",
      "Training Epoch: 27 [66560/72641]\tLoss: 0.6290\tLR: 0.000000\n",
      "Training Epoch: 27 [66880/72641]\tLoss: 0.6364\tLR: 0.000000\n",
      "Training Epoch: 27 [67200/72641]\tLoss: 0.6128\tLR: 0.000000\n",
      "Training Epoch: 27 [67520/72641]\tLoss: 0.6375\tLR: 0.000000\n",
      "Training Epoch: 27 [67840/72641]\tLoss: 0.6226\tLR: 0.000000\n",
      "Training Epoch: 27 [68160/72641]\tLoss: 0.6255\tLR: 0.000000\n",
      "Training Epoch: 27 [68480/72641]\tLoss: 0.5919\tLR: 0.000000\n",
      "Training Epoch: 27 [68800/72641]\tLoss: 0.6433\tLR: 0.000000\n",
      "Training Epoch: 27 [69120/72641]\tLoss: 0.6282\tLR: 0.000000\n",
      "Training Epoch: 27 [69440/72641]\tLoss: 0.6222\tLR: 0.000000\n",
      "Training Epoch: 27 [69760/72641]\tLoss: 0.6580\tLR: 0.000000\n",
      "Training Epoch: 27 [70080/72641]\tLoss: 0.6457\tLR: 0.000000\n",
      "Training Epoch: 27 [70400/72641]\tLoss: 0.6132\tLR: 0.000000\n",
      "Training Epoch: 27 [70720/72641]\tLoss: 0.6709\tLR: 0.000000\n",
      "Training Epoch: 27 [71040/72641]\tLoss: 0.5836\tLR: 0.000000\n",
      "Training Epoch: 27 [71360/72641]\tLoss: 0.6582\tLR: 0.000000\n",
      "Training Epoch: 27 [71680/72641]\tLoss: 0.6797\tLR: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [72000/72641]\tLoss: 0.6618\tLR: 0.000000\n",
      "Training Epoch: 27 [72320/72641]\tLoss: 0.5631\tLR: 0.000000\n",
      "Training Epoch: 27 [72640/72641]\tLoss: 0.5766\tLR: 0.000000\n",
      "Val Result: Acc: 0.1468, C_ACC: 0.7088, DOA: 88.0165, ACC_k: 0.1086\n",
      "ext:0.0, cls:0.561748, coar:0.0, fine:0.0,\n",
      "Training Epoch: 28 [320/72641]\tLoss: 0.6818\tLR: 0.000000\n",
      "Training Epoch: 28 [640/72641]\tLoss: 0.6003\tLR: 0.000000\n",
      "Training Epoch: 28 [960/72641]\tLoss: 0.6011\tLR: 0.000000\n",
      "Training Epoch: 28 [1280/72641]\tLoss: 0.6486\tLR: 0.000000\n",
      "Training Epoch: 28 [1600/72641]\tLoss: 0.6160\tLR: 0.000000\n",
      "Training Epoch: 28 [1920/72641]\tLoss: 0.6209\tLR: 0.000000\n",
      "Training Epoch: 28 [2240/72641]\tLoss: 0.5863\tLR: 0.000000\n",
      "Training Epoch: 28 [2560/72641]\tLoss: 0.6272\tLR: 0.000000\n",
      "Training Epoch: 28 [2880/72641]\tLoss: 0.6726\tLR: 0.000000\n",
      "Training Epoch: 28 [3200/72641]\tLoss: 0.6149\tLR: 0.000000\n",
      "Training Epoch: 28 [3520/72641]\tLoss: 0.6401\tLR: 0.000000\n",
      "Training Epoch: 28 [3840/72641]\tLoss: 0.6059\tLR: 0.000000\n",
      "Training Epoch: 28 [4160/72641]\tLoss: 0.6654\tLR: 0.000000\n",
      "Training Epoch: 28 [4480/72641]\tLoss: 0.6474\tLR: 0.000000\n",
      "Training Epoch: 28 [4800/72641]\tLoss: 0.6204\tLR: 0.000000\n",
      "Training Epoch: 28 [5120/72641]\tLoss: 0.6150\tLR: 0.000000\n",
      "Training Epoch: 28 [5440/72641]\tLoss: 0.6254\tLR: 0.000000\n",
      "Training Epoch: 28 [5760/72641]\tLoss: 0.6330\tLR: 0.000000\n",
      "Training Epoch: 28 [6080/72641]\tLoss: 0.5559\tLR: 0.000000\n",
      "Training Epoch: 28 [6400/72641]\tLoss: 0.6189\tLR: 0.000000\n",
      "Training Epoch: 28 [6720/72641]\tLoss: 0.6209\tLR: 0.000000\n",
      "Training Epoch: 28 [7040/72641]\tLoss: 0.6406\tLR: 0.000000\n",
      "Training Epoch: 28 [7360/72641]\tLoss: 0.5904\tLR: 0.000000\n",
      "Training Epoch: 28 [7680/72641]\tLoss: 0.6204\tLR: 0.000000\n",
      "Training Epoch: 28 [8000/72641]\tLoss: 0.6609\tLR: 0.000000\n",
      "Training Epoch: 28 [8320/72641]\tLoss: 0.6552\tLR: 0.000000\n",
      "Training Epoch: 28 [8640/72641]\tLoss: 0.6597\tLR: 0.000000\n",
      "Training Epoch: 28 [8960/72641]\tLoss: 0.5586\tLR: 0.000000\n",
      "Training Epoch: 28 [9280/72641]\tLoss: 0.6225\tLR: 0.000000\n",
      "Training Epoch: 28 [9600/72641]\tLoss: 0.6243\tLR: 0.000000\n",
      "Training Epoch: 28 [9920/72641]\tLoss: 0.6268\tLR: 0.000000\n",
      "Training Epoch: 28 [10240/72641]\tLoss: 0.6599\tLR: 0.000000\n",
      "Training Epoch: 28 [10560/72641]\tLoss: 0.5845\tLR: 0.000000\n",
      "Training Epoch: 28 [10880/72641]\tLoss: 0.6258\tLR: 0.000000\n",
      "Training Epoch: 28 [11200/72641]\tLoss: 0.6421\tLR: 0.000000\n",
      "Training Epoch: 28 [11520/72641]\tLoss: 0.6295\tLR: 0.000000\n",
      "Training Epoch: 28 [11840/72641]\tLoss: 0.5943\tLR: 0.000000\n",
      "Training Epoch: 28 [12160/72641]\tLoss: 0.6509\tLR: 0.000000\n",
      "Training Epoch: 28 [12480/72641]\tLoss: 0.6735\tLR: 0.000000\n",
      "Training Epoch: 28 [12800/72641]\tLoss: 0.6198\tLR: 0.000000\n",
      "Training Epoch: 28 [13120/72641]\tLoss: 0.5903\tLR: 0.000000\n",
      "Training Epoch: 28 [13440/72641]\tLoss: 0.6258\tLR: 0.000000\n",
      "Training Epoch: 28 [13760/72641]\tLoss: 0.6523\tLR: 0.000000\n",
      "Training Epoch: 28 [14080/72641]\tLoss: 0.6096\tLR: 0.000000\n",
      "Training Epoch: 28 [14400/72641]\tLoss: 0.6444\tLR: 0.000000\n",
      "Training Epoch: 28 [14720/72641]\tLoss: 0.6260\tLR: 0.000000\n",
      "Training Epoch: 28 [15040/72641]\tLoss: 0.6539\tLR: 0.000000\n",
      "Training Epoch: 28 [15360/72641]\tLoss: 0.6577\tLR: 0.000000\n",
      "Training Epoch: 28 [15680/72641]\tLoss: 0.6463\tLR: 0.000000\n",
      "Training Epoch: 28 [16000/72641]\tLoss: 0.6425\tLR: 0.000000\n",
      "Training Epoch: 28 [16320/72641]\tLoss: 0.6291\tLR: 0.000000\n",
      "Training Epoch: 28 [16640/72641]\tLoss: 0.6056\tLR: 0.000000\n",
      "Training Epoch: 28 [16960/72641]\tLoss: 0.5960\tLR: 0.000000\n",
      "Training Epoch: 28 [17280/72641]\tLoss: 0.5813\tLR: 0.000000\n",
      "Training Epoch: 28 [17600/72641]\tLoss: 0.6330\tLR: 0.000000\n",
      "Training Epoch: 28 [17920/72641]\tLoss: 0.6191\tLR: 0.000000\n",
      "Training Epoch: 28 [18240/72641]\tLoss: 0.6408\tLR: 0.000000\n",
      "Training Epoch: 28 [18560/72641]\tLoss: 0.6384\tLR: 0.000000\n",
      "Training Epoch: 28 [18880/72641]\tLoss: 0.6657\tLR: 0.000000\n",
      "Training Epoch: 28 [19200/72641]\tLoss: 0.6447\tLR: 0.000000\n",
      "Training Epoch: 28 [19520/72641]\tLoss: 0.6244\tLR: 0.000000\n",
      "Training Epoch: 28 [19840/72641]\tLoss: 0.6065\tLR: 0.000000\n",
      "Training Epoch: 28 [20160/72641]\tLoss: 0.5819\tLR: 0.000000\n",
      "Training Epoch: 28 [20480/72641]\tLoss: 0.6109\tLR: 0.000000\n",
      "Training Epoch: 28 [20800/72641]\tLoss: 0.6050\tLR: 0.000000\n",
      "Training Epoch: 28 [21120/72641]\tLoss: 0.6664\tLR: 0.000000\n",
      "Training Epoch: 28 [21440/72641]\tLoss: 0.6106\tLR: 0.000000\n",
      "Training Epoch: 28 [21760/72641]\tLoss: 0.5869\tLR: 0.000000\n",
      "Training Epoch: 28 [22080/72641]\tLoss: 0.6099\tLR: 0.000000\n",
      "Training Epoch: 28 [22400/72641]\tLoss: 0.6256\tLR: 0.000000\n",
      "Training Epoch: 28 [22720/72641]\tLoss: 0.5924\tLR: 0.000000\n",
      "Training Epoch: 28 [23040/72641]\tLoss: 0.6077\tLR: 0.000000\n",
      "Training Epoch: 28 [23360/72641]\tLoss: 0.5999\tLR: 0.000000\n",
      "Training Epoch: 28 [23680/72641]\tLoss: 0.6558\tLR: 0.000000\n",
      "Training Epoch: 28 [24000/72641]\tLoss: 0.6326\tLR: 0.000000\n",
      "Training Epoch: 28 [24320/72641]\tLoss: 0.6365\tLR: 0.000000\n",
      "Training Epoch: 28 [24640/72641]\tLoss: 0.6498\tLR: 0.000000\n",
      "Training Epoch: 28 [24960/72641]\tLoss: 0.6492\tLR: 0.000000\n",
      "Training Epoch: 28 [25280/72641]\tLoss: 0.5990\tLR: 0.000000\n",
      "Training Epoch: 28 [25600/72641]\tLoss: 0.6229\tLR: 0.000000\n",
      "Training Epoch: 28 [25920/72641]\tLoss: 0.6001\tLR: 0.000000\n",
      "Training Epoch: 28 [26240/72641]\tLoss: 0.6301\tLR: 0.000000\n",
      "Training Epoch: 28 [26560/72641]\tLoss: 0.6359\tLR: 0.000000\n",
      "Training Epoch: 28 [26880/72641]\tLoss: 0.5733\tLR: 0.000000\n",
      "Training Epoch: 28 [27200/72641]\tLoss: 0.6108\tLR: 0.000000\n",
      "Training Epoch: 28 [27520/72641]\tLoss: 0.6333\tLR: 0.000000\n",
      "Training Epoch: 28 [27840/72641]\tLoss: 0.6032\tLR: 0.000000\n",
      "Training Epoch: 28 [28160/72641]\tLoss: 0.5956\tLR: 0.000000\n",
      "Training Epoch: 28 [28480/72641]\tLoss: 0.6476\tLR: 0.000000\n",
      "Training Epoch: 28 [28800/72641]\tLoss: 0.6104\tLR: 0.000000\n",
      "Training Epoch: 28 [29120/72641]\tLoss: 0.6404\tLR: 0.000000\n",
      "Training Epoch: 28 [29440/72641]\tLoss: 0.6538\tLR: 0.000000\n",
      "Training Epoch: 28 [29760/72641]\tLoss: 0.6552\tLR: 0.000000\n",
      "Training Epoch: 28 [30080/72641]\tLoss: 0.6243\tLR: 0.000000\n",
      "Training Epoch: 28 [30400/72641]\tLoss: 0.6140\tLR: 0.000000\n",
      "Training Epoch: 28 [30720/72641]\tLoss: 0.6555\tLR: 0.000000\n",
      "Training Epoch: 28 [31040/72641]\tLoss: 0.6533\tLR: 0.000000\n",
      "Training Epoch: 28 [31360/72641]\tLoss: 0.6185\tLR: 0.000000\n",
      "Training Epoch: 28 [31680/72641]\tLoss: 0.6696\tLR: 0.000000\n",
      "Training Epoch: 28 [32000/72641]\tLoss: 0.5789\tLR: 0.000000\n",
      "Training Epoch: 28 [32320/72641]\tLoss: 0.6051\tLR: 0.000000\n",
      "Training Epoch: 28 [32640/72641]\tLoss: 0.6031\tLR: 0.000000\n",
      "Training Epoch: 28 [32960/72641]\tLoss: 0.6425\tLR: 0.000000\n",
      "Training Epoch: 28 [33280/72641]\tLoss: 0.6539\tLR: 0.000000\n",
      "Training Epoch: 28 [33600/72641]\tLoss: 0.6297\tLR: 0.000000\n",
      "Training Epoch: 28 [33920/72641]\tLoss: 0.6288\tLR: 0.000000\n",
      "Training Epoch: 28 [34240/72641]\tLoss: 0.6282\tLR: 0.000000\n",
      "Training Epoch: 28 [34560/72641]\tLoss: 0.6656\tLR: 0.000000\n",
      "Training Epoch: 28 [34880/72641]\tLoss: 0.6197\tLR: 0.000000\n",
      "Training Epoch: 28 [35200/72641]\tLoss: 0.6072\tLR: 0.000000\n",
      "Training Epoch: 28 [35520/72641]\tLoss: 0.6534\tLR: 0.000000\n",
      "Training Epoch: 28 [35840/72641]\tLoss: 0.6458\tLR: 0.000000\n",
      "Training Epoch: 28 [36160/72641]\tLoss: 0.6587\tLR: 0.000000\n",
      "Training Epoch: 28 [36480/72641]\tLoss: 0.5926\tLR: 0.000000\n",
      "Training Epoch: 28 [36800/72641]\tLoss: 0.6247\tLR: 0.000000\n",
      "Training Epoch: 28 [37120/72641]\tLoss: 0.6731\tLR: 0.000000\n",
      "Training Epoch: 28 [37440/72641]\tLoss: 0.6482\tLR: 0.000000\n",
      "Training Epoch: 28 [37760/72641]\tLoss: 0.6445\tLR: 0.000000\n",
      "Training Epoch: 28 [38080/72641]\tLoss: 0.6372\tLR: 0.000000\n",
      "Training Epoch: 28 [38400/72641]\tLoss: 0.6421\tLR: 0.000000\n",
      "Training Epoch: 28 [38720/72641]\tLoss: 0.6493\tLR: 0.000000\n",
      "Training Epoch: 28 [39040/72641]\tLoss: 0.6439\tLR: 0.000000\n",
      "Training Epoch: 28 [39360/72641]\tLoss: 0.6464\tLR: 0.000000\n",
      "Training Epoch: 28 [39680/72641]\tLoss: 0.6395\tLR: 0.000000\n",
      "Training Epoch: 28 [40000/72641]\tLoss: 0.6285\tLR: 0.000000\n",
      "Training Epoch: 28 [40320/72641]\tLoss: 0.5902\tLR: 0.000000\n",
      "Training Epoch: 28 [40640/72641]\tLoss: 0.6138\tLR: 0.000000\n",
      "Training Epoch: 28 [40960/72641]\tLoss: 0.6390\tLR: 0.000000\n",
      "Training Epoch: 28 [41280/72641]\tLoss: 0.5971\tLR: 0.000000\n",
      "Training Epoch: 28 [41600/72641]\tLoss: 0.6037\tLR: 0.000000\n",
      "Training Epoch: 28 [41920/72641]\tLoss: 0.5698\tLR: 0.000000\n",
      "Training Epoch: 28 [42240/72641]\tLoss: 0.5696\tLR: 0.000000\n",
      "Training Epoch: 28 [42560/72641]\tLoss: 0.6037\tLR: 0.000000\n",
      "Training Epoch: 28 [42880/72641]\tLoss: 0.6655\tLR: 0.000000\n",
      "Training Epoch: 28 [43200/72641]\tLoss: 0.6691\tLR: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 28 [43520/72641]\tLoss: 0.6071\tLR: 0.000000\n",
      "Training Epoch: 28 [43840/72641]\tLoss: 0.6220\tLR: 0.000000\n",
      "Training Epoch: 28 [44160/72641]\tLoss: 0.6263\tLR: 0.000000\n",
      "Training Epoch: 28 [44480/72641]\tLoss: 0.5909\tLR: 0.000000\n",
      "Training Epoch: 28 [44800/72641]\tLoss: 0.5845\tLR: 0.000000\n",
      "Training Epoch: 28 [45120/72641]\tLoss: 0.6074\tLR: 0.000000\n",
      "Training Epoch: 28 [45440/72641]\tLoss: 0.6134\tLR: 0.000000\n",
      "Training Epoch: 28 [45760/72641]\tLoss: 0.6256\tLR: 0.000000\n",
      "Training Epoch: 28 [46080/72641]\tLoss: 0.6094\tLR: 0.000000\n",
      "Training Epoch: 28 [46400/72641]\tLoss: 0.6555\tLR: 0.000000\n",
      "Training Epoch: 28 [46720/72641]\tLoss: 0.6351\tLR: 0.000000\n",
      "Training Epoch: 28 [47040/72641]\tLoss: 0.6323\tLR: 0.000000\n",
      "Training Epoch: 28 [47360/72641]\tLoss: 0.6017\tLR: 0.000000\n",
      "Training Epoch: 28 [47680/72641]\tLoss: 0.6243\tLR: 0.000000\n",
      "Training Epoch: 28 [48000/72641]\tLoss: 0.6495\tLR: 0.000000\n",
      "Training Epoch: 28 [48320/72641]\tLoss: 0.6673\tLR: 0.000000\n",
      "Training Epoch: 28 [48640/72641]\tLoss: 0.6235\tLR: 0.000000\n",
      "Training Epoch: 28 [48960/72641]\tLoss: 0.6181\tLR: 0.000000\n",
      "Training Epoch: 28 [49280/72641]\tLoss: 0.6576\tLR: 0.000000\n",
      "Training Epoch: 28 [49600/72641]\tLoss: 0.6109\tLR: 0.000000\n",
      "Training Epoch: 28 [49920/72641]\tLoss: 0.6017\tLR: 0.000000\n",
      "Training Epoch: 28 [50240/72641]\tLoss: 0.6323\tLR: 0.000000\n",
      "Training Epoch: 28 [50560/72641]\tLoss: 0.6121\tLR: 0.000000\n",
      "Training Epoch: 28 [50880/72641]\tLoss: 0.6304\tLR: 0.000000\n",
      "Training Epoch: 28 [51200/72641]\tLoss: 0.6189\tLR: 0.000000\n",
      "Training Epoch: 28 [51520/72641]\tLoss: 0.5957\tLR: 0.000000\n",
      "Training Epoch: 28 [51840/72641]\tLoss: 0.5981\tLR: 0.000000\n",
      "Training Epoch: 28 [52160/72641]\tLoss: 0.6411\tLR: 0.000000\n",
      "Training Epoch: 28 [52480/72641]\tLoss: 0.6079\tLR: 0.000000\n",
      "Training Epoch: 28 [52800/72641]\tLoss: 0.6194\tLR: 0.000000\n",
      "Training Epoch: 28 [53120/72641]\tLoss: 0.5913\tLR: 0.000000\n",
      "Training Epoch: 28 [53440/72641]\tLoss: 0.6211\tLR: 0.000000\n",
      "Training Epoch: 28 [53760/72641]\tLoss: 0.6885\tLR: 0.000000\n",
      "Training Epoch: 28 [54080/72641]\tLoss: 0.5975\tLR: 0.000000\n",
      "Training Epoch: 28 [54400/72641]\tLoss: 0.6119\tLR: 0.000000\n",
      "Training Epoch: 28 [54720/72641]\tLoss: 0.6452\tLR: 0.000000\n",
      "Training Epoch: 28 [55040/72641]\tLoss: 0.6434\tLR: 0.000000\n",
      "Training Epoch: 28 [55360/72641]\tLoss: 0.6532\tLR: 0.000000\n",
      "Training Epoch: 28 [55680/72641]\tLoss: 0.5904\tLR: 0.000000\n",
      "Training Epoch: 28 [56000/72641]\tLoss: 0.5811\tLR: 0.000000\n",
      "Training Epoch: 28 [56320/72641]\tLoss: 0.6458\tLR: 0.000000\n",
      "Training Epoch: 28 [56640/72641]\tLoss: 0.6813\tLR: 0.000000\n",
      "Training Epoch: 28 [56960/72641]\tLoss: 0.6376\tLR: 0.000000\n",
      "Training Epoch: 28 [57280/72641]\tLoss: 0.5790\tLR: 0.000000\n",
      "Training Epoch: 28 [57600/72641]\tLoss: 0.6309\tLR: 0.000000\n",
      "Training Epoch: 28 [57920/72641]\tLoss: 0.6590\tLR: 0.000000\n",
      "Training Epoch: 28 [58240/72641]\tLoss: 0.6153\tLR: 0.000000\n",
      "Training Epoch: 28 [58560/72641]\tLoss: 0.5784\tLR: 0.000000\n",
      "Training Epoch: 28 [58880/72641]\tLoss: 0.6499\tLR: 0.000000\n",
      "Training Epoch: 28 [59200/72641]\tLoss: 0.6049\tLR: 0.000000\n",
      "Training Epoch: 28 [59520/72641]\tLoss: 0.6110\tLR: 0.000000\n",
      "Training Epoch: 28 [59840/72641]\tLoss: 0.6926\tLR: 0.000000\n",
      "Training Epoch: 28 [60160/72641]\tLoss: 0.6209\tLR: 0.000000\n",
      "Training Epoch: 28 [60480/72641]\tLoss: 0.6024\tLR: 0.000000\n",
      "Training Epoch: 28 [60800/72641]\tLoss: 0.6195\tLR: 0.000000\n",
      "Training Epoch: 28 [61120/72641]\tLoss: 0.5950\tLR: 0.000000\n",
      "Training Epoch: 28 [61440/72641]\tLoss: 0.5803\tLR: 0.000000\n",
      "Training Epoch: 28 [61760/72641]\tLoss: 0.5748\tLR: 0.000000\n",
      "Training Epoch: 28 [62080/72641]\tLoss: 0.6336\tLR: 0.000000\n",
      "Training Epoch: 28 [62400/72641]\tLoss: 0.6507\tLR: 0.000000\n",
      "Training Epoch: 28 [62720/72641]\tLoss: 0.6278\tLR: 0.000000\n",
      "Training Epoch: 28 [63040/72641]\tLoss: 0.6544\tLR: 0.000000\n",
      "Training Epoch: 28 [63360/72641]\tLoss: 0.6244\tLR: 0.000000\n",
      "Training Epoch: 28 [63680/72641]\tLoss: 0.6037\tLR: 0.000000\n",
      "Training Epoch: 28 [64000/72641]\tLoss: 0.5709\tLR: 0.000000\n",
      "Training Epoch: 28 [64320/72641]\tLoss: 0.6252\tLR: 0.000000\n",
      "Training Epoch: 28 [64640/72641]\tLoss: 0.6306\tLR: 0.000000\n",
      "Training Epoch: 28 [64960/72641]\tLoss: 0.6143\tLR: 0.000000\n",
      "Training Epoch: 28 [65280/72641]\tLoss: 0.6749\tLR: 0.000000\n",
      "Training Epoch: 28 [65600/72641]\tLoss: 0.6255\tLR: 0.000000\n",
      "Training Epoch: 28 [65920/72641]\tLoss: 0.6284\tLR: 0.000000\n",
      "Training Epoch: 28 [66240/72641]\tLoss: 0.5745\tLR: 0.000000\n",
      "Training Epoch: 28 [66560/72641]\tLoss: 0.6167\tLR: 0.000000\n",
      "Training Epoch: 28 [66880/72641]\tLoss: 0.6287\tLR: 0.000000\n",
      "Training Epoch: 28 [67200/72641]\tLoss: 0.6086\tLR: 0.000000\n",
      "Training Epoch: 28 [67520/72641]\tLoss: 0.6242\tLR: 0.000000\n",
      "Training Epoch: 28 [67840/72641]\tLoss: 0.6587\tLR: 0.000000\n",
      "Training Epoch: 28 [68160/72641]\tLoss: 0.6594\tLR: 0.000000\n",
      "Training Epoch: 28 [68480/72641]\tLoss: 0.6512\tLR: 0.000000\n",
      "Training Epoch: 28 [68800/72641]\tLoss: 0.6390\tLR: 0.000000\n",
      "Training Epoch: 28 [69120/72641]\tLoss: 0.6139\tLR: 0.000000\n",
      "Training Epoch: 28 [69440/72641]\tLoss: 0.6161\tLR: 0.000000\n",
      "Training Epoch: 28 [69760/72641]\tLoss: 0.5715\tLR: 0.000000\n",
      "Training Epoch: 28 [70080/72641]\tLoss: 0.6558\tLR: 0.000000\n",
      "Training Epoch: 28 [70400/72641]\tLoss: 0.6487\tLR: 0.000000\n",
      "Training Epoch: 28 [70720/72641]\tLoss: 0.6500\tLR: 0.000000\n",
      "Training Epoch: 28 [71040/72641]\tLoss: 0.6351\tLR: 0.000000\n",
      "Training Epoch: 28 [71360/72641]\tLoss: 0.6363\tLR: 0.000000\n",
      "Training Epoch: 28 [71680/72641]\tLoss: 0.6630\tLR: 0.000000\n",
      "Training Epoch: 28 [72000/72641]\tLoss: 0.6096\tLR: 0.000000\n",
      "Training Epoch: 28 [72320/72641]\tLoss: 0.6425\tLR: 0.000000\n",
      "Training Epoch: 28 [72640/72641]\tLoss: 0.6429\tLR: 0.000000\n",
      "Val Result: Acc: 0.1499, C_ACC: 0.7014, DOA: 88.0152, ACC_k: 0.1055\n",
      "ext:0.0, cls:0.571658, coar:0.0, fine:0.0,\n",
      "Training Epoch: 29 [320/72641]\tLoss: 0.6057\tLR: 0.000000\n",
      "Training Epoch: 29 [640/72641]\tLoss: 0.6268\tLR: 0.000000\n",
      "Training Epoch: 29 [960/72641]\tLoss: 0.6625\tLR: 0.000000\n",
      "Training Epoch: 29 [1280/72641]\tLoss: 0.6392\tLR: 0.000000\n",
      "Training Epoch: 29 [1600/72641]\tLoss: 0.6584\tLR: 0.000000\n",
      "Training Epoch: 29 [1920/72641]\tLoss: 0.6184\tLR: 0.000000\n",
      "Training Epoch: 29 [2240/72641]\tLoss: 0.6350\tLR: 0.000000\n",
      "Training Epoch: 29 [2560/72641]\tLoss: 0.6168\tLR: 0.000000\n",
      "Training Epoch: 29 [2880/72641]\tLoss: 0.6319\tLR: 0.000000\n",
      "Training Epoch: 29 [3200/72641]\tLoss: 0.6380\tLR: 0.000000\n",
      "Training Epoch: 29 [3520/72641]\tLoss: 0.5811\tLR: 0.000000\n",
      "Training Epoch: 29 [3840/72641]\tLoss: 0.6096\tLR: 0.000000\n",
      "Training Epoch: 29 [4160/72641]\tLoss: 0.6558\tLR: 0.000000\n",
      "Training Epoch: 29 [4480/72641]\tLoss: 0.6087\tLR: 0.000000\n",
      "Training Epoch: 29 [4800/72641]\tLoss: 0.6371\tLR: 0.000000\n",
      "Training Epoch: 29 [5120/72641]\tLoss: 0.5703\tLR: 0.000000\n",
      "Training Epoch: 29 [5440/72641]\tLoss: 0.6146\tLR: 0.000000\n",
      "Training Epoch: 29 [5760/72641]\tLoss: 0.6288\tLR: 0.000000\n",
      "Training Epoch: 29 [6080/72641]\tLoss: 0.6074\tLR: 0.000000\n",
      "Training Epoch: 29 [6400/72641]\tLoss: 0.6156\tLR: 0.000000\n",
      "Training Epoch: 29 [6720/72641]\tLoss: 0.6590\tLR: 0.000000\n",
      "Training Epoch: 29 [7040/72641]\tLoss: 0.6338\tLR: 0.000000\n",
      "Training Epoch: 29 [7360/72641]\tLoss: 0.6320\tLR: 0.000000\n",
      "Training Epoch: 29 [7680/72641]\tLoss: 0.6663\tLR: 0.000000\n",
      "Training Epoch: 29 [8000/72641]\tLoss: 0.6317\tLR: 0.000000\n",
      "Training Epoch: 29 [8320/72641]\tLoss: 0.6146\tLR: 0.000000\n",
      "Training Epoch: 29 [8640/72641]\tLoss: 0.6452\tLR: 0.000000\n",
      "Training Epoch: 29 [8960/72641]\tLoss: 0.5922\tLR: 0.000000\n",
      "Training Epoch: 29 [9280/72641]\tLoss: 0.5723\tLR: 0.000000\n",
      "Training Epoch: 29 [9600/72641]\tLoss: 0.6039\tLR: 0.000000\n",
      "Training Epoch: 29 [9920/72641]\tLoss: 0.6523\tLR: 0.000000\n",
      "Training Epoch: 29 [10240/72641]\tLoss: 0.6535\tLR: 0.000000\n",
      "Training Epoch: 29 [10560/72641]\tLoss: 0.6025\tLR: 0.000000\n",
      "Training Epoch: 29 [10880/72641]\tLoss: 0.6589\tLR: 0.000000\n",
      "Training Epoch: 29 [11200/72641]\tLoss: 0.6170\tLR: 0.000000\n",
      "Training Epoch: 29 [11520/72641]\tLoss: 0.6037\tLR: 0.000000\n",
      "Training Epoch: 29 [11840/72641]\tLoss: 0.6099\tLR: 0.000000\n",
      "Training Epoch: 29 [12160/72641]\tLoss: 0.5753\tLR: 0.000000\n",
      "Training Epoch: 29 [12480/72641]\tLoss: 0.6653\tLR: 0.000000\n",
      "Training Epoch: 29 [12800/72641]\tLoss: 0.6598\tLR: 0.000000\n",
      "Training Epoch: 29 [13120/72641]\tLoss: 0.6276\tLR: 0.000000\n",
      "Training Epoch: 29 [13440/72641]\tLoss: 0.6605\tLR: 0.000000\n",
      "Training Epoch: 29 [13760/72641]\tLoss: 0.6897\tLR: 0.000000\n",
      "Training Epoch: 29 [14080/72641]\tLoss: 0.6283\tLR: 0.000000\n",
      "Training Epoch: 29 [14400/72641]\tLoss: 0.5850\tLR: 0.000000\n",
      "Training Epoch: 29 [14720/72641]\tLoss: 0.6081\tLR: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [15040/72641]\tLoss: 0.6461\tLR: 0.000000\n",
      "Training Epoch: 29 [15360/72641]\tLoss: 0.6379\tLR: 0.000000\n",
      "Training Epoch: 29 [15680/72641]\tLoss: 0.6476\tLR: 0.000000\n",
      "Training Epoch: 29 [16000/72641]\tLoss: 0.6330\tLR: 0.000000\n",
      "Training Epoch: 29 [16320/72641]\tLoss: 0.6374\tLR: 0.000000\n",
      "Training Epoch: 29 [16640/72641]\tLoss: 0.6430\tLR: 0.000000\n",
      "Training Epoch: 29 [16960/72641]\tLoss: 0.6062\tLR: 0.000000\n",
      "Training Epoch: 29 [17280/72641]\tLoss: 0.6000\tLR: 0.000000\n",
      "Training Epoch: 29 [17600/72641]\tLoss: 0.5775\tLR: 0.000000\n",
      "Training Epoch: 29 [17920/72641]\tLoss: 0.6620\tLR: 0.000000\n",
      "Training Epoch: 29 [18240/72641]\tLoss: 0.6281\tLR: 0.000000\n",
      "Training Epoch: 29 [18560/72641]\tLoss: 0.6420\tLR: 0.000000\n",
      "Training Epoch: 29 [18880/72641]\tLoss: 0.6232\tLR: 0.000000\n",
      "Training Epoch: 29 [19200/72641]\tLoss: 0.6791\tLR: 0.000000\n",
      "Training Epoch: 29 [19520/72641]\tLoss: 0.6496\tLR: 0.000000\n",
      "Training Epoch: 29 [19840/72641]\tLoss: 0.6012\tLR: 0.000000\n",
      "Training Epoch: 29 [20160/72641]\tLoss: 0.5851\tLR: 0.000000\n",
      "Training Epoch: 29 [20480/72641]\tLoss: 0.5843\tLR: 0.000000\n",
      "Training Epoch: 29 [20800/72641]\tLoss: 0.6502\tLR: 0.000000\n",
      "Training Epoch: 29 [21120/72641]\tLoss: 0.6456\tLR: 0.000000\n",
      "Training Epoch: 29 [21440/72641]\tLoss: 0.6097\tLR: 0.000000\n",
      "Training Epoch: 29 [21760/72641]\tLoss: 0.5897\tLR: 0.000000\n",
      "Training Epoch: 29 [22080/72641]\tLoss: 0.6259\tLR: 0.000000\n",
      "Training Epoch: 29 [22400/72641]\tLoss: 0.5515\tLR: 0.000000\n",
      "Training Epoch: 29 [22720/72641]\tLoss: 0.6097\tLR: 0.000000\n",
      "Training Epoch: 29 [23040/72641]\tLoss: 0.6079\tLR: 0.000000\n",
      "Training Epoch: 29 [23360/72641]\tLoss: 0.5789\tLR: 0.000000\n",
      "Training Epoch: 29 [23680/72641]\tLoss: 0.6062\tLR: 0.000000\n",
      "Training Epoch: 29 [24000/72641]\tLoss: 0.5948\tLR: 0.000000\n",
      "Training Epoch: 29 [24320/72641]\tLoss: 0.5755\tLR: 0.000000\n",
      "Training Epoch: 29 [24640/72641]\tLoss: 0.6231\tLR: 0.000000\n",
      "Training Epoch: 29 [24960/72641]\tLoss: 0.6311\tLR: 0.000000\n",
      "Training Epoch: 29 [25280/72641]\tLoss: 0.6323\tLR: 0.000000\n",
      "Training Epoch: 29 [25600/72641]\tLoss: 0.5887\tLR: 0.000000\n",
      "Training Epoch: 29 [25920/72641]\tLoss: 0.6037\tLR: 0.000000\n",
      "Training Epoch: 29 [26240/72641]\tLoss: 0.5920\tLR: 0.000000\n",
      "Training Epoch: 29 [26560/72641]\tLoss: 0.6260\tLR: 0.000000\n",
      "Training Epoch: 29 [26880/72641]\tLoss: 0.5934\tLR: 0.000000\n",
      "Training Epoch: 29 [27200/72641]\tLoss: 0.6626\tLR: 0.000000\n",
      "Training Epoch: 29 [27520/72641]\tLoss: 0.6267\tLR: 0.000000\n",
      "Training Epoch: 29 [27840/72641]\tLoss: 0.6355\tLR: 0.000000\n",
      "Training Epoch: 29 [28160/72641]\tLoss: 0.6323\tLR: 0.000000\n",
      "Training Epoch: 29 [28480/72641]\tLoss: 0.6099\tLR: 0.000000\n",
      "Training Epoch: 29 [28800/72641]\tLoss: 0.6014\tLR: 0.000000\n",
      "Training Epoch: 29 [29120/72641]\tLoss: 0.6062\tLR: 0.000000\n",
      "Training Epoch: 29 [29440/72641]\tLoss: 0.6078\tLR: 0.000000\n",
      "Training Epoch: 29 [29760/72641]\tLoss: 0.6571\tLR: 0.000000\n",
      "Training Epoch: 29 [30080/72641]\tLoss: 0.6399\tLR: 0.000000\n",
      "Training Epoch: 29 [30400/72641]\tLoss: 0.6340\tLR: 0.000000\n",
      "Training Epoch: 29 [30720/72641]\tLoss: 0.6809\tLR: 0.000000\n",
      "Training Epoch: 29 [31040/72641]\tLoss: 0.6251\tLR: 0.000000\n",
      "Training Epoch: 29 [31360/72641]\tLoss: 0.5643\tLR: 0.000000\n",
      "Training Epoch: 29 [31680/72641]\tLoss: 0.6541\tLR: 0.000000\n",
      "Training Epoch: 29 [32000/72641]\tLoss: 0.6294\tLR: 0.000000\n",
      "Training Epoch: 29 [32320/72641]\tLoss: 0.6309\tLR: 0.000000\n",
      "Training Epoch: 29 [32640/72641]\tLoss: 0.6607\tLR: 0.000000\n",
      "Training Epoch: 29 [32960/72641]\tLoss: 0.6453\tLR: 0.000000\n",
      "Training Epoch: 29 [33280/72641]\tLoss: 0.6874\tLR: 0.000000\n",
      "Training Epoch: 29 [33600/72641]\tLoss: 0.5663\tLR: 0.000000\n",
      "Training Epoch: 29 [33920/72641]\tLoss: 0.6562\tLR: 0.000000\n",
      "Training Epoch: 29 [34240/72641]\tLoss: 0.5906\tLR: 0.000000\n",
      "Training Epoch: 29 [34560/72641]\tLoss: 0.6220\tLR: 0.000000\n",
      "Training Epoch: 29 [34880/72641]\tLoss: 0.6603\tLR: 0.000000\n",
      "Training Epoch: 29 [35200/72641]\tLoss: 0.5795\tLR: 0.000000\n",
      "Training Epoch: 29 [35520/72641]\tLoss: 0.6000\tLR: 0.000000\n",
      "Training Epoch: 29 [35840/72641]\tLoss: 0.6125\tLR: 0.000000\n",
      "Training Epoch: 29 [36160/72641]\tLoss: 0.6414\tLR: 0.000000\n",
      "Training Epoch: 29 [36480/72641]\tLoss: 0.5938\tLR: 0.000000\n",
      "Training Epoch: 29 [36800/72641]\tLoss: 0.6284\tLR: 0.000000\n",
      "Training Epoch: 29 [37120/72641]\tLoss: 0.6503\tLR: 0.000000\n",
      "Training Epoch: 29 [37440/72641]\tLoss: 0.6157\tLR: 0.000000\n",
      "Training Epoch: 29 [37760/72641]\tLoss: 0.6498\tLR: 0.000000\n",
      "Training Epoch: 29 [38080/72641]\tLoss: 0.6232\tLR: 0.000000\n",
      "Training Epoch: 29 [38400/72641]\tLoss: 0.6433\tLR: 0.000000\n",
      "Training Epoch: 29 [38720/72641]\tLoss: 0.6087\tLR: 0.000000\n",
      "Training Epoch: 29 [39040/72641]\tLoss: 0.6254\tLR: 0.000000\n",
      "Training Epoch: 29 [39360/72641]\tLoss: 0.5988\tLR: 0.000000\n",
      "Training Epoch: 29 [39680/72641]\tLoss: 0.6052\tLR: 0.000000\n",
      "Training Epoch: 29 [40000/72641]\tLoss: 0.6296\tLR: 0.000000\n",
      "Training Epoch: 29 [40320/72641]\tLoss: 0.6021\tLR: 0.000000\n",
      "Training Epoch: 29 [40640/72641]\tLoss: 0.6261\tLR: 0.000000\n",
      "Training Epoch: 29 [40960/72641]\tLoss: 0.6178\tLR: 0.000000\n",
      "Training Epoch: 29 [41280/72641]\tLoss: 0.6795\tLR: 0.000000\n",
      "Training Epoch: 29 [41600/72641]\tLoss: 0.6526\tLR: 0.000000\n",
      "Training Epoch: 29 [41920/72641]\tLoss: 0.5979\tLR: 0.000000\n",
      "Training Epoch: 29 [42240/72641]\tLoss: 0.5887\tLR: 0.000000\n",
      "Training Epoch: 29 [42560/72641]\tLoss: 0.6259\tLR: 0.000000\n",
      "Training Epoch: 29 [42880/72641]\tLoss: 0.6451\tLR: 0.000000\n",
      "Training Epoch: 29 [43200/72641]\tLoss: 0.5758\tLR: 0.000000\n",
      "Training Epoch: 29 [43520/72641]\tLoss: 0.6464\tLR: 0.000000\n",
      "Training Epoch: 29 [43840/72641]\tLoss: 0.6057\tLR: 0.000000\n",
      "Training Epoch: 29 [44160/72641]\tLoss: 0.6265\tLR: 0.000000\n",
      "Training Epoch: 29 [44480/72641]\tLoss: 0.6052\tLR: 0.000000\n",
      "Training Epoch: 29 [44800/72641]\tLoss: 0.5728\tLR: 0.000000\n",
      "Training Epoch: 29 [45120/72641]\tLoss: 0.5702\tLR: 0.000000\n",
      "Training Epoch: 29 [45440/72641]\tLoss: 0.5990\tLR: 0.000000\n",
      "Training Epoch: 29 [45760/72641]\tLoss: 0.6400\tLR: 0.000000\n",
      "Training Epoch: 29 [46080/72641]\tLoss: 0.6246\tLR: 0.000000\n",
      "Training Epoch: 29 [46400/72641]\tLoss: 0.6606\tLR: 0.000000\n",
      "Training Epoch: 29 [46720/72641]\tLoss: 0.6513\tLR: 0.000000\n",
      "Training Epoch: 29 [47040/72641]\tLoss: 0.5911\tLR: 0.000000\n",
      "Training Epoch: 29 [47360/72641]\tLoss: 0.6038\tLR: 0.000000\n",
      "Training Epoch: 29 [47680/72641]\tLoss: 0.5825\tLR: 0.000000\n",
      "Training Epoch: 29 [48000/72641]\tLoss: 0.6277\tLR: 0.000000\n",
      "Training Epoch: 29 [48320/72641]\tLoss: 0.7014\tLR: 0.000000\n",
      "Training Epoch: 29 [48640/72641]\tLoss: 0.6149\tLR: 0.000000\n",
      "Training Epoch: 29 [48960/72641]\tLoss: 0.6330\tLR: 0.000000\n",
      "Training Epoch: 29 [49280/72641]\tLoss: 0.6196\tLR: 0.000000\n",
      "Training Epoch: 29 [49600/72641]\tLoss: 0.6482\tLR: 0.000000\n",
      "Training Epoch: 29 [49920/72641]\tLoss: 0.6151\tLR: 0.000000\n",
      "Training Epoch: 29 [50240/72641]\tLoss: 0.5983\tLR: 0.000000\n",
      "Training Epoch: 29 [50560/72641]\tLoss: 0.6460\tLR: 0.000000\n",
      "Training Epoch: 29 [50880/72641]\tLoss: 0.6589\tLR: 0.000000\n",
      "Training Epoch: 29 [51200/72641]\tLoss: 0.6175\tLR: 0.000000\n",
      "Training Epoch: 29 [51520/72641]\tLoss: 0.6092\tLR: 0.000000\n",
      "Training Epoch: 29 [51840/72641]\tLoss: 0.5983\tLR: 0.000000\n",
      "Training Epoch: 29 [52160/72641]\tLoss: 0.6542\tLR: 0.000000\n",
      "Training Epoch: 29 [52480/72641]\tLoss: 0.6489\tLR: 0.000000\n",
      "Training Epoch: 29 [52800/72641]\tLoss: 0.6761\tLR: 0.000000\n",
      "Training Epoch: 29 [53120/72641]\tLoss: 0.6368\tLR: 0.000000\n",
      "Training Epoch: 29 [53440/72641]\tLoss: 0.5933\tLR: 0.000000\n",
      "Training Epoch: 29 [53760/72641]\tLoss: 0.6763\tLR: 0.000000\n",
      "Training Epoch: 29 [54080/72641]\tLoss: 0.5766\tLR: 0.000000\n",
      "Training Epoch: 29 [54400/72641]\tLoss: 0.6688\tLR: 0.000000\n",
      "Training Epoch: 29 [54720/72641]\tLoss: 0.6137\tLR: 0.000000\n",
      "Training Epoch: 29 [55040/72641]\tLoss: 0.6036\tLR: 0.000000\n",
      "Training Epoch: 29 [55360/72641]\tLoss: 0.6134\tLR: 0.000000\n",
      "Training Epoch: 29 [55680/72641]\tLoss: 0.5954\tLR: 0.000000\n",
      "Training Epoch: 29 [56000/72641]\tLoss: 0.5992\tLR: 0.000000\n",
      "Training Epoch: 29 [56320/72641]\tLoss: 0.6257\tLR: 0.000000\n",
      "Training Epoch: 29 [56640/72641]\tLoss: 0.6509\tLR: 0.000000\n",
      "Training Epoch: 29 [56960/72641]\tLoss: 0.5859\tLR: 0.000000\n",
      "Training Epoch: 29 [57280/72641]\tLoss: 0.6085\tLR: 0.000000\n",
      "Training Epoch: 29 [57600/72641]\tLoss: 0.6351\tLR: 0.000000\n",
      "Training Epoch: 29 [57920/72641]\tLoss: 0.6386\tLR: 0.000000\n",
      "Training Epoch: 29 [58240/72641]\tLoss: 0.6066\tLR: 0.000000\n",
      "Training Epoch: 29 [58560/72641]\tLoss: 0.6341\tLR: 0.000000\n",
      "Training Epoch: 29 [58880/72641]\tLoss: 0.6560\tLR: 0.000000\n",
      "Training Epoch: 29 [59200/72641]\tLoss: 0.6439\tLR: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [59520/72641]\tLoss: 0.5784\tLR: 0.000000\n",
      "Training Epoch: 29 [59840/72641]\tLoss: 0.6429\tLR: 0.000000\n",
      "Training Epoch: 29 [60160/72641]\tLoss: 0.6078\tLR: 0.000000\n",
      "Training Epoch: 29 [60480/72641]\tLoss: 0.6606\tLR: 0.000000\n",
      "Training Epoch: 29 [60800/72641]\tLoss: 0.6142\tLR: 0.000000\n",
      "Training Epoch: 29 [61120/72641]\tLoss: 0.6205\tLR: 0.000000\n",
      "Training Epoch: 29 [61440/72641]\tLoss: 0.6056\tLR: 0.000000\n",
      "Training Epoch: 29 [61760/72641]\tLoss: 0.6029\tLR: 0.000000\n",
      "Training Epoch: 29 [62080/72641]\tLoss: 0.6629\tLR: 0.000000\n",
      "Training Epoch: 29 [62400/72641]\tLoss: 0.6348\tLR: 0.000000\n",
      "Training Epoch: 29 [62720/72641]\tLoss: 0.6094\tLR: 0.000000\n",
      "Training Epoch: 29 [63040/72641]\tLoss: 0.6449\tLR: 0.000000\n",
      "Training Epoch: 29 [63360/72641]\tLoss: 0.6515\tLR: 0.000000\n",
      "Training Epoch: 29 [63680/72641]\tLoss: 0.6509\tLR: 0.000000\n",
      "Training Epoch: 29 [64000/72641]\tLoss: 0.5862\tLR: 0.000000\n",
      "Training Epoch: 29 [64320/72641]\tLoss: 0.5701\tLR: 0.000000\n",
      "Training Epoch: 29 [64640/72641]\tLoss: 0.6035\tLR: 0.000000\n",
      "Training Epoch: 29 [64960/72641]\tLoss: 0.6437\tLR: 0.000000\n",
      "Training Epoch: 29 [65280/72641]\tLoss: 0.6137\tLR: 0.000000\n",
      "Training Epoch: 29 [65600/72641]\tLoss: 0.6031\tLR: 0.000000\n",
      "Training Epoch: 29 [65920/72641]\tLoss: 0.6243\tLR: 0.000000\n",
      "Training Epoch: 29 [66240/72641]\tLoss: 0.6222\tLR: 0.000000\n",
      "Training Epoch: 29 [66560/72641]\tLoss: 0.5871\tLR: 0.000000\n",
      "Training Epoch: 29 [66880/72641]\tLoss: 0.6028\tLR: 0.000000\n",
      "Training Epoch: 29 [67200/72641]\tLoss: 0.5771\tLR: 0.000000\n",
      "Training Epoch: 29 [67520/72641]\tLoss: 0.6507\tLR: 0.000000\n",
      "Training Epoch: 29 [67840/72641]\tLoss: 0.6320\tLR: 0.000000\n",
      "Training Epoch: 29 [68160/72641]\tLoss: 0.6215\tLR: 0.000000\n",
      "Training Epoch: 29 [68480/72641]\tLoss: 0.6469\tLR: 0.000000\n",
      "Training Epoch: 29 [68800/72641]\tLoss: 0.6267\tLR: 0.000000\n",
      "Training Epoch: 29 [69120/72641]\tLoss: 0.6447\tLR: 0.000000\n",
      "Training Epoch: 29 [69440/72641]\tLoss: 0.5965\tLR: 0.000000\n",
      "Training Epoch: 29 [69760/72641]\tLoss: 0.6272\tLR: 0.000000\n",
      "Training Epoch: 29 [70080/72641]\tLoss: 0.6109\tLR: 0.000000\n",
      "Training Epoch: 29 [70400/72641]\tLoss: 0.6338\tLR: 0.000000\n",
      "Training Epoch: 29 [70720/72641]\tLoss: 0.6290\tLR: 0.000000\n",
      "Training Epoch: 29 [71040/72641]\tLoss: 0.6309\tLR: 0.000000\n",
      "Training Epoch: 29 [71360/72641]\tLoss: 0.6368\tLR: 0.000000\n",
      "Training Epoch: 29 [71680/72641]\tLoss: 0.6538\tLR: 0.000000\n",
      "Training Epoch: 29 [72000/72641]\tLoss: 0.6202\tLR: 0.000000\n",
      "Training Epoch: 29 [72320/72641]\tLoss: 0.5773\tLR: 0.000000\n",
      "Training Epoch: 29 [72640/72641]\tLoss: 0.6063\tLR: 0.000000\n",
      "Val Result: Acc: 0.1462, C_ACC: 0.7087, DOA: 88.0410, ACC_k: 0.0998\n",
      "ext:0.0, cls:0.559723, coar:0.0, fine:0.0,\n",
      "Training Epoch: 30 [320/72641]\tLoss: 0.6102\tLR: 0.000000\n",
      "Training Epoch: 30 [640/72641]\tLoss: 0.6231\tLR: 0.000000\n",
      "Training Epoch: 30 [960/72641]\tLoss: 0.6616\tLR: 0.000000\n",
      "Training Epoch: 30 [1280/72641]\tLoss: 0.5978\tLR: 0.000000\n",
      "Training Epoch: 30 [1600/72641]\tLoss: 0.6625\tLR: 0.000000\n",
      "Training Epoch: 30 [1920/72641]\tLoss: 0.6070\tLR: 0.000000\n",
      "Training Epoch: 30 [2240/72641]\tLoss: 0.5908\tLR: 0.000000\n",
      "Training Epoch: 30 [2560/72641]\tLoss: 0.5782\tLR: 0.000000\n",
      "Training Epoch: 30 [2880/72641]\tLoss: 0.6351\tLR: 0.000000\n",
      "Training Epoch: 30 [3200/72641]\tLoss: 0.6223\tLR: 0.000000\n",
      "Training Epoch: 30 [3520/72641]\tLoss: 0.6272\tLR: 0.000000\n",
      "Training Epoch: 30 [3840/72641]\tLoss: 0.5672\tLR: 0.000000\n",
      "Training Epoch: 30 [4160/72641]\tLoss: 0.6587\tLR: 0.000000\n",
      "Training Epoch: 30 [4480/72641]\tLoss: 0.5948\tLR: 0.000000\n",
      "Training Epoch: 30 [4800/72641]\tLoss: 0.6487\tLR: 0.000000\n",
      "Training Epoch: 30 [5120/72641]\tLoss: 0.6340\tLR: 0.000000\n",
      "Training Epoch: 30 [5440/72641]\tLoss: 0.6478\tLR: 0.000000\n",
      "Training Epoch: 30 [5760/72641]\tLoss: 0.5891\tLR: 0.000000\n",
      "Training Epoch: 30 [6080/72641]\tLoss: 0.6505\tLR: 0.000000\n",
      "Training Epoch: 30 [6400/72641]\tLoss: 0.6280\tLR: 0.000000\n",
      "Training Epoch: 30 [6720/72641]\tLoss: 0.6427\tLR: 0.000000\n",
      "Training Epoch: 30 [7040/72641]\tLoss: 0.6350\tLR: 0.000000\n",
      "Training Epoch: 30 [7360/72641]\tLoss: 0.6031\tLR: 0.000000\n",
      "Training Epoch: 30 [7680/72641]\tLoss: 0.6169\tLR: 0.000000\n",
      "Training Epoch: 30 [8000/72641]\tLoss: 0.6441\tLR: 0.000000\n",
      "Training Epoch: 30 [8320/72641]\tLoss: 0.6345\tLR: 0.000000\n",
      "Training Epoch: 30 [8640/72641]\tLoss: 0.6976\tLR: 0.000000\n",
      "Training Epoch: 30 [8960/72641]\tLoss: 0.5771\tLR: 0.000000\n",
      "Training Epoch: 30 [9280/72641]\tLoss: 0.6190\tLR: 0.000000\n",
      "Training Epoch: 30 [9600/72641]\tLoss: 0.6368\tLR: 0.000000\n",
      "Training Epoch: 30 [9920/72641]\tLoss: 0.6497\tLR: 0.000000\n",
      "Training Epoch: 30 [10240/72641]\tLoss: 0.6375\tLR: 0.000000\n",
      "Training Epoch: 30 [10560/72641]\tLoss: 0.5867\tLR: 0.000000\n",
      "Training Epoch: 30 [10880/72641]\tLoss: 0.6452\tLR: 0.000000\n",
      "Training Epoch: 30 [11200/72641]\tLoss: 0.6443\tLR: 0.000000\n",
      "Training Epoch: 30 [11520/72641]\tLoss: 0.6360\tLR: 0.000000\n",
      "Training Epoch: 30 [11840/72641]\tLoss: 0.5993\tLR: 0.000000\n",
      "Training Epoch: 30 [12160/72641]\tLoss: 0.6148\tLR: 0.000000\n",
      "Training Epoch: 30 [12480/72641]\tLoss: 0.6127\tLR: 0.000000\n",
      "Training Epoch: 30 [12800/72641]\tLoss: 0.6085\tLR: 0.000000\n",
      "Training Epoch: 30 [13120/72641]\tLoss: 0.6061\tLR: 0.000000\n",
      "Training Epoch: 30 [13440/72641]\tLoss: 0.6422\tLR: 0.000000\n",
      "Training Epoch: 30 [13760/72641]\tLoss: 0.6089\tLR: 0.000000\n",
      "Training Epoch: 30 [14080/72641]\tLoss: 0.6418\tLR: 0.000000\n",
      "Training Epoch: 30 [14400/72641]\tLoss: 0.6284\tLR: 0.000000\n",
      "Training Epoch: 30 [14720/72641]\tLoss: 0.5996\tLR: 0.000000\n",
      "Training Epoch: 30 [15040/72641]\tLoss: 0.5855\tLR: 0.000000\n",
      "Training Epoch: 30 [15360/72641]\tLoss: 0.6563\tLR: 0.000000\n",
      "Training Epoch: 30 [15680/72641]\tLoss: 0.6340\tLR: 0.000000\n",
      "Training Epoch: 30 [16000/72641]\tLoss: 0.6343\tLR: 0.000000\n",
      "Training Epoch: 30 [16320/72641]\tLoss: 0.6288\tLR: 0.000000\n",
      "Training Epoch: 30 [16640/72641]\tLoss: 0.6402\tLR: 0.000000\n",
      "Training Epoch: 30 [16960/72641]\tLoss: 0.6149\tLR: 0.000000\n",
      "Training Epoch: 30 [17280/72641]\tLoss: 0.5993\tLR: 0.000000\n",
      "Training Epoch: 30 [17600/72641]\tLoss: 0.6054\tLR: 0.000000\n",
      "Training Epoch: 30 [17920/72641]\tLoss: 0.6243\tLR: 0.000000\n",
      "Training Epoch: 30 [18240/72641]\tLoss: 0.6104\tLR: 0.000000\n",
      "Training Epoch: 30 [18560/72641]\tLoss: 0.5882\tLR: 0.000000\n",
      "Training Epoch: 30 [18880/72641]\tLoss: 0.5858\tLR: 0.000000\n",
      "Training Epoch: 30 [19200/72641]\tLoss: 0.6432\tLR: 0.000000\n",
      "Training Epoch: 30 [19520/72641]\tLoss: 0.6081\tLR: 0.000000\n",
      "Training Epoch: 30 [19840/72641]\tLoss: 0.6615\tLR: 0.000000\n",
      "Training Epoch: 30 [20160/72641]\tLoss: 0.6066\tLR: 0.000000\n",
      "Training Epoch: 30 [20480/72641]\tLoss: 0.6353\tLR: 0.000000\n",
      "Training Epoch: 30 [20800/72641]\tLoss: 0.6490\tLR: 0.000000\n",
      "Training Epoch: 30 [21120/72641]\tLoss: 0.6309\tLR: 0.000000\n",
      "Training Epoch: 30 [21440/72641]\tLoss: 0.6073\tLR: 0.000000\n",
      "Training Epoch: 30 [21760/72641]\tLoss: 0.6227\tLR: 0.000000\n",
      "Training Epoch: 30 [22080/72641]\tLoss: 0.6719\tLR: 0.000000\n",
      "Training Epoch: 30 [22400/72641]\tLoss: 0.6376\tLR: 0.000000\n",
      "Training Epoch: 30 [22720/72641]\tLoss: 0.6140\tLR: 0.000000\n",
      "Training Epoch: 30 [23040/72641]\tLoss: 0.6004\tLR: 0.000000\n",
      "Training Epoch: 30 [23360/72641]\tLoss: 0.6242\tLR: 0.000000\n",
      "Training Epoch: 30 [23680/72641]\tLoss: 0.6200\tLR: 0.000000\n",
      "Training Epoch: 30 [24000/72641]\tLoss: 0.6597\tLR: 0.000000\n",
      "Training Epoch: 30 [24320/72641]\tLoss: 0.6413\tLR: 0.000000\n",
      "Training Epoch: 30 [24640/72641]\tLoss: 0.6466\tLR: 0.000000\n",
      "Training Epoch: 30 [24960/72641]\tLoss: 0.6855\tLR: 0.000000\n",
      "Training Epoch: 30 [25280/72641]\tLoss: 0.5767\tLR: 0.000000\n",
      "Training Epoch: 30 [25600/72641]\tLoss: 0.6031\tLR: 0.000000\n",
      "Training Epoch: 30 [25920/72641]\tLoss: 0.6523\tLR: 0.000000\n",
      "Training Epoch: 30 [26240/72641]\tLoss: 0.6016\tLR: 0.000000\n",
      "Training Epoch: 30 [26560/72641]\tLoss: 0.6454\tLR: 0.000000\n",
      "Training Epoch: 30 [26880/72641]\tLoss: 0.6081\tLR: 0.000000\n",
      "Training Epoch: 30 [27200/72641]\tLoss: 0.6050\tLR: 0.000000\n",
      "Training Epoch: 30 [27520/72641]\tLoss: 0.6788\tLR: 0.000000\n",
      "Training Epoch: 30 [27840/72641]\tLoss: 0.6308\tLR: 0.000000\n",
      "Training Epoch: 30 [28160/72641]\tLoss: 0.6028\tLR: 0.000000\n",
      "Training Epoch: 30 [28480/72641]\tLoss: 0.6209\tLR: 0.000000\n",
      "Training Epoch: 30 [28800/72641]\tLoss: 0.6024\tLR: 0.000000\n",
      "Training Epoch: 30 [29120/72641]\tLoss: 0.6734\tLR: 0.000000\n",
      "Training Epoch: 30 [29440/72641]\tLoss: 0.6236\tLR: 0.000000\n",
      "Training Epoch: 30 [29760/72641]\tLoss: 0.5889\tLR: 0.000000\n",
      "Training Epoch: 30 [30080/72641]\tLoss: 0.6593\tLR: 0.000000\n",
      "Training Epoch: 30 [30400/72641]\tLoss: 0.6670\tLR: 0.000000\n",
      "Training Epoch: 30 [30720/72641]\tLoss: 0.6066\tLR: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 30 [31040/72641]\tLoss: 0.6210\tLR: 0.000000\n",
      "Training Epoch: 30 [31360/72641]\tLoss: 0.6090\tLR: 0.000000\n",
      "Training Epoch: 30 [31680/72641]\tLoss: 0.6071\tLR: 0.000000\n",
      "Training Epoch: 30 [32000/72641]\tLoss: 0.6496\tLR: 0.000000\n",
      "Training Epoch: 30 [32320/72641]\tLoss: 0.6146\tLR: 0.000000\n",
      "Training Epoch: 30 [32640/72641]\tLoss: 0.6462\tLR: 0.000000\n",
      "Training Epoch: 30 [32960/72641]\tLoss: 0.6642\tLR: 0.000000\n",
      "Training Epoch: 30 [33280/72641]\tLoss: 0.6641\tLR: 0.000000\n",
      "Training Epoch: 30 [33600/72641]\tLoss: 0.5600\tLR: 0.000000\n",
      "Training Epoch: 30 [33920/72641]\tLoss: 0.6139\tLR: 0.000000\n",
      "Training Epoch: 30 [34240/72641]\tLoss: 0.6215\tLR: 0.000000\n",
      "Training Epoch: 30 [34560/72641]\tLoss: 0.6567\tLR: 0.000000\n",
      "Training Epoch: 30 [34880/72641]\tLoss: 0.6332\tLR: 0.000000\n",
      "Training Epoch: 30 [35200/72641]\tLoss: 0.6078\tLR: 0.000000\n",
      "Training Epoch: 30 [35520/72641]\tLoss: 0.6343\tLR: 0.000000\n",
      "Training Epoch: 30 [35840/72641]\tLoss: 0.6394\tLR: 0.000000\n",
      "Training Epoch: 30 [36160/72641]\tLoss: 0.6399\tLR: 0.000000\n",
      "Training Epoch: 30 [36480/72641]\tLoss: 0.6218\tLR: 0.000000\n",
      "Training Epoch: 30 [36800/72641]\tLoss: 0.6323\tLR: 0.000000\n",
      "Training Epoch: 30 [37120/72641]\tLoss: 0.6178\tLR: 0.000000\n",
      "Training Epoch: 30 [37440/72641]\tLoss: 0.6313\tLR: 0.000000\n",
      "Training Epoch: 30 [37760/72641]\tLoss: 0.5963\tLR: 0.000000\n",
      "Training Epoch: 30 [38080/72641]\tLoss: 0.5882\tLR: 0.000000\n",
      "Training Epoch: 30 [38400/72641]\tLoss: 0.6398\tLR: 0.000000\n",
      "Training Epoch: 30 [38720/72641]\tLoss: 0.6334\tLR: 0.000000\n",
      "Training Epoch: 30 [39040/72641]\tLoss: 0.6045\tLR: 0.000000\n",
      "Training Epoch: 30 [39360/72641]\tLoss: 0.6064\tLR: 0.000000\n",
      "Training Epoch: 30 [39680/72641]\tLoss: 0.6314\tLR: 0.000000\n",
      "Training Epoch: 30 [40000/72641]\tLoss: 0.6507\tLR: 0.000000\n",
      "Training Epoch: 30 [40320/72641]\tLoss: 0.6250\tLR: 0.000000\n",
      "Training Epoch: 30 [40640/72641]\tLoss: 0.5879\tLR: 0.000000\n",
      "Training Epoch: 30 [40960/72641]\tLoss: 0.6267\tLR: 0.000000\n",
      "Training Epoch: 30 [41280/72641]\tLoss: 0.6783\tLR: 0.000000\n",
      "Training Epoch: 30 [41600/72641]\tLoss: 0.6060\tLR: 0.000000\n",
      "Training Epoch: 30 [41920/72641]\tLoss: 0.6338\tLR: 0.000000\n",
      "Training Epoch: 30 [42240/72641]\tLoss: 0.6199\tLR: 0.000000\n",
      "Training Epoch: 30 [42560/72641]\tLoss: 0.5894\tLR: 0.000000\n",
      "Training Epoch: 30 [42880/72641]\tLoss: 0.6447\tLR: 0.000000\n",
      "Training Epoch: 30 [43200/72641]\tLoss: 0.6624\tLR: 0.000000\n",
      "Training Epoch: 30 [43520/72641]\tLoss: 0.6967\tLR: 0.000000\n",
      "Training Epoch: 30 [43840/72641]\tLoss: 0.5747\tLR: 0.000000\n",
      "Training Epoch: 30 [44160/72641]\tLoss: 0.6360\tLR: 0.000000\n",
      "Training Epoch: 30 [44480/72641]\tLoss: 0.6497\tLR: 0.000000\n",
      "Training Epoch: 30 [44800/72641]\tLoss: 0.6073\tLR: 0.000000\n",
      "Training Epoch: 30 [45120/72641]\tLoss: 0.6080\tLR: 0.000000\n",
      "Training Epoch: 30 [45440/72641]\tLoss: 0.6567\tLR: 0.000000\n",
      "Training Epoch: 30 [45760/72641]\tLoss: 0.6623\tLR: 0.000000\n",
      "Training Epoch: 30 [46080/72641]\tLoss: 0.6418\tLR: 0.000000\n",
      "Training Epoch: 30 [46400/72641]\tLoss: 0.6467\tLR: 0.000000\n",
      "Training Epoch: 30 [46720/72641]\tLoss: 0.6420\tLR: 0.000000\n",
      "Training Epoch: 30 [47040/72641]\tLoss: 0.6035\tLR: 0.000000\n",
      "Training Epoch: 30 [47360/72641]\tLoss: 0.6392\tLR: 0.000000\n",
      "Training Epoch: 30 [47680/72641]\tLoss: 0.6574\tLR: 0.000000\n",
      "Training Epoch: 30 [48000/72641]\tLoss: 0.6261\tLR: 0.000000\n",
      "Training Epoch: 30 [48320/72641]\tLoss: 0.6969\tLR: 0.000000\n",
      "Training Epoch: 30 [48640/72641]\tLoss: 0.6440\tLR: 0.000000\n",
      "Training Epoch: 30 [48960/72641]\tLoss: 0.6122\tLR: 0.000000\n",
      "Training Epoch: 30 [49280/72641]\tLoss: 0.6130\tLR: 0.000000\n",
      "Training Epoch: 30 [49600/72641]\tLoss: 0.6116\tLR: 0.000000\n",
      "Training Epoch: 30 [49920/72641]\tLoss: 0.6240\tLR: 0.000000\n",
      "Training Epoch: 30 [50240/72641]\tLoss: 0.6195\tLR: 0.000000\n",
      "Training Epoch: 30 [50560/72641]\tLoss: 0.6348\tLR: 0.000000\n",
      "Training Epoch: 30 [50880/72641]\tLoss: 0.6306\tLR: 0.000000\n",
      "Training Epoch: 30 [51200/72641]\tLoss: 0.6704\tLR: 0.000000\n",
      "Training Epoch: 30 [51520/72641]\tLoss: 0.6288\tLR: 0.000000\n",
      "Training Epoch: 30 [51840/72641]\tLoss: 0.6388\tLR: 0.000000\n",
      "Training Epoch: 30 [52160/72641]\tLoss: 0.6701\tLR: 0.000000\n",
      "Training Epoch: 30 [52480/72641]\tLoss: 0.6069\tLR: 0.000000\n",
      "Training Epoch: 30 [52800/72641]\tLoss: 0.6421\tLR: 0.000000\n",
      "Training Epoch: 30 [53120/72641]\tLoss: 0.6645\tLR: 0.000000\n",
      "Training Epoch: 30 [53440/72641]\tLoss: 0.5862\tLR: 0.000000\n",
      "Training Epoch: 30 [53760/72641]\tLoss: 0.6806\tLR: 0.000000\n",
      "Training Epoch: 30 [54080/72641]\tLoss: 0.6070\tLR: 0.000000\n",
      "Training Epoch: 30 [54400/72641]\tLoss: 0.6370\tLR: 0.000000\n",
      "Training Epoch: 30 [54720/72641]\tLoss: 0.6278\tLR: 0.000000\n",
      "Training Epoch: 30 [55040/72641]\tLoss: 0.6173\tLR: 0.000000\n",
      "Training Epoch: 30 [55360/72641]\tLoss: 0.6417\tLR: 0.000000\n",
      "Training Epoch: 30 [55680/72641]\tLoss: 0.6065\tLR: 0.000000\n",
      "Training Epoch: 30 [56000/72641]\tLoss: 0.6279\tLR: 0.000000\n",
      "Training Epoch: 30 [56320/72641]\tLoss: 0.6333\tLR: 0.000000\n",
      "Training Epoch: 30 [56640/72641]\tLoss: 0.6818\tLR: 0.000000\n",
      "Training Epoch: 30 [56960/72641]\tLoss: 0.6475\tLR: 0.000000\n",
      "Training Epoch: 30 [57280/72641]\tLoss: 0.5939\tLR: 0.000000\n",
      "Training Epoch: 30 [57600/72641]\tLoss: 0.6493\tLR: 0.000000\n",
      "Training Epoch: 30 [57920/72641]\tLoss: 0.6238\tLR: 0.000000\n",
      "Training Epoch: 30 [58240/72641]\tLoss: 0.6410\tLR: 0.000000\n",
      "Training Epoch: 30 [58560/72641]\tLoss: 0.5859\tLR: 0.000000\n",
      "Training Epoch: 30 [58880/72641]\tLoss: 0.6055\tLR: 0.000000\n",
      "Training Epoch: 30 [59200/72641]\tLoss: 0.6181\tLR: 0.000000\n",
      "Training Epoch: 30 [59520/72641]\tLoss: 0.6434\tLR: 0.000000\n",
      "Training Epoch: 30 [59840/72641]\tLoss: 0.6805\tLR: 0.000000\n",
      "Training Epoch: 30 [60160/72641]\tLoss: 0.6331\tLR: 0.000000\n",
      "Training Epoch: 30 [60480/72641]\tLoss: 0.6251\tLR: 0.000000\n",
      "Training Epoch: 30 [60800/72641]\tLoss: 0.6168\tLR: 0.000000\n",
      "Training Epoch: 30 [61120/72641]\tLoss: 0.6096\tLR: 0.000000\n",
      "Training Epoch: 30 [61440/72641]\tLoss: 0.5887\tLR: 0.000000\n",
      "Training Epoch: 30 [61760/72641]\tLoss: 0.5983\tLR: 0.000000\n",
      "Training Epoch: 30 [62080/72641]\tLoss: 0.6684\tLR: 0.000000\n",
      "Training Epoch: 30 [62400/72641]\tLoss: 0.6525\tLR: 0.000000\n",
      "Training Epoch: 30 [62720/72641]\tLoss: 0.6161\tLR: 0.000000\n",
      "Training Epoch: 30 [63040/72641]\tLoss: 0.6447\tLR: 0.000000\n",
      "Training Epoch: 30 [63360/72641]\tLoss: 0.6553\tLR: 0.000000\n",
      "Training Epoch: 30 [63680/72641]\tLoss: 0.6144\tLR: 0.000000\n",
      "Training Epoch: 30 [64000/72641]\tLoss: 0.6245\tLR: 0.000000\n",
      "Training Epoch: 30 [64320/72641]\tLoss: 0.6025\tLR: 0.000000\n",
      "Training Epoch: 30 [64640/72641]\tLoss: 0.6460\tLR: 0.000000\n",
      "Training Epoch: 30 [64960/72641]\tLoss: 0.5933\tLR: 0.000000\n",
      "Training Epoch: 30 [65280/72641]\tLoss: 0.6022\tLR: 0.000000\n",
      "Training Epoch: 30 [65600/72641]\tLoss: 0.6410\tLR: 0.000000\n",
      "Training Epoch: 30 [65920/72641]\tLoss: 0.6507\tLR: 0.000000\n",
      "Training Epoch: 30 [66240/72641]\tLoss: 0.6750\tLR: 0.000000\n",
      "Training Epoch: 30 [66560/72641]\tLoss: 0.6034\tLR: 0.000000\n",
      "Training Epoch: 30 [66880/72641]\tLoss: 0.6406\tLR: 0.000000\n",
      "Training Epoch: 30 [67200/72641]\tLoss: 0.6215\tLR: 0.000000\n",
      "Training Epoch: 30 [67520/72641]\tLoss: 0.6193\tLR: 0.000000\n",
      "Training Epoch: 30 [67840/72641]\tLoss: 0.6089\tLR: 0.000000\n",
      "Training Epoch: 30 [68160/72641]\tLoss: 0.6391\tLR: 0.000000\n",
      "Training Epoch: 30 [68480/72641]\tLoss: 0.6323\tLR: 0.000000\n",
      "Training Epoch: 30 [68800/72641]\tLoss: 0.6730\tLR: 0.000000\n",
      "Training Epoch: 30 [69120/72641]\tLoss: 0.6730\tLR: 0.000000\n",
      "Training Epoch: 30 [69440/72641]\tLoss: 0.6162\tLR: 0.000000\n",
      "Training Epoch: 30 [69760/72641]\tLoss: 0.6073\tLR: 0.000000\n",
      "Training Epoch: 30 [70080/72641]\tLoss: 0.5835\tLR: 0.000000\n",
      "Training Epoch: 30 [70400/72641]\tLoss: 0.6528\tLR: 0.000000\n",
      "Training Epoch: 30 [70720/72641]\tLoss: 0.5937\tLR: 0.000000\n",
      "Training Epoch: 30 [71040/72641]\tLoss: 0.6058\tLR: 0.000000\n",
      "Training Epoch: 30 [71360/72641]\tLoss: 0.6496\tLR: 0.000000\n",
      "Training Epoch: 30 [71680/72641]\tLoss: 0.6211\tLR: 0.000000\n",
      "Training Epoch: 30 [72000/72641]\tLoss: 0.6650\tLR: 0.000000\n",
      "Training Epoch: 30 [72320/72641]\tLoss: 0.6216\tLR: 0.000000\n",
      "Training Epoch: 30 [72640/72641]\tLoss: 0.6365\tLR: 0.000000\n",
      "Val Result: Acc: 0.1433, C_ACC: 0.7115, DOA: 87.7428, ACC_k: 0.0993\n",
      "ext:0.0, cls:0.561152, coar:0.0, fine:0.0,\n"
     ]
    }
   ],
   "source": [
    "b1 = 0\n",
    "b2 = 0\n",
    "for epoch in range(1, 31):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    tloss_ext = 0.\n",
    "    tloss_cls = 0.\n",
    "    tloss_coar = 0.\n",
    "    tloss_fine = 0.\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_index, (x_sg, y_sg, y_region, y_angle, y_class, bce, _) in enumerate(train_loader):\n",
    "        \n",
    "        x_sg = x_sg.cuda()\n",
    "        y_sg = y_sg.cuda()\n",
    "        y_region = y_region.cuda()\n",
    "        y_angle = y_angle.cuda()\n",
    "        y_class = y_class.cuda()\n",
    "        bce = bce.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out_localizer1, out_localizer2, out_extractor, out_detector = net(x_sg)\n",
    "        \n",
    "        loss_ext = ExtW * ext_loss(y_class.unsqueeze(1).unsqueeze(1) * torch.tanh(out_extractor), y_sg)\n",
    "        loss_cls = ClsW * cls_loss(torch.sigmoid(out_detector), y_class.unsqueeze(1))\n",
    "        loss_coarse = CoarW * coarse_loss(y_class.unsqueeze(1) * out_localizer1, (y_class * y_region).long())\n",
    "        loss_fine = FineW * fine_loss(y_class.unsqueeze(1) * bce * torch.sigmoid(out_localizer2), y_class.unsqueeze(1) * y_angle)\n",
    "        \n",
    "        loss = loss_ext + loss_cls + \\\n",
    "            loss_coarse + loss_fine\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tloss_ext += loss_ext.item()\n",
    "        tloss_cls += loss_cls.item()\n",
    "        tloss_coar += loss_coarse.item()\n",
    "        tloss_fine += loss_fine.item()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if batch_index % 10 == 9:\n",
    "            print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
    "                running_loss / (9 * batch_size),\n",
    "                optimizer.param_groups[0]['lr'],\n",
    "                epoch=epoch,\n",
    "                trained_samples=batch_index * batch_size + len(x_sg),\n",
    "                total_samples=len(train_indices)\n",
    "            ))\n",
    "            running_loss = 0.0\n",
    "            wandb.log({\"train/Train_loss_coar\": tloss_coar / (batch_size * 10),\n",
    "               \"train/Train_loss_fine\": tloss_fine / (batch_size * 10),\n",
    "               \"train/Train_loss_cls\": tloss_cls / (batch_size * 10),\n",
    "               \"train/Train_loss_ext\": tloss_ext / (batch_size * 10)},\n",
    "                      step = b1)\n",
    "            tloss_ext = 0.0\n",
    "            tloss_cls = 0.0\n",
    "            tloss_coar = 0.0\n",
    "            tloss_fine = 0.0\n",
    "            b1 += 1\n",
    "        \n",
    "    \"\"\"\n",
    "    for sample_idx in range(5):\n",
    "        name_xg = f'{sample_dir}/{epoch}_xg_{sample_idx}.wav'\n",
    "        name_yg = f'{sample_dir}/{epoch}_yg_{sample_idx}.wav'\n",
    "        \n",
    "        torchaudio.save(name_xg, x_sg[sample_idx].cpu(), 16000)\n",
    "        torchaudio.save(name_yg, y_sg[sample_idx].cpu(), 16000)\n",
    "    \"\"\"\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    net.eval()\n",
    "    accuracy = 0.0\n",
    "    class_acc = 0.0\n",
    "    doa_err = 0.0\n",
    "    doa_acc = 0.0\n",
    "    count = 0.0\n",
    "    tloss_ext = 0.\n",
    "    tloss_cls = 0.\n",
    "    tloss_coar = 0.\n",
    "    tloss_fine = 0.\n",
    "    total_loss = 0.0\n",
    "    for batch_index, (x_sg, y_sg, y_region, y_angle, y_class, bce, _) in enumerate(validation_loader):\n",
    "        x_sg = x_sg.cuda()\n",
    "        y_sg = y_sg.cuda()\n",
    "        y_region = y_region.cuda()\n",
    "        y_angle = y_angle.cuda()\n",
    "        y_class = y_class.cuda()\n",
    "        bce = bce.cuda()\n",
    "\n",
    "        out_localizer1, out_localizer2, out_extractor, out_detector = net(x_sg)\n",
    "        loss_ext = ExtW * ext_loss(y_class.unsqueeze(1).unsqueeze(1) * torch.tanh(out_extractor), y_sg)\n",
    "        loss_cls = ClsW * cls_loss(torch.sigmoid(out_detector), y_class.unsqueeze(1))\n",
    "        loss_coarse = CoarW * coarse_loss(y_class.unsqueeze(1) * out_localizer1, (y_class * y_region).long())\n",
    "        loss_fine = FineW * fine_loss(y_class.unsqueeze(1) * bce * torch.sigmoid(out_localizer2), y_class.unsqueeze(1) * y_angle)\n",
    "        \n",
    "        tloss_ext += loss_ext.item()\n",
    "        tloss_cls += loss_cls.item()\n",
    "        tloss_coar += loss_coarse.item()\n",
    "        tloss_fine += loss_fine.item()\n",
    "        \n",
    "        loss = loss_ext + loss_cls + loss_coarse + loss_fine\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, region_pidx = out_localizer1.max(1)\n",
    "        accuracy += region_pidx.eq(y_region).sum()\n",
    "        \n",
    "        class_acc += (torch.sigmoid(out_detector) > 0.5).squeeze(1).eq(y_class).sum()\n",
    "        a, b, c = DOA(y_region, y_angle, out_localizer1, out_localizer2, 20, y_class, torch.sigmoid(out_detector))\n",
    "        \n",
    "        doa_err += a\n",
    "        doa_acc += b\n",
    "        count += c\n",
    "        \n",
    "    wandb.log({\"val/Val_loss_coar\": tloss_coar / len(val_indices),\n",
    "               \"val/Val_loss_fine\": tloss_fine / len(val_indices),\n",
    "               \"val/Val_loss_cls\": tloss_cls / len(val_indices),\n",
    "               \"val/Val_loss_ext\": tloss_ext / len(val_indices),\n",
    "               \"val/Val_Cls_acc\": class_acc.float() / len(val_indices),\n",
    "               \"val/Val_doa_acc\": doa_acc / count,\n",
    "               \"val/Val_doa_err\": doa_err / count},\n",
    "             commit = False)\n",
    "    b2 += 1\n",
    "    print('Val Result: Acc: {:0.4f}, C_ACC: {:0.4f}, DOA: {:0.4f}, ACC_k: {:0.4f}'.format(\n",
    "        accuracy.float() / len(val_indices),\n",
    "        class_acc.float() / len(val_indices),\n",
    "        doa_err / count,\n",
    "        doa_acc / count\n",
    "    ))\n",
    "    print(f'ext:{round(tloss_ext/len(val_indices), 6)}, cls:{round(tloss_cls/len(val_indices), 6)}, coar:{round(tloss_coar/len(val_indices), 6)}, fine:{round(tloss_fine/len(val_indices), 6)},')\n",
    "    weight_path = os.path.join(weight_dir, str(epoch) + '.pt')\n",
    "    torch.save(net.state_dict(), weight_path, _use_new_zipfile_serialization=False)\n",
    "    \n",
    "    \"\"\"\n",
    "    for sample_idx in range(4):\n",
    "        parent_path = os.path.join('samples', t + '_val')\n",
    "        name_gen = f'{parent_path}/{epoch}_gen_{sample_idx}.wav'\n",
    "        name_mixed = f'{parent_path}/{epoch}_mix_{sample_idx}.wav'\n",
    "        name_target = f'{parent_path}/{epoch}_target_{sample_idx}.wav'\n",
    "        torchaudio.save(name_gen, torch.tanh(out_extractor)[sample_idx].cpu(), 16000)\n",
    "        torchaudio.save(name_mixed, x_sg[sample_idx].cpu(), 16000)\n",
    "        torchaudio.save(name_target, y_sg[sample_idx].cpu(), 16000)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "gts = []\n",
    "net = KeywordSpeakerLocalizer(encoder_ch).cuda()\n",
    "weight_file = 'weight/20230103-205newApproach/12.pt'\n",
    "net.load_state_dict(torch.load(weight_file))\n",
    "net.eval()\n",
    "\n",
    "accuracy = 0.0\n",
    "class_acc = 0.0\n",
    "doa_err = 0.0\n",
    "doa_acc = 0.0\n",
    "count = 0.0\n",
    "tloss_ext = 0.\n",
    "tloss_cls = 0.\n",
    "tloss_coar = 0.\n",
    "tloss_fine = 0.\n",
    "total_loss = 0.0\n",
    "for batch_index, (x_sg, y_sg, y_region, y_angle, y_class, bce, _) in enumerate(test_loader):\n",
    "    x_sg = x_sg.cuda()\n",
    "    y_sg = y_sg.cuda()\n",
    "    y_region = y_region.cuda()\n",
    "    y_angle = y_angle.cuda()\n",
    "    y_class = y_class.cuda()\n",
    "    bce = bce.cuda()\n",
    "\n",
    "    out_localizer1, out_localizer2, out_extractor, out_detector = net(x_sg)\n",
    "    loss_ext = ExtW * ext_loss(y_class.unsqueeze(1).unsqueeze(1) * torch.tanh(out_extractor), y_sg)\n",
    "    loss_cls = ClsW * cls_loss(torch.sigmoid(out_detector), y_class.unsqueeze(1))\n",
    "    loss_coarse = CoarW * coarse_loss(y_class.unsqueeze(1) * out_localizer1, (y_class * y_region).long())\n",
    "    loss_fine = FineW * fine_loss(y_class.unsqueeze(1) * bce * torch.sigmoid(out_localizer2), y_class.unsqueeze(1) * y_angle)\n",
    "\n",
    "    tloss_ext += loss_ext.item()\n",
    "    tloss_cls += loss_cls.item()\n",
    "    tloss_coar += loss_coarse.item()\n",
    "    tloss_fine += loss_fine.item()\n",
    "\n",
    "    loss = loss_ext + loss_cls + loss_coarse + loss_fine\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    _, region_pidx = out_localizer1.max(1)\n",
    "    accuracy += region_pidx.eq(y_region).sum()\n",
    "\n",
    "    class_acc += (torch.sigmoid(out_detector) > 0.75).squeeze(1).eq(y_class).sum()\n",
    "    a, b, c = DOA(y_region, y_angle, out_localizer1, out_localizer2, 20, y_class, torch.sigmoid(out_detector))\n",
    "    \n",
    "    predicts.append(torch.sigmoid(out_detector).detach().cpu()[0])\n",
    "    gts.append(y_class.detach().cpu())\n",
    "    \n",
    "    doa_err += a\n",
    "    doa_acc += b\n",
    "    count += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ROC(y_pred, y_true):\n",
    "    predict2 = []\n",
    "    gt2 = []\n",
    "    for i, j in enumerate(y_pred):\n",
    "        predict2.append(float(j[0]))\n",
    "        gt2.append(float(y_true[i]))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(gt2, predict2, pos_label=1)\n",
    "    #plt.plot(fpr, tpr)\n",
    "    return fpr, tpr, thresholds\n",
    "\n",
    "def draw_DET(y_pred, y_true):\n",
    "    predict2 = []\n",
    "    gt2 = []\n",
    "    for i, j in enumerate(y_pred):\n",
    "        predict2.append(float(j[0]))\n",
    "        gt2.append(float(y_true[i]))\n",
    "    fpr, fnr, thresholds = metrics.det_curve(gt2, predict2, pos_label=1)\n",
    "    return fpr, fnr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_fpr, st_tpr, st_thresholds = draw_ROC(predicts, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt2_fpr, mt_tpr, mt_thresholds= draw_ROC(predicts, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc491169d0>]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(st_fpr, st_tpr)\n",
    "plt.plot(mt_fpr, mt_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_fpr, st_fnr, st_thresholds = draw_DET(predicts, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_fpr, mt_fnr, mt_thresholds = draw_DET(predicts, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc4871e640>]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(st_fpr* (10000/3600), st_fnr)\n",
    "plt.plot(mt_fpr* (10000/3600), mt_fnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 75.46641791044776, recall: 0.7175732217573222, precision: 0.7713064987632111\n"
     ]
    }
   ],
   "source": [
    "for threshold in [0.5]:\n",
    "    predict2 = []\n",
    "    gt2 = []\n",
    "    for i, j in enumerate(predicts):\n",
    "        predict2.append(float(j[0]))\n",
    "        gt2.append(float(gts[i]))\n",
    "\n",
    "    predict2 = np.array(predict2)\n",
    "    gt2 = np.array(gt2)\n",
    "\n",
    "    predict2[predict2 > 0.5] = 1\n",
    "    predict2[predict2 <= 0.5] = 0\n",
    "    gt2[gt2==0] = 0\n",
    "\n",
    "\n",
    "    confusion_dictionary = {'TP':0, 'TN':0, 'FP':0, 'FN':0}\n",
    "    for i in range(len(predict2)):\n",
    "        #print(predict2[i], gt2[i])\n",
    "        if predict2[i] and gt2[i]:\n",
    "            confusion_dictionary['TP'] += 1\n",
    "        elif not predict2[i] and not gt2[i]:\n",
    "            confusion_dictionary['TN'] += 1\n",
    "        elif predict2[i] and not gt2[i]:\n",
    "            confusion_dictionary['FP'] += 1\n",
    "        elif not predict2[i] and gt2[i]:\n",
    "            confusion_dictionary['FN'] += 1\n",
    "\n",
    "    predict2 = predict2 > 0.3\n",
    "    correct = sum(predict2 == gt2)\n",
    "    accuracy = correct / len(gt2)\n",
    "    recall = confusion_dictionary['TP'] / (confusion_dictionary['TP'] + confusion_dictionary['FN'])\n",
    "    precision = confusion_dictionary['TP'] / (confusion_dictionary['TP'] + confusion_dictionary['FP'])\n",
    "    print(f'accuracy: {accuracy*100}, recall: {recall}, precision: {precision}')\n",
    "    #print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_confusion_dictionary = confusion_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_confusion_dictionary = confusion_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'TP': 3355, 'TN': 4051, 'FP': 744, 'FN': 1498},\n",
       " {'TP': 3430, 'TN': 3851, 'FP': 1017, 'FN': 1350})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_confusion_dictionary, mt_confusion_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 74.77197346600332, recall: 0.894923012900541, precision: 0.6903691813804174\n",
      "accuracy: 76.73092868988391, recall: 0.7546816479400749, precision: 0.7728531855955678\n",
      "accuracy: 74.61650082918739, recall: 0.6371202663337495, precision: 0.8128484204937616\n",
      "accuracy: 72.19112769485903, recall: 0.5384935497295048, precision: 0.8476907959384212\n",
      "accuracy: 69.7035655058043, recall: 0.45588847274240535, precision: 0.876750700280112\n",
      "accuracy: 66.3971807628524, recall: 0.3651685393258427, precision: 0.9018499486125385\n",
      "accuracy: 62.63474295190713, recall: 0.2725759467332501, precision: 0.923185341789993\n",
      "accuracy: 58.2089552238806, recall: 0.17103620474406991, precision: 0.9448275862068966\n",
      "accuracy: 53.64842454394692, recall: 0.07074490220557636, precision: 0.9826589595375722\n"
     ]
    }
   ],
   "source": [
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    predict2 = []\n",
    "    gt2 = []\n",
    "    for i, j in enumerate(predicts):\n",
    "        predict2.append(float(j[0]))\n",
    "        gt2.append(float(gts[i]))\n",
    "\n",
    "    predict2 = np.array(predict2)\n",
    "    gt2 = np.array(gt2)\n",
    "\n",
    "    predict2[predict2 > threshold] = 1\n",
    "    predict2[predict2 <= threshold] = 0\n",
    "    gt2[gt2==0] = 0\n",
    "\n",
    "\n",
    "    confusion_dictionary = {'TP':0, 'TN':0, 'FP':0, 'FN':0}\n",
    "    for i in range(len(predict2)):\n",
    "        #print(predict2[i], gt2[i])\n",
    "        if predict2[i] and gt2[i]:\n",
    "            confusion_dictionary['TP'] += 1\n",
    "        elif not predict2[i] and not gt2[i]:\n",
    "            confusion_dictionary['TN'] += 1\n",
    "        elif predict2[i] and not gt2[i]:\n",
    "            confusion_dictionary['FP'] += 1\n",
    "        elif not predict2[i] and gt2[i]:\n",
    "            confusion_dictionary['FN'] += 1\n",
    "\n",
    "    predict2 = predict2 > 0.3\n",
    "    correct = sum(predict2 == gt2)\n",
    "    accuracy = correct / len(gt2)\n",
    "    recall = confusion_dictionary['TP'] / (confusion_dictionary['TP'] + confusion_dictionary['FN'])\n",
    "    precision = confusion_dictionary['TP'] / (confusion_dictionary['TP'] + confusion_dictionary['FP'])\n",
    "    print(f'accuracy: {accuracy*100}, recall: {recall}, precision: {precision}')\n",
    "    #print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.3974989749897499\n",
      "{'TP': 1939, 'TN': 4612, 'FP': 158, 'FN': 2939}\n"
     ]
    }
   ],
   "source": [
    "predict2 = []\n",
    "gt2 = []\n",
    "for i, j in enumerate(predicts):\n",
    "    predict2.append(float(j[0]))\n",
    "    gt2.append(float(gts[i]))\n",
    "\n",
    "predict2 = np.array(predict2)\n",
    "gt2 = np.array(gt2)\n",
    "\n",
    "predict2[predict2 > threshold] = 1\n",
    "predict2[predict2 <= threshold] = 0\n",
    "gt2[gt2==0] = 0\n",
    "\n",
    "\n",
    "confusion_dictionary = {'TP':0, 'TN':0, 'FP':0, 'FN':0}\n",
    "for i in range(len(predict2)):\n",
    "    #print(predict2[i], gt2[i])\n",
    "    if predict2[i] and gt2[i]:\n",
    "        confusion_dictionary['TP'] += 1\n",
    "    elif not predict2[i] and not gt2[i]:\n",
    "        confusion_dictionary['TN'] += 1\n",
    "    elif predict2[i] and not gt2[i]:\n",
    "        confusion_dictionary['FP'] += 1\n",
    "    elif not predict2[i] and gt2[i]:\n",
    "        confusion_dictionary['FN'] += 1\n",
    "\n",
    "predict2 = predict2 > 0.3\n",
    "correct = sum(predict2 == gt2)\n",
    "accuracy = correct / len(gt2)\n",
    "recall = confusion_dictionary['TP'] / (confusion_dictionary['TP'] + confusion_dictionary['FN'])\n",
    "print(f'recall: {recall}')\n",
    "#print(\"Threshold: \", threshold, \", accuracy: \", accuracy)\n",
    "#print(accuracy * 100)\n",
    "\n",
    "print(confusion_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falseAlarmperHour = 158 /((2939 + 1939) * (10000/3600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.7",
   "language": "python",
   "name": "torch1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
